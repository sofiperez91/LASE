{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction on Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6930, Val: 0.6833, Test: 0.7031\n",
      "Epoch: 002, Loss: 0.6833, Val: 0.6812, Test: 0.6972\n",
      "Epoch: 003, Loss: 0.7039, Val: 0.6885, Test: 0.7066\n",
      "Epoch: 004, Loss: 0.6784, Val: 0.6980, Test: 0.7239\n",
      "Epoch: 005, Loss: 0.6852, Val: 0.7068, Test: 0.7394\n",
      "Epoch: 006, Loss: 0.6878, Val: 0.7131, Test: 0.7451\n",
      "Epoch: 007, Loss: 0.6883, Val: 0.7073, Test: 0.7348\n",
      "Epoch: 008, Loss: 0.6870, Val: 0.6965, Test: 0.7197\n",
      "Epoch: 009, Loss: 0.6830, Val: 0.6891, Test: 0.7076\n",
      "Epoch: 010, Loss: 0.6761, Val: 0.6857, Test: 0.6987\n",
      "Epoch: 011, Loss: 0.6698, Val: 0.6835, Test: 0.6920\n",
      "Epoch: 012, Loss: 0.6734, Val: 0.6832, Test: 0.6921\n",
      "Epoch: 013, Loss: 0.6685, Val: 0.6930, Test: 0.7060\n",
      "Epoch: 014, Loss: 0.6586, Val: 0.7174, Test: 0.7335\n",
      "Epoch: 015, Loss: 0.6553, Val: 0.7420, Test: 0.7504\n",
      "Epoch: 016, Loss: 0.6487, Val: 0.7464, Test: 0.7467\n",
      "Epoch: 017, Loss: 0.6378, Val: 0.7380, Test: 0.7356\n",
      "Epoch: 018, Loss: 0.6248, Val: 0.7408, Test: 0.7335\n",
      "Epoch: 019, Loss: 0.6165, Val: 0.7703, Test: 0.7496\n",
      "Epoch: 020, Loss: 0.5982, Val: 0.7870, Test: 0.7649\n",
      "Epoch: 021, Loss: 0.5883, Val: 0.7852, Test: 0.7658\n",
      "Epoch: 022, Loss: 0.5790, Val: 0.7852, Test: 0.7584\n",
      "Epoch: 023, Loss: 0.5668, Val: 0.7860, Test: 0.7576\n",
      "Epoch: 024, Loss: 0.5702, Val: 0.7848, Test: 0.7650\n",
      "Epoch: 025, Loss: 0.5696, Val: 0.7893, Test: 0.7669\n",
      "Epoch: 026, Loss: 0.5580, Val: 0.7957, Test: 0.7701\n",
      "Epoch: 027, Loss: 0.5552, Val: 0.8007, Test: 0.7791\n",
      "Epoch: 028, Loss: 0.5541, Val: 0.8095, Test: 0.7890\n",
      "Epoch: 029, Loss: 0.5460, Val: 0.8213, Test: 0.7981\n",
      "Epoch: 030, Loss: 0.5369, Val: 0.8302, Test: 0.8084\n",
      "Epoch: 031, Loss: 0.5329, Val: 0.8441, Test: 0.8269\n",
      "Epoch: 032, Loss: 0.5280, Val: 0.8562, Test: 0.8422\n",
      "Epoch: 033, Loss: 0.5219, Val: 0.8614, Test: 0.8500\n",
      "Epoch: 034, Loss: 0.5167, Val: 0.8639, Test: 0.8533\n",
      "Epoch: 035, Loss: 0.4996, Val: 0.8636, Test: 0.8537\n",
      "Epoch: 036, Loss: 0.5076, Val: 0.8647, Test: 0.8566\n",
      "Epoch: 037, Loss: 0.5070, Val: 0.8638, Test: 0.8581\n",
      "Epoch: 038, Loss: 0.5009, Val: 0.8699, Test: 0.8638\n",
      "Epoch: 039, Loss: 0.4978, Val: 0.8790, Test: 0.8738\n",
      "Epoch: 040, Loss: 0.4914, Val: 0.8876, Test: 0.8832\n",
      "Epoch: 041, Loss: 0.4830, Val: 0.8948, Test: 0.8911\n",
      "Epoch: 042, Loss: 0.4730, Val: 0.9013, Test: 0.8961\n",
      "Epoch: 043, Loss: 0.4687, Val: 0.9071, Test: 0.9004\n",
      "Epoch: 044, Loss: 0.4658, Val: 0.9127, Test: 0.9046\n",
      "Epoch: 045, Loss: 0.4723, Val: 0.9162, Test: 0.9071\n",
      "Epoch: 046, Loss: 0.4605, Val: 0.9181, Test: 0.9081\n",
      "Epoch: 047, Loss: 0.4582, Val: 0.9194, Test: 0.9074\n",
      "Epoch: 048, Loss: 0.4659, Val: 0.9200, Test: 0.9071\n",
      "Epoch: 049, Loss: 0.4623, Val: 0.9209, Test: 0.9082\n",
      "Epoch: 050, Loss: 0.4701, Val: 0.9209, Test: 0.9100\n",
      "Epoch: 051, Loss: 0.4682, Val: 0.9199, Test: 0.9111\n",
      "Epoch: 052, Loss: 0.4626, Val: 0.9191, Test: 0.9103\n",
      "Epoch: 053, Loss: 0.4618, Val: 0.9173, Test: 0.9089\n",
      "Epoch: 054, Loss: 0.4599, Val: 0.9149, Test: 0.9056\n",
      "Epoch: 055, Loss: 0.4624, Val: 0.9136, Test: 0.9053\n",
      "Epoch: 056, Loss: 0.4538, Val: 0.9124, Test: 0.9055\n",
      "Epoch: 057, Loss: 0.4594, Val: 0.9112, Test: 0.9055\n",
      "Epoch: 058, Loss: 0.4585, Val: 0.9111, Test: 0.9048\n",
      "Epoch: 059, Loss: 0.4592, Val: 0.9124, Test: 0.9056\n",
      "Epoch: 060, Loss: 0.4523, Val: 0.9127, Test: 0.9068\n",
      "Epoch: 061, Loss: 0.4565, Val: 0.9141, Test: 0.9074\n",
      "Epoch: 062, Loss: 0.4553, Val: 0.9149, Test: 0.9076\n",
      "Epoch: 063, Loss: 0.4491, Val: 0.9162, Test: 0.9072\n",
      "Epoch: 064, Loss: 0.4547, Val: 0.9175, Test: 0.9086\n",
      "Epoch: 065, Loss: 0.4551, Val: 0.9182, Test: 0.9094\n",
      "Epoch: 066, Loss: 0.4559, Val: 0.9189, Test: 0.9092\n",
      "Epoch: 067, Loss: 0.4478, Val: 0.9187, Test: 0.9086\n",
      "Epoch: 068, Loss: 0.4548, Val: 0.9188, Test: 0.9085\n",
      "Epoch: 069, Loss: 0.4533, Val: 0.9197, Test: 0.9096\n",
      "Epoch: 070, Loss: 0.4540, Val: 0.9202, Test: 0.9113\n",
      "Epoch: 071, Loss: 0.4484, Val: 0.9202, Test: 0.9125\n",
      "Epoch: 072, Loss: 0.4495, Val: 0.9202, Test: 0.9126\n",
      "Epoch: 073, Loss: 0.4477, Val: 0.9195, Test: 0.9110\n",
      "Epoch: 074, Loss: 0.4520, Val: 0.9193, Test: 0.9102\n",
      "Epoch: 075, Loss: 0.4450, Val: 0.9200, Test: 0.9119\n",
      "Epoch: 076, Loss: 0.4395, Val: 0.9202, Test: 0.9134\n",
      "Epoch: 077, Loss: 0.4453, Val: 0.9199, Test: 0.9136\n",
      "Epoch: 078, Loss: 0.4530, Val: 0.9201, Test: 0.9132\n",
      "Epoch: 079, Loss: 0.4474, Val: 0.9200, Test: 0.9124\n",
      "Epoch: 080, Loss: 0.4445, Val: 0.9211, Test: 0.9127\n",
      "Epoch: 081, Loss: 0.4510, Val: 0.9216, Test: 0.9135\n",
      "Epoch: 082, Loss: 0.4487, Val: 0.9225, Test: 0.9142\n",
      "Epoch: 083, Loss: 0.4425, Val: 0.9232, Test: 0.9142\n",
      "Epoch: 084, Loss: 0.4448, Val: 0.9246, Test: 0.9144\n",
      "Epoch: 085, Loss: 0.4395, Val: 0.9252, Test: 0.9150\n",
      "Epoch: 086, Loss: 0.4429, Val: 0.9253, Test: 0.9160\n",
      "Epoch: 087, Loss: 0.4464, Val: 0.9258, Test: 0.9169\n",
      "Epoch: 088, Loss: 0.4387, Val: 0.9265, Test: 0.9169\n",
      "Epoch: 089, Loss: 0.4412, Val: 0.9265, Test: 0.9170\n",
      "Epoch: 090, Loss: 0.4485, Val: 0.9258, Test: 0.9177\n",
      "Epoch: 091, Loss: 0.4401, Val: 0.9248, Test: 0.9171\n",
      "Epoch: 092, Loss: 0.4360, Val: 0.9253, Test: 0.9170\n",
      "Epoch: 093, Loss: 0.4326, Val: 0.9261, Test: 0.9175\n",
      "Epoch: 094, Loss: 0.4410, Val: 0.9261, Test: 0.9174\n",
      "Epoch: 095, Loss: 0.4320, Val: 0.9262, Test: 0.9171\n",
      "Epoch: 096, Loss: 0.4436, Val: 0.9255, Test: 0.9162\n",
      "Epoch: 097, Loss: 0.4369, Val: 0.9254, Test: 0.9159\n",
      "Epoch: 098, Loss: 0.4343, Val: 0.9261, Test: 0.9159\n",
      "Epoch: 099, Loss: 0.4331, Val: 0.9262, Test: 0.9154\n",
      "Epoch: 100, Loss: 0.4374, Val: 0.9256, Test: 0.9148\n",
      "Final Test: 0.9169\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "dataset = Planetoid(\"./\", name='Cora', transform=transform)\n",
    "# After applying the `RandomLinkSplit` transform, the data is transformed from\n",
    "# a data object to a list of tuples (train_data, val_data, test_data), with\n",
    "# each element representing the corresponding split.\n",
    "train_data, val_data, test_data = dataset[0]\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(dataset.num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction on Senators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sofia/lase/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/sofia/lase/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/sofia/lase/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/sofia/lase/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sofia/lase/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/sofia/lase/lib/python3.10/site-packages/graspologic/models/edge_swaps.py:215: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  _edge_swap_numba = nb.jit(_edge_swap, nopython=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0., -1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]])\n",
      "Iteraciones:  16\n",
      "Loss:  tensor(131.9575)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.utils import stochastic_blockmodel_graph, to_dense_adj\n",
    "from graspologic.embed import AdjacencySpectralEmbed \n",
    "from models.RDPG_GD import GRDPG_GD_Armijo\n",
    "from models.GLASE_unshared_normalized import gLASE \n",
    "# from models.GLASE_unshared_normalized_v2 import gLASE_v2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "d = 4\n",
    "\n",
    "n_P1 = 100 # NUMERO DE SENADORES P1\n",
    "n_P2 = 100 # NUMERO DE SENADORES P2\n",
    "n_L1 = 200 # NUMERO DE LEYES P1\n",
    "n_L2 = 200 # NUMERO DE LEYES P2\n",
    "n_L3 = 60 # NUMERO DE LEYES NEUTRAS\n",
    "\n",
    "P1_L1 = 0.9 ## Votos de senadores del partido 1 a leyes grupo 1\n",
    "P1_L2 = 0.01 ## Votos de senadores del partido 1 a leyes grupo 2\n",
    "P1_L3 = 0.2 ## Votos de senadores del partido 1 a leyes grupo 3\n",
    "P2_L1 = 0.1 ## Votos de senadores del partido 2 a leyes grupo 1\n",
    "P2_L2 = 0.8 ## Votos de senadores del partido 2 a leyes grupo 2\n",
    "P2_L3 = 0.3 ## Votos de senadores del partido 2 a leyes grupo 3\n",
    "\n",
    "# n_P1 = 100\n",
    "# n_P2 = 100\n",
    "# n_L1 = 200\n",
    "# n_L2 = 200\n",
    "# n_L3 = 100\n",
    "\n",
    "# P1_L1 = 0.8\n",
    "# P1_L2 = 0.01\n",
    "# P1_L3 = 0.2 \n",
    "# P2_L1 = 0.01\n",
    "# P2_L2 = 0.5\n",
    "# P2_L3 = 0.8\n",
    "\n",
    "\n",
    "# n_party1 = 100\n",
    "# n_party2 = 100\n",
    "# l_party1 = 200\n",
    "# l_party2 = 200\n",
    "# lmix = 100\n",
    "# num_leyes = l_party1 + l_party2 + lmix\n",
    "# p_party1_party1 = 0.8\n",
    "# p_party1_party2 = 0.01\n",
    "# p_party1_mix = 0.2\n",
    "# p_party2_party1 = 0.01\n",
    "# p_party2_party2 = 0.5\n",
    "# p_party2_mix = 0.8\n",
    "\n",
    "\n",
    "p = [\n",
    "    [0, 0, P1_L1, P1_L2, P1_L3],\n",
    "    [0, 0, P2_L1, P2_L2, P2_L3],\n",
    "    [P1_L1, P2_L1, 0, 0, 0], \n",
    "    [P1_L2, P2_L2, 0, 0, 0], \n",
    "    [P1_L3, P2_L3, 0, 0, 0]\n",
    "    ]\n",
    "\n",
    "n = [n_P1, n_P2, n_L1, n_L2, n_L3]\n",
    "\n",
    "num_nodes = np.sum(n)\n",
    "edge_index = stochastic_blockmodel_graph(n, p)\n",
    "\n",
    "\n",
    "## MASK\n",
    "n_P1_np = 20\n",
    "n_P2_np = 10\n",
    "senadores_no_presentes = list(range(n_P1_np)) + list(range(n_P1,n_P1+n_P2_np))\n",
    "\n",
    "mask = torch.ones([num_nodes,num_nodes]).squeeze(0)\n",
    "for i in senadores_no_presentes:\n",
    "    votos = (torch.rand(1, num_nodes) < 0.7).int()\n",
    "    mask[i,:] = votos\n",
    "    mask[:,i] = votos\n",
    "\n",
    "\n",
    "## ASE \n",
    "adj_matrix = to_dense_adj(edge_index.to('cpu')).squeeze(0)\n",
    "ase = AdjacencySpectralEmbed(n_components=d, diag_aug=True, algorithm='full')\n",
    "masked_adj = adj_matrix*mask\n",
    "x_ase = ase.fit_transform(masked_adj.numpy())\n",
    "x_ase = torch.from_numpy(x_ase)\n",
    "\n",
    "A = to_dense_adj(edge_index.to('cpu'), max_num_nodes=num_nodes).squeeze(0)\n",
    "\n",
    "u, V = torch.linalg.eig(A)\n",
    "\n",
    "list_q=[]\n",
    "for i in range(d):\n",
    "    if u[i].numpy()>0:\n",
    "        list_q.append(1)\n",
    "    else:\n",
    "        list_q.append(-1)\n",
    "        \n",
    "# list_q.sort(reverse=True)\n",
    "q = torch.Tensor(list_q)\n",
    "Q=torch.diag(q)\n",
    "\n",
    "print(Q)\n",
    "\n",
    "\n",
    "torch.norm((x_ase@Q@x_ase.T - to_dense_adj(edge_index).squeeze(0))*mask)\n",
    "\n",
    "\n",
    "x_grdpg, cost, k  = GRDPG_GD_Armijo(x_ase, edge_index, Q, mask.nonzero().t().contiguous())\n",
    "x_grdpg = x_grdpg.detach()\n",
    "print(\"Iteraciones: \", k)\n",
    "print(\"Loss: \", torch.norm((x_grdpg@Q@x_grdpg.T - to_dense_adj(edge_index).squeeze(0))*to_dense_adj(mask.nonzero().t().contiguous()).squeeze(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vecinos = adj_matrix@torch.ones(num_nodes)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize =(15,5))\n",
    "axes[0].scatter(np.arange(n_P1),vecinos[:n_P1])\n",
    "axes[0].set_title('Vecinos senadores P1')\n",
    "axes[0].set_xlabel('# nodo')\n",
    "axes[0].set_ylabel('vecinos')\n",
    "\n",
    "axes[1].scatter(np.arange(n_P2),vecinos[n_P1:n_P1+n_P2])\n",
    "axes[1].set_title('Vecinos senadores P1')\n",
    "axes[1].set_xlabel('# nodo')\n",
    "axes[1].set_ylabel('vecinos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(460.3063, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(132.6590, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(132.2844, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(132.0934, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gd_steps = 20\n",
    "lr = 1e-2\n",
    "device = 'cuda'\n",
    "model = gLASE(d,d, gd_steps)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "epochs = 400\n",
    "\n",
    "## Initialization\n",
    "for step in range(gd_steps):\n",
    "    model.gd[step].lin1.weight.data = (torch.eye(d,d)*lr).to(device)#torch.nn.init.xavier_uniform_(model.gd[step].lin1.weight)*lr\n",
    "    model.gd[step].lin2.weight.data = (torch.eye(d,d)*lr).to(device)#torch.nn.init.xavier_uniform_(model.gd[step].lin2.weight)*lr\n",
    "    \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Define ATT mask\n",
    "edge_index_2 = torch.ones([num_nodes,num_nodes],).nonzero().t().contiguous().to(device)\n",
    "mask = mask.to(device)\n",
    "x_ase = x_ase.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "Q = Q.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x_ase, edge_index, edge_index_2, Q, mask.nonzero().t().contiguous())\n",
    "    loss = torch.norm((out@Q@out.T - to_dense_adj(edge_index).squeeze(0))*mask)\n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "\n",
    "    if epoch % 100 ==0:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(132.0460, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = torch.norm((out@Q@out.T - to_dense_adj(edge_index).squeeze(0))*mask)\n",
    "print(loss)\n",
    "x_glase = out.detach().to('cpu')\n",
    "x_ase = x_ase.to('cpu')\n",
    "\n",
    "from models.SVD_truncate import align_Xs\n",
    "x_grdpg = align_Xs(x_grdpg, x_ase)\n",
    "x_glase = align_Xs(x_glase, x_ase)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize =(15,5))\n",
    "axes[0].scatter(x_ase[n_P1_np:n_P1,0],x_ase[n_P1_np:n_P1,2], c='royalblue',marker='o',label='Party 1')\n",
    "axes[0].scatter(x_ase[:n_P1_np,0],x_ase[:n_P1_np,2], c='gold',marker='X',label='Not present (Party 1)')\n",
    "axes[0].scatter(x_ase[n_P1+n_P2_np:n_P1+n_P2,0],x_ase[n_P1+n_P2_np:n_P1+n_P2,2], c='firebrick',marker='o',label='Party 2')\n",
    "axes[0].scatter(x_ase[n_P1:n_P1+n_P2_np,0],x_ase[n_P1:n_P1+n_P2_np,2], c='limegreen',marker='X',label='Not present (Party 2)')\n",
    "axes[0].scatter(x_ase[n_P1+n_P2:n_P1+n_P2+n_L1,0],x_ase[n_P1+n_P2:n_P1+n_P2+n_L1,2], c='cornflowerblue',marker='^',label='Party 1 laws')\n",
    "axes[0].scatter(x_ase[n_P1+n_P2+n_L1:n_P1+n_P2+n_L1+n_L2,0],x_ase[n_P1+n_P2+n_L1:n_P1+n_P2+n_L1+n_L2,2],c='indianred',marker='^',label='Party 2 laws')\n",
    "axes[0].scatter(x_ase[n_P1+n_P2+n_L1+n_L2:,0],x_ase[n_P1+n_P2+n_L1+n_L2:,2],c='darkorange',marker='^',label='Bipartisan laws')\n",
    "axes[0].set_title('ASE')\n",
    "\n",
    "axes[1].scatter(x_grdpg[n_P1_np:n_P1,0],x_grdpg[n_P1_np:n_P1,2], c='royalblue',marker='o',label='Party 1')\n",
    "axes[1].scatter(x_grdpg[:n_P1_np,0],x_grdpg[:n_P1_np,2], c='gold',marker='X',label='Not present (Party 1)')\n",
    "axes[1].scatter(x_grdpg[n_P1+n_P2_np:n_P1+n_P2,0],x_grdpg[n_P1+n_P2_np:n_P1+n_P2,2], c='firebrick',marker='o',label='Party 2')\n",
    "axes[1].scatter(x_grdpg[n_P1:n_P1+n_P2_np,0],x_grdpg[n_P1:n_P1+n_P2_np,2], c='limegreen',marker='X',label='Not present (Party 2)')\n",
    "axes[1].scatter(x_grdpg[n_P1+n_P2:n_P1+n_P2+n_L1,0],x_grdpg[n_P1+n_P2:n_P1+n_P2+n_L1,2], c='cornflowerblue',marker='^',label='Party 1 laws')\n",
    "axes[1].scatter(x_grdpg[n_P1+n_P2+n_L1:n_P1+n_P2+n_L1+n_L2,0],x_grdpg[n_P1+n_P2+n_L1:n_P1+n_P2+n_L1+n_L2,2],c='indianred',marker='^',label='Party 2 laws')\n",
    "axes[1].scatter(x_grdpg[n_P1+n_P2+n_L1+n_L2:,0],x_grdpg[n_P1+n_P2+n_L1+n_L2:,2],c='darkorange',marker='^',label='Bipartisan laws')\n",
    "axes[1].set_title('GD')\n",
    "\n",
    "axes[2].scatter(x_glase[n_P1_np:n_P1,0],x_glase[n_P1_np:n_P1,2], c='royalblue',marker='o',label='Party 1')\n",
    "axes[2].scatter(x_glase[:n_P1_np,0],x_glase[:n_P1_np,2], c='gold',marker='X',label='Not present (Party 1)')\n",
    "axes[2].scatter(x_glase[n_P1+n_P2_np:n_P1+n_P2,0],x_glase[n_P1+n_P2_np:n_P1+n_P2,2], c='firebrick',marker='o',label='Party 2')\n",
    "axes[2].scatter(x_glase[n_P1:n_P1+n_P2_np,0],x_glase[n_P1:n_P1+n_P2_np,2], c='limegreen',marker='X',label='Not present (Party 2)')\n",
    "axes[2].scatter(x_glase[n_P1+n_P2:n_P1+n_P2+n_L1,0],x_glase[n_P1+n_P2:n_P1+n_P2+n_L1,2], c='cornflowerblue',marker='^',label='Party 1 laws')\n",
    "axes[2].scatter(x_glase[n_P1+n_P2+n_L1:n_P1+n_P2+n_L1+n_L2,0],x_glase[n_P1+n_P2+n_L1:n_P1+n_P2+n_L1+n_L2,2],c='indianred',marker='^',label='Party 2 laws')\n",
    "axes[2].scatter(x_glase[n_P1+n_P2+n_L1+n_L2:,0],x_glase[n_P1+n_P2+n_L1+n_L2:,2],c='darkorange',marker='^',label='Bipartisan laws')\n",
    "axes[2].set_title('LASE')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(-1, -0.07),fancybox=True, shadow=True, ncol=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([380, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Node features -- One hot encoding of label P1, P2, L1, L2, L3\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "labels = np.concatenate((np.ones(n_P1)*0,np.ones(n_P2), np.ones(n_L1)*2, np.ones(n_L2)*3, np.ones(n_L3)*4))\n",
    "labels = labels.tolist()\n",
    "labels = torch.tensor(labels).long()\n",
    "labels = F.one_hot(labels)\n",
    "\n",
    "\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Train, Val, Test\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "masked_edge_index = masked_adj.nonzero().t().contiguous()\n",
    "\n",
    "data = Data(x=labels.float(), x_ase=x_ase, x_glase=x_glase, edge_index=masked_edge_index)\n",
    "\n",
    "transform = T.Compose([\n",
    "    # T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6929, Val: 0.9109, Test: 0.8962\n",
      "Epoch: 002, Loss: 0.6478, Val: 0.9019, Test: 0.8843\n",
      "Epoch: 003, Loss: 0.6156, Val: 0.8797, Test: 0.8647\n",
      "Epoch: 004, Loss: 0.5787, Val: 0.8749, Test: 0.8593\n",
      "Epoch: 005, Loss: 0.5600, Val: 0.8679, Test: 0.8514\n",
      "Epoch: 006, Loss: 0.5605, Val: 0.8743, Test: 0.8544\n",
      "Epoch: 007, Loss: 0.5578, Val: 0.8743, Test: 0.8550\n",
      "Epoch: 008, Loss: 0.5525, Val: 0.8749, Test: 0.8539\n",
      "Epoch: 009, Loss: 0.5431, Val: 0.8783, Test: 0.8548\n",
      "Epoch: 010, Loss: 0.5413, Val: 0.8799, Test: 0.8563\n",
      "Epoch: 011, Loss: 0.5432, Val: 0.8802, Test: 0.8560\n",
      "Epoch: 012, Loss: 0.5417, Val: 0.8797, Test: 0.8549\n",
      "Epoch: 013, Loss: 0.5385, Val: 0.8801, Test: 0.8561\n",
      "Epoch: 014, Loss: 0.5393, Val: 0.8794, Test: 0.8570\n",
      "Epoch: 015, Loss: 0.5390, Val: 0.8795, Test: 0.8571\n",
      "Epoch: 016, Loss: 0.5385, Val: 0.8787, Test: 0.8565\n",
      "Epoch: 017, Loss: 0.5346, Val: 0.8772, Test: 0.8560\n",
      "Epoch: 018, Loss: 0.5390, Val: 0.8771, Test: 0.8560\n",
      "Epoch: 019, Loss: 0.5334, Val: 0.8769, Test: 0.8562\n",
      "Epoch: 020, Loss: 0.5331, Val: 0.8770, Test: 0.8558\n",
      "Epoch: 021, Loss: 0.5380, Val: 0.8774, Test: 0.8554\n",
      "Epoch: 022, Loss: 0.5353, Val: 0.8778, Test: 0.8553\n",
      "Epoch: 023, Loss: 0.5301, Val: 0.8779, Test: 0.8555\n",
      "Epoch: 024, Loss: 0.5404, Val: 0.8775, Test: 0.8561\n",
      "Epoch: 025, Loss: 0.5368, Val: 0.8776, Test: 0.8565\n",
      "Epoch: 026, Loss: 0.5398, Val: 0.8781, Test: 0.8561\n",
      "Epoch: 027, Loss: 0.5317, Val: 0.8784, Test: 0.8563\n",
      "Epoch: 028, Loss: 0.5440, Val: 0.8778, Test: 0.8562\n",
      "Epoch: 029, Loss: 0.5438, Val: 0.8773, Test: 0.8578\n",
      "Epoch: 030, Loss: 0.5330, Val: 0.8770, Test: 0.8583\n",
      "Epoch: 031, Loss: 0.5298, Val: 0.8772, Test: 0.8581\n",
      "Epoch: 032, Loss: 0.5288, Val: 0.8772, Test: 0.8573\n",
      "Epoch: 033, Loss: 0.5283, Val: 0.8778, Test: 0.8561\n",
      "Epoch: 034, Loss: 0.5362, Val: 0.8791, Test: 0.8563\n",
      "Epoch: 035, Loss: 0.5446, Val: 0.8793, Test: 0.8563\n",
      "Epoch: 036, Loss: 0.5361, Val: 0.8795, Test: 0.8561\n",
      "Epoch: 037, Loss: 0.5354, Val: 0.8789, Test: 0.8562\n",
      "Epoch: 038, Loss: 0.5362, Val: 0.8789, Test: 0.8560\n",
      "Epoch: 039, Loss: 0.5233, Val: 0.8792, Test: 0.8560\n",
      "Epoch: 040, Loss: 0.5295, Val: 0.8792, Test: 0.8561\n",
      "Epoch: 041, Loss: 0.5296, Val: 0.8790, Test: 0.8563\n",
      "Epoch: 042, Loss: 0.5384, Val: 0.8781, Test: 0.8561\n",
      "Epoch: 043, Loss: 0.5328, Val: 0.8775, Test: 0.8569\n",
      "Epoch: 044, Loss: 0.5279, Val: 0.8778, Test: 0.8576\n",
      "Epoch: 045, Loss: 0.5342, Val: 0.8779, Test: 0.8579\n",
      "Epoch: 046, Loss: 0.5330, Val: 0.8775, Test: 0.8575\n",
      "Epoch: 047, Loss: 0.5288, Val: 0.8782, Test: 0.8564\n",
      "Epoch: 048, Loss: 0.5269, Val: 0.8791, Test: 0.8565\n",
      "Epoch: 049, Loss: 0.5424, Val: 0.8788, Test: 0.8567\n",
      "Epoch: 050, Loss: 0.5304, Val: 0.8789, Test: 0.8575\n",
      "Epoch: 051, Loss: 0.5255, Val: 0.8786, Test: 0.8582\n",
      "Epoch: 052, Loss: 0.5323, Val: 0.8790, Test: 0.8577\n",
      "Epoch: 053, Loss: 0.5257, Val: 0.8795, Test: 0.8570\n",
      "Epoch: 054, Loss: 0.5253, Val: 0.8795, Test: 0.8568\n",
      "Epoch: 055, Loss: 0.5313, Val: 0.8788, Test: 0.8577\n",
      "Epoch: 056, Loss: 0.5347, Val: 0.8786, Test: 0.8580\n",
      "Epoch: 057, Loss: 0.5330, Val: 0.8785, Test: 0.8572\n",
      "Epoch: 058, Loss: 0.5198, Val: 0.8787, Test: 0.8574\n",
      "Epoch: 059, Loss: 0.5309, Val: 0.8786, Test: 0.8579\n",
      "Epoch: 060, Loss: 0.5324, Val: 0.8786, Test: 0.8581\n",
      "Epoch: 061, Loss: 0.5370, Val: 0.8786, Test: 0.8583\n",
      "Epoch: 062, Loss: 0.5334, Val: 0.8787, Test: 0.8580\n",
      "Epoch: 063, Loss: 0.5297, Val: 0.8788, Test: 0.8580\n",
      "Epoch: 064, Loss: 0.5295, Val: 0.8795, Test: 0.8580\n",
      "Epoch: 065, Loss: 0.5334, Val: 0.8796, Test: 0.8591\n",
      "Epoch: 066, Loss: 0.5301, Val: 0.8798, Test: 0.8596\n",
      "Epoch: 067, Loss: 0.5228, Val: 0.8799, Test: 0.8594\n",
      "Epoch: 068, Loss: 0.5310, Val: 0.8797, Test: 0.8588\n",
      "Epoch: 069, Loss: 0.5240, Val: 0.8799, Test: 0.8585\n",
      "Epoch: 070, Loss: 0.5256, Val: 0.8801, Test: 0.8597\n",
      "Epoch: 071, Loss: 0.5287, Val: 0.8802, Test: 0.8599\n",
      "Epoch: 072, Loss: 0.5340, Val: 0.8802, Test: 0.8596\n",
      "Epoch: 073, Loss: 0.5312, Val: 0.8802, Test: 0.8588\n",
      "Epoch: 074, Loss: 0.5283, Val: 0.8804, Test: 0.8590\n",
      "Epoch: 075, Loss: 0.5266, Val: 0.8805, Test: 0.8608\n",
      "Epoch: 076, Loss: 0.5257, Val: 0.8809, Test: 0.8616\n",
      "Epoch: 077, Loss: 0.5262, Val: 0.8805, Test: 0.8614\n",
      "Epoch: 078, Loss: 0.5240, Val: 0.8802, Test: 0.8601\n",
      "Epoch: 079, Loss: 0.5280, Val: 0.8799, Test: 0.8596\n",
      "Epoch: 080, Loss: 0.5261, Val: 0.8802, Test: 0.8598\n",
      "Epoch: 081, Loss: 0.5272, Val: 0.8806, Test: 0.8608\n",
      "Epoch: 082, Loss: 0.5239, Val: 0.8813, Test: 0.8615\n",
      "Epoch: 083, Loss: 0.5284, Val: 0.8819, Test: 0.8616\n",
      "Epoch: 084, Loss: 0.5298, Val: 0.8813, Test: 0.8608\n",
      "Epoch: 085, Loss: 0.5289, Val: 0.8816, Test: 0.8620\n",
      "Epoch: 086, Loss: 0.5285, Val: 0.8808, Test: 0.8625\n",
      "Epoch: 087, Loss: 0.5351, Val: 0.8810, Test: 0.8617\n",
      "Epoch: 088, Loss: 0.5304, Val: 0.8809, Test: 0.8614\n",
      "Epoch: 089, Loss: 0.5308, Val: 0.8812, Test: 0.8624\n",
      "Epoch: 090, Loss: 0.5197, Val: 0.8814, Test: 0.8636\n",
      "Epoch: 091, Loss: 0.5223, Val: 0.8808, Test: 0.8636\n",
      "Epoch: 092, Loss: 0.5203, Val: 0.8813, Test: 0.8628\n",
      "Epoch: 093, Loss: 0.5155, Val: 0.8808, Test: 0.8612\n",
      "Epoch: 094, Loss: 0.5307, Val: 0.8810, Test: 0.8610\n",
      "Epoch: 095, Loss: 0.5235, Val: 0.8822, Test: 0.8622\n",
      "Epoch: 096, Loss: 0.5166, Val: 0.8829, Test: 0.8636\n",
      "Epoch: 097, Loss: 0.5228, Val: 0.8841, Test: 0.8630\n",
      "Epoch: 098, Loss: 0.5282, Val: 0.8832, Test: 0.8619\n",
      "Epoch: 099, Loss: 0.5355, Val: 0.8827, Test: 0.8621\n",
      "Epoch: 100, Loss: 0.5212, Val: 0.8806, Test: 0.8618\n",
      "Final Test: 0.8962\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(5, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict on entire masked graph\n",
    "\n",
    "z = model.encode(data.x, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 230])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix[senadores_no_presentes][:,n_P1+n_P2:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8604)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / 48 / 230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6901, Val: 0.8946, Test: 0.8835\n",
      "Epoch: 002, Loss: 0.6330, Val: 0.8840, Test: 0.8689\n",
      "Epoch: 003, Loss: 0.5941, Val: 0.8782, Test: 0.8613\n",
      "Epoch: 004, Loss: 0.5537, Val: 0.8779, Test: 0.8574\n",
      "Epoch: 005, Loss: 0.5509, Val: 0.8788, Test: 0.8562\n",
      "Epoch: 006, Loss: 0.5619, Val: 0.8800, Test: 0.8563\n",
      "Epoch: 007, Loss: 0.5363, Val: 0.8796, Test: 0.8552\n",
      "Epoch: 008, Loss: 0.5334, Val: 0.8782, Test: 0.8540\n",
      "Epoch: 009, Loss: 0.5376, Val: 0.8797, Test: 0.8558\n",
      "Epoch: 010, Loss: 0.5494, Val: 0.8797, Test: 0.8570\n",
      "Epoch: 011, Loss: 0.5422, Val: 0.8789, Test: 0.8572\n",
      "Epoch: 012, Loss: 0.5431, Val: 0.8784, Test: 0.8570\n",
      "Epoch: 013, Loss: 0.5417, Val: 0.8780, Test: 0.8573\n",
      "Epoch: 014, Loss: 0.5347, Val: 0.8786, Test: 0.8566\n",
      "Epoch: 015, Loss: 0.5312, Val: 0.8794, Test: 0.8560\n",
      "Epoch: 016, Loss: 0.5382, Val: 0.8796, Test: 0.8557\n",
      "Epoch: 017, Loss: 0.5344, Val: 0.8797, Test: 0.8557\n",
      "Epoch: 018, Loss: 0.5309, Val: 0.8791, Test: 0.8561\n",
      "Epoch: 019, Loss: 0.5350, Val: 0.8791, Test: 0.8569\n",
      "Epoch: 020, Loss: 0.5313, Val: 0.8788, Test: 0.8584\n",
      "Epoch: 021, Loss: 0.5382, Val: 0.8785, Test: 0.8589\n",
      "Epoch: 022, Loss: 0.5361, Val: 0.8787, Test: 0.8581\n",
      "Epoch: 023, Loss: 0.5350, Val: 0.8790, Test: 0.8587\n",
      "Epoch: 024, Loss: 0.5335, Val: 0.8791, Test: 0.8582\n",
      "Epoch: 025, Loss: 0.5298, Val: 0.8792, Test: 0.8574\n",
      "Epoch: 026, Loss: 0.5220, Val: 0.8800, Test: 0.8566\n",
      "Epoch: 027, Loss: 0.5341, Val: 0.8804, Test: 0.8565\n",
      "Epoch: 028, Loss: 0.5307, Val: 0.8806, Test: 0.8567\n",
      "Epoch: 029, Loss: 0.5305, Val: 0.8801, Test: 0.8566\n",
      "Epoch: 030, Loss: 0.5306, Val: 0.8786, Test: 0.8582\n",
      "Epoch: 031, Loss: 0.5318, Val: 0.8788, Test: 0.8584\n",
      "Epoch: 032, Loss: 0.5314, Val: 0.8792, Test: 0.8571\n",
      "Epoch: 033, Loss: 0.5351, Val: 0.8793, Test: 0.8569\n",
      "Epoch: 034, Loss: 0.5331, Val: 0.8791, Test: 0.8572\n",
      "Epoch: 035, Loss: 0.5346, Val: 0.8790, Test: 0.8586\n",
      "Epoch: 036, Loss: 0.5275, Val: 0.8788, Test: 0.8592\n",
      "Epoch: 037, Loss: 0.5405, Val: 0.8790, Test: 0.8594\n",
      "Epoch: 038, Loss: 0.5290, Val: 0.8791, Test: 0.8596\n",
      "Epoch: 039, Loss: 0.5273, Val: 0.8791, Test: 0.8594\n",
      "Epoch: 040, Loss: 0.5312, Val: 0.8792, Test: 0.8586\n",
      "Epoch: 041, Loss: 0.5316, Val: 0.8796, Test: 0.8582\n",
      "Epoch: 042, Loss: 0.5239, Val: 0.8797, Test: 0.8591\n",
      "Epoch: 043, Loss: 0.5304, Val: 0.8796, Test: 0.8592\n",
      "Epoch: 044, Loss: 0.5356, Val: 0.8800, Test: 0.8579\n",
      "Epoch: 045, Loss: 0.5250, Val: 0.8801, Test: 0.8573\n",
      "Epoch: 046, Loss: 0.5331, Val: 0.8799, Test: 0.8573\n",
      "Epoch: 047, Loss: 0.5266, Val: 0.8797, Test: 0.8579\n",
      "Epoch: 048, Loss: 0.5247, Val: 0.8796, Test: 0.8591\n",
      "Epoch: 049, Loss: 0.5242, Val: 0.8797, Test: 0.8592\n",
      "Epoch: 050, Loss: 0.5220, Val: 0.8796, Test: 0.8584\n",
      "Epoch: 051, Loss: 0.5364, Val: 0.8795, Test: 0.8586\n",
      "Epoch: 052, Loss: 0.5300, Val: 0.8797, Test: 0.8597\n",
      "Epoch: 053, Loss: 0.5324, Val: 0.8791, Test: 0.8602\n",
      "Epoch: 054, Loss: 0.5366, Val: 0.8793, Test: 0.8604\n",
      "Epoch: 055, Loss: 0.5247, Val: 0.8798, Test: 0.8609\n",
      "Epoch: 056, Loss: 0.5284, Val: 0.8804, Test: 0.8612\n",
      "Epoch: 057, Loss: 0.5327, Val: 0.8807, Test: 0.8616\n",
      "Epoch: 058, Loss: 0.5316, Val: 0.8808, Test: 0.8614\n",
      "Epoch: 059, Loss: 0.5232, Val: 0.8807, Test: 0.8609\n",
      "Epoch: 060, Loss: 0.5294, Val: 0.8806, Test: 0.8606\n",
      "Epoch: 061, Loss: 0.5282, Val: 0.8806, Test: 0.8607\n",
      "Epoch: 062, Loss: 0.5271, Val: 0.8810, Test: 0.8612\n",
      "Epoch: 063, Loss: 0.5349, Val: 0.8814, Test: 0.8618\n",
      "Epoch: 064, Loss: 0.5271, Val: 0.8819, Test: 0.8625\n",
      "Epoch: 065, Loss: 0.5228, Val: 0.8816, Test: 0.8629\n",
      "Epoch: 066, Loss: 0.5265, Val: 0.8817, Test: 0.8624\n",
      "Epoch: 067, Loss: 0.5199, Val: 0.8810, Test: 0.8617\n",
      "Epoch: 068, Loss: 0.5170, Val: 0.8813, Test: 0.8616\n",
      "Epoch: 069, Loss: 0.5268, Val: 0.8797, Test: 0.8613\n",
      "Epoch: 070, Loss: 0.5305, Val: 0.8808, Test: 0.8618\n",
      "Epoch: 071, Loss: 0.5234, Val: 0.8807, Test: 0.8617\n",
      "Epoch: 072, Loss: 0.5255, Val: 0.8817, Test: 0.8634\n",
      "Epoch: 073, Loss: 0.5238, Val: 0.8815, Test: 0.8637\n",
      "Epoch: 074, Loss: 0.5274, Val: 0.8802, Test: 0.8629\n",
      "Epoch: 075, Loss: 0.5245, Val: 0.8800, Test: 0.8630\n",
      "Epoch: 076, Loss: 0.5143, Val: 0.8807, Test: 0.8635\n",
      "Epoch: 077, Loss: 0.5207, Val: 0.8818, Test: 0.8644\n",
      "Epoch: 078, Loss: 0.5220, Val: 0.8832, Test: 0.8646\n",
      "Epoch: 079, Loss: 0.5147, Val: 0.8836, Test: 0.8646\n",
      "Epoch: 080, Loss: 0.5251, Val: 0.8831, Test: 0.8633\n",
      "Epoch: 081, Loss: 0.5208, Val: 0.8831, Test: 0.8626\n",
      "Epoch: 082, Loss: 0.5243, Val: 0.8822, Test: 0.8635\n",
      "Epoch: 083, Loss: 0.5204, Val: 0.8799, Test: 0.8632\n",
      "Epoch: 084, Loss: 0.5232, Val: 0.8792, Test: 0.8624\n",
      "Epoch: 085, Loss: 0.5259, Val: 0.8796, Test: 0.8625\n",
      "Epoch: 086, Loss: 0.5244, Val: 0.8802, Test: 0.8629\n",
      "Epoch: 087, Loss: 0.5223, Val: 0.8809, Test: 0.8645\n",
      "Epoch: 088, Loss: 0.5291, Val: 0.8821, Test: 0.8652\n",
      "Epoch: 089, Loss: 0.5179, Val: 0.8829, Test: 0.8653\n",
      "Epoch: 090, Loss: 0.5158, Val: 0.8831, Test: 0.8645\n",
      "Epoch: 091, Loss: 0.5170, Val: 0.8834, Test: 0.8645\n",
      "Epoch: 092, Loss: 0.5143, Val: 0.8837, Test: 0.8650\n",
      "Epoch: 093, Loss: 0.5230, Val: 0.8816, Test: 0.8624\n",
      "Epoch: 094, Loss: 0.5279, Val: 0.8794, Test: 0.8613\n",
      "Epoch: 095, Loss: 0.5154, Val: 0.8801, Test: 0.8630\n",
      "Epoch: 096, Loss: 0.5202, Val: 0.8812, Test: 0.8649\n",
      "Epoch: 097, Loss: 0.5269, Val: 0.8806, Test: 0.8634\n",
      "Epoch: 098, Loss: 0.5188, Val: 0.8803, Test: 0.8618\n",
      "Epoch: 099, Loss: 0.5252, Val: 0.8811, Test: 0.8628\n",
      "Epoch: 100, Loss: 0.5159, Val: 0.8827, Test: 0.8650\n",
      "Final Test: 0.8835\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_ase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_ase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8276)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / 48 / 230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLASE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.9126, Val: 0.7181, Test: 0.6946\n",
      "Epoch: 002, Loss: 0.6863, Val: 0.7589, Test: 0.7235\n",
      "Epoch: 003, Loss: 0.6503, Val: 0.7725, Test: 0.7417\n",
      "Epoch: 004, Loss: 0.6319, Val: 0.8122, Test: 0.7988\n",
      "Epoch: 005, Loss: 0.6119, Val: 0.8434, Test: 0.8359\n",
      "Epoch: 006, Loss: 0.5813, Val: 0.8629, Test: 0.8494\n",
      "Epoch: 007, Loss: 0.5719, Val: 0.8715, Test: 0.8499\n",
      "Epoch: 008, Loss: 0.5638, Val: 0.8738, Test: 0.8505\n",
      "Epoch: 009, Loss: 0.5510, Val: 0.8760, Test: 0.8530\n",
      "Epoch: 010, Loss: 0.5423, Val: 0.8796, Test: 0.8566\n",
      "Epoch: 011, Loss: 0.5639, Val: 0.8812, Test: 0.8583\n",
      "Epoch: 012, Loss: 0.5478, Val: 0.8813, Test: 0.8579\n",
      "Epoch: 013, Loss: 0.5420, Val: 0.8809, Test: 0.8573\n",
      "Epoch: 014, Loss: 0.5383, Val: 0.8801, Test: 0.8570\n",
      "Epoch: 015, Loss: 0.5342, Val: 0.8792, Test: 0.8572\n",
      "Epoch: 016, Loss: 0.5366, Val: 0.8786, Test: 0.8566\n",
      "Epoch: 017, Loss: 0.5367, Val: 0.8778, Test: 0.8558\n",
      "Epoch: 018, Loss: 0.5422, Val: 0.8770, Test: 0.8547\n",
      "Epoch: 019, Loss: 0.5379, Val: 0.8755, Test: 0.8547\n",
      "Epoch: 020, Loss: 0.5311, Val: 0.8767, Test: 0.8555\n",
      "Epoch: 021, Loss: 0.5361, Val: 0.8776, Test: 0.8560\n",
      "Epoch: 022, Loss: 0.5343, Val: 0.8776, Test: 0.8566\n",
      "Epoch: 023, Loss: 0.5402, Val: 0.8771, Test: 0.8570\n",
      "Epoch: 024, Loss: 0.5436, Val: 0.8767, Test: 0.8577\n",
      "Epoch: 025, Loss: 0.5359, Val: 0.8769, Test: 0.8572\n",
      "Epoch: 026, Loss: 0.5385, Val: 0.8770, Test: 0.8568\n",
      "Epoch: 027, Loss: 0.5224, Val: 0.8774, Test: 0.8567\n",
      "Epoch: 028, Loss: 0.5303, Val: 0.8781, Test: 0.8564\n",
      "Epoch: 029, Loss: 0.5319, Val: 0.8783, Test: 0.8566\n",
      "Epoch: 030, Loss: 0.5310, Val: 0.8783, Test: 0.8566\n",
      "Epoch: 031, Loss: 0.5301, Val: 0.8786, Test: 0.8565\n",
      "Epoch: 032, Loss: 0.5348, Val: 0.8790, Test: 0.8569\n",
      "Epoch: 033, Loss: 0.5407, Val: 0.8791, Test: 0.8571\n",
      "Epoch: 034, Loss: 0.5422, Val: 0.8788, Test: 0.8570\n",
      "Epoch: 035, Loss: 0.5349, Val: 0.8781, Test: 0.8571\n",
      "Epoch: 036, Loss: 0.5328, Val: 0.8777, Test: 0.8586\n",
      "Epoch: 037, Loss: 0.5291, Val: 0.8771, Test: 0.8589\n",
      "Epoch: 038, Loss: 0.5271, Val: 0.8777, Test: 0.8588\n",
      "Epoch: 039, Loss: 0.5292, Val: 0.8783, Test: 0.8572\n",
      "Epoch: 040, Loss: 0.5337, Val: 0.8790, Test: 0.8571\n",
      "Epoch: 041, Loss: 0.5316, Val: 0.8787, Test: 0.8567\n",
      "Epoch: 042, Loss: 0.5237, Val: 0.8776, Test: 0.8575\n",
      "Epoch: 043, Loss: 0.5331, Val: 0.8775, Test: 0.8578\n",
      "Epoch: 044, Loss: 0.5267, Val: 0.8777, Test: 0.8573\n",
      "Epoch: 045, Loss: 0.5383, Val: 0.8783, Test: 0.8565\n",
      "Epoch: 046, Loss: 0.5353, Val: 0.8789, Test: 0.8568\n",
      "Epoch: 047, Loss: 0.5307, Val: 0.8789, Test: 0.8567\n",
      "Epoch: 048, Loss: 0.5340, Val: 0.8789, Test: 0.8567\n",
      "Epoch: 049, Loss: 0.5362, Val: 0.8784, Test: 0.8568\n",
      "Epoch: 050, Loss: 0.5325, Val: 0.8787, Test: 0.8568\n",
      "Epoch: 051, Loss: 0.5310, Val: 0.8786, Test: 0.8569\n",
      "Epoch: 052, Loss: 0.5291, Val: 0.8783, Test: 0.8569\n",
      "Epoch: 053, Loss: 0.5382, Val: 0.8782, Test: 0.8570\n",
      "Epoch: 054, Loss: 0.5282, Val: 0.8784, Test: 0.8571\n",
      "Epoch: 055, Loss: 0.5343, Val: 0.8785, Test: 0.8573\n",
      "Epoch: 056, Loss: 0.5314, Val: 0.8784, Test: 0.8575\n",
      "Epoch: 057, Loss: 0.5340, Val: 0.8790, Test: 0.8571\n",
      "Epoch: 058, Loss: 0.5308, Val: 0.8791, Test: 0.8571\n",
      "Epoch: 059, Loss: 0.5273, Val: 0.8787, Test: 0.8570\n",
      "Epoch: 060, Loss: 0.5319, Val: 0.8781, Test: 0.8577\n",
      "Epoch: 061, Loss: 0.5373, Val: 0.8781, Test: 0.8578\n",
      "Epoch: 062, Loss: 0.5360, Val: 0.8786, Test: 0.8572\n",
      "Epoch: 063, Loss: 0.5332, Val: 0.8791, Test: 0.8571\n",
      "Epoch: 064, Loss: 0.5379, Val: 0.8783, Test: 0.8580\n",
      "Epoch: 065, Loss: 0.5338, Val: 0.8781, Test: 0.8587\n",
      "Epoch: 066, Loss: 0.5273, Val: 0.8779, Test: 0.8583\n",
      "Epoch: 067, Loss: 0.5360, Val: 0.8785, Test: 0.8577\n",
      "Epoch: 068, Loss: 0.5270, Val: 0.8785, Test: 0.8577\n",
      "Epoch: 069, Loss: 0.5221, Val: 0.8789, Test: 0.8585\n",
      "Epoch: 070, Loss: 0.5288, Val: 0.8787, Test: 0.8584\n",
      "Epoch: 071, Loss: 0.5269, Val: 0.8791, Test: 0.8580\n",
      "Epoch: 072, Loss: 0.5341, Val: 0.8796, Test: 0.8573\n",
      "Epoch: 073, Loss: 0.5324, Val: 0.8792, Test: 0.8572\n",
      "Epoch: 074, Loss: 0.5296, Val: 0.8787, Test: 0.8578\n",
      "Epoch: 075, Loss: 0.5302, Val: 0.8785, Test: 0.8592\n",
      "Epoch: 076, Loss: 0.5286, Val: 0.8787, Test: 0.8595\n",
      "Epoch: 077, Loss: 0.5226, Val: 0.8785, Test: 0.8585\n",
      "Epoch: 078, Loss: 0.5316, Val: 0.8796, Test: 0.8578\n",
      "Epoch: 079, Loss: 0.5327, Val: 0.8803, Test: 0.8576\n",
      "Epoch: 080, Loss: 0.5304, Val: 0.8803, Test: 0.8580\n",
      "Epoch: 081, Loss: 0.5317, Val: 0.8795, Test: 0.8584\n",
      "Epoch: 082, Loss: 0.5325, Val: 0.8794, Test: 0.8593\n",
      "Epoch: 083, Loss: 0.5287, Val: 0.8788, Test: 0.8592\n",
      "Epoch: 084, Loss: 0.5304, Val: 0.8784, Test: 0.8587\n",
      "Epoch: 085, Loss: 0.5295, Val: 0.8782, Test: 0.8593\n",
      "Epoch: 086, Loss: 0.5305, Val: 0.8787, Test: 0.8597\n",
      "Epoch: 087, Loss: 0.5366, Val: 0.8788, Test: 0.8600\n",
      "Epoch: 088, Loss: 0.5347, Val: 0.8793, Test: 0.8600\n",
      "Epoch: 089, Loss: 0.5336, Val: 0.8802, Test: 0.8604\n",
      "Epoch: 090, Loss: 0.5269, Val: 0.8808, Test: 0.8598\n",
      "Epoch: 091, Loss: 0.5237, Val: 0.8807, Test: 0.8597\n",
      "Epoch: 092, Loss: 0.5339, Val: 0.8800, Test: 0.8604\n",
      "Epoch: 093, Loss: 0.5296, Val: 0.8791, Test: 0.8608\n",
      "Epoch: 094, Loss: 0.5235, Val: 0.8788, Test: 0.8606\n",
      "Epoch: 095, Loss: 0.5293, Val: 0.8791, Test: 0.8601\n",
      "Epoch: 096, Loss: 0.5308, Val: 0.8793, Test: 0.8595\n",
      "Epoch: 097, Loss: 0.5215, Val: 0.8808, Test: 0.8598\n",
      "Epoch: 098, Loss: 0.5259, Val: 0.8820, Test: 0.8608\n",
      "Epoch: 099, Loss: 0.5331, Val: 0.8821, Test: 0.8615\n",
      "Epoch: 100, Loss: 0.5294, Val: 0.8812, Test: 0.8612\n",
      "Final Test: 0.8615\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_glase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_glase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8653)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / 48 / 230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904],\n",
       "        [0.6009, 0.2566, 0.7936, 0.9408, 0.1332],\n",
       "        [0.9346, 0.5936, 0.8694, 0.5677, 0.7411],\n",
       "        ...,\n",
       "        [0.7899, 0.6379, 0.8041, 0.8159, 0.7029],\n",
       "        [0.3492, 0.3597, 0.6947, 0.7725, 0.4733],\n",
       "        [0.7506, 0.1992, 0.7818, 0.4386, 0.1559]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "random_features=torch.rand([num_nodes, 5])\n",
    "random_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Train, Val, Test\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "masked_edge_index = masked_adj.nonzero().t().contiguous()\n",
    "\n",
    "data = Data(x=random_features.float(), x_ase=x_ase, x_glase=x_glase, x_grdpg=x_grdpg, edge_index=masked_edge_index)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.01, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 0.6968, Val: 0.7353, Test: 0.7408\n",
      "Epoch: 040, Loss: 0.6977, Val: 0.7527, Test: 0.7565\n",
      "Epoch: 060, Loss: 0.6993, Val: 0.7550, Test: 0.7624\n",
      "Epoch: 080, Loss: 0.7014, Val: 0.7556, Test: 0.7628\n",
      "Epoch: 100, Loss: 0.7015, Val: 0.7562, Test: 0.7632\n",
      "Epoch: 020, Loss: 0.6270, Val: 0.8142, Test: 0.8006\n",
      "Epoch: 040, Loss: 0.6221, Val: 0.8175, Test: 0.8042\n",
      "Epoch: 060, Loss: 0.6192, Val: 0.8186, Test: 0.8054\n",
      "Epoch: 080, Loss: 0.6195, Val: 0.8187, Test: 0.8055\n",
      "Epoch: 100, Loss: 0.6184, Val: 0.8192, Test: 0.8060\n",
      "Epoch: 020, Loss: 0.7169, Val: 0.7744, Test: 0.7721\n",
      "Epoch: 040, Loss: 0.7178, Val: 0.7747, Test: 0.7723\n",
      "Epoch: 060, Loss: 0.7133, Val: 0.7747, Test: 0.7723\n",
      "Epoch: 080, Loss: 0.7076, Val: 0.7753, Test: 0.7732\n",
      "Epoch: 100, Loss: 0.7064, Val: 0.7754, Test: 0.7735\n",
      "Epoch: 020, Loss: 0.6547, Val: 0.7603, Test: 0.7398\n",
      "Epoch: 040, Loss: 0.6468, Val: 0.7711, Test: 0.7521\n",
      "Epoch: 060, Loss: 0.6475, Val: 0.7733, Test: 0.7557\n",
      "Epoch: 080, Loss: 0.6448, Val: 0.7762, Test: 0.7585\n",
      "Epoch: 100, Loss: 0.6425, Val: 0.7795, Test: 0.7623\n",
      "GCN acc: 0.0000, ASE acc: 0.4741, GRDPG acc: 0.5175, GLASE acc: 0.4807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Net(\n",
       "   (conv1): GCNConv(5, 128)\n",
       "   (conv2): GCNConv(128, 64)\n",
       " ),\n",
       " Net(\n",
       "   (conv1): GCNConv(9, 128)\n",
       "   (conv2): GCNConv(128, 64)\n",
       " ),\n",
       " Net(\n",
       "   (conv1): GCNConv(9, 128)\n",
       "   (conv2): GCNConv(128, 64)\n",
       " ),\n",
       " Net(\n",
       "   (conv1): GCNConv(9, 128)\n",
       "   (conv2): GCNConv(128, 64)\n",
       " ),\n",
       " tensor(0.),\n",
       " tensor(0.4741),\n",
       " tensor(0.5175),\n",
       " tensor(0.4807))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from training.run_link_prediction import link_prediction\n",
    "\n",
    "inverted_mask_matrix = (torch.ones([num_nodes,num_nodes]).squeeze(0) - mask.to('cpu')) \n",
    "link_prediction(data.edge_index, mask, inverted_mask_matrix, data, 5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 0.6989, Val: 0.7367, Test: 0.7576\n",
      "Epoch: 040, Loss: 0.6957, Val: 0.7484, Test: 0.7700\n",
      "Epoch: 060, Loss: 0.6962, Val: 0.7492, Test: 0.7704\n",
      "Epoch: 080, Loss: 0.6970, Val: 0.7496, Test: 0.7707\n",
      "Epoch: 100, Loss: 0.6965, Val: 0.7488, Test: 0.7696\n",
      "Final Test: 0.7734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4229)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from models.link_prediction import Net\n",
    "\n",
    "\n",
    "model = Net(5, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# def train():\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "#     # We perform a new round of negative sampling for every training epoch:\n",
    "#     neg_edge_index = negative_sampling(\n",
    "#         edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "#         num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "#     edge_label_index = torch.cat(\n",
    "#         [train_data.edge_label_index, neg_edge_index],\n",
    "#         dim=-1,\n",
    "#     )\n",
    "#     edge_label = torch.cat([\n",
    "#         train_data.edge_label,\n",
    "#         train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "#     ], dim=0)\n",
    "\n",
    "#     out = model.decode(z, edge_label_index).view(-1)\n",
    "#     loss = criterion(out, edge_label)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return loss\n",
    "\n",
    "\n",
    "def train(x_input, train_data, model):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x_input, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(train_data.x, train_data, model)\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "            f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "## Predict on entire masked graph\n",
    "\n",
    "z = model.encode(data.x, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / len(senadores_no_presentes) / (n_L1 + n_L2 + n_L3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2999)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_mask_matrix = (torch.ones([num_nodes,num_nodes]).squeeze(0) - mask.to('cpu')) \n",
    "indices = torch.where(inverted_mask_matrix)\n",
    "((adj_matrix[indices[0], indices[1]]) == (predicted_adj[indices[0], indices[1]])).sum() / indices[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2999)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.link_prediction import train_link_prediction, eval_link_prediction\n",
    "eval_link_prediction(data.x, data.edge_index, model, adj_matrix, inverted_mask_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2999)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.link_prediction import train_link_prediction, eval_link_prediction\n",
    "\n",
    "x_train = train_data.x\n",
    "x_val = val_data.x\n",
    "x_test = test_data.x\n",
    "\n",
    "best_model = train_link_prediction(x_train, x_val, x_test, train_data, val_data, test_data, input_dim=5, epochs=100, patience=20)\n",
    "\n",
    "inverted_mask_matrix = (torch.ones([num_nodes,num_nodes]).squeeze(0) - mask.to('cpu')) \n",
    "eval_link_prediction(data.x, data.edge_index, best_model, adj_matrix, inverted_mask_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2999)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = torch.concatenate((train_data.x, train_data.x_ase), axis=1)\n",
    "x_val = torch.concatenate((val_data.x, val_data.x_ase), axis=1)\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_ase), axis=1)\n",
    "\n",
    "best_model = train_link_prediction(x_train, x_val, x_test, train_data, val_data, test_data, 9, epochs=100, patience=20)\n",
    "\n",
    "# inverted_mask_matrix = (torch.ones([num_nodes,num_nodes]).squeeze(0) - mask.to('cpu'))\n",
    "x_eval = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "eval_link_prediction(x_eval, data.edge_index, best_model, adj_matrix, inverted_mask_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.concatenate((train_data.x, train_data.x_grdpg), axis=1)\n",
    "x_val = torch.concatenate((val_data.x, val_data.x_grdpg), axis=1)\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_grdpg), axis=1)\n",
    "\n",
    "best_model = train_link_prediction(x_train, x_val, x_test, train_data, val_data, test_data, 9, epochs=100, patience=1000)\n",
    "\n",
    "# inverted_mask_matrix = (torch.ones([num_nodes,num_nodes]).squeeze(0) - mask.to('cpu'))\n",
    "x_eval = torch.concatenate((data.x, data.x_grdpg), axis=1)\n",
    "eval_link_prediction(x_eval, data.edge_index, best_model, adj_matrix, inverted_mask_matrix) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "GCN acc: 0.1765, ASE acc: 0.1765, GRDPG acc: 0.5434, GLASE acc: 0.5353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Net(\n",
       "   (conv1): GCNConv(5, 128)\n",
       "   (conv2): GCNConv(128, 64)\n",
       " ),\n",
       " Net(\n",
       "   (conv1): GCNConv(9, 128)\n",
       "   (conv2): GCNConv(128, 64)\n",
       " ),\n",
       " Net(\n",
       "   (conv1): GCNConv(9, 128)\n",
       "   (conv2): GCNConv(128, 64)\n",
       " ),\n",
       " Net(\n",
       "   (conv1): GCNConv(9, 128)\n",
       "   (conv2): GCNConv(128, 64)\n",
       " ),\n",
       " tensor(0.1765),\n",
       " tensor(0.1765),\n",
       " tensor(0.5434),\n",
       " tensor(0.5353))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from training.run_link_prediction import link_prediction\n",
    "\n",
    "link_prediction(data.edge_index, inverted_mask_matrix, mask, data, 5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 0.5391, Val: 0.8500, Test: 0.8606\n",
      "Epoch: 040, Loss: 0.5351, Val: 0.8490, Test: 0.8605\n",
      "Epoch: 060, Loss: 0.5345, Val: 0.8511, Test: 0.8622\n",
      "Epoch: 080, Loss: 0.5322, Val: 0.8528, Test: 0.8630\n",
      "Epoch: 100, Loss: 0.5336, Val: 0.8496, Test: 0.8611\n",
      "Final Test: 0.8688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7664)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from models.link_prediction import Net\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_ase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "            f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / (n_L1 + n_L2 + n_L3)\n",
    "\n",
    "inverted_mask_matrix = (torch.ones([num_nodes,num_nodes]).squeeze(0) - mask.to('cpu')) \n",
    "indices = torch.where(inverted_mask_matrix)\n",
    "((adj_matrix[indices[0], indices[1]]) == (predicted_adj[indices[0], indices[1]])).sum() / indices[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7655)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_link_prediction(x_test, data.edge_index, model, adj_matrix, inverted_mask_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 0.7430, Val: 0.7815, Test: 0.7797\n",
      "Epoch: 040, Loss: 0.7400, Val: 0.7845, Test: 0.7825\n",
      "Epoch: 060, Loss: 0.7355, Val: 0.7864, Test: 0.7838\n",
      "Epoch: 080, Loss: 0.7369, Val: 0.7904, Test: 0.7867\n",
      "Epoch: 100, Loss: 0.7316, Val: 0.7884, Test: 0.7851\n",
      "Final Test: 0.8743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7676)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy \n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# def train(x_input, train_data, model):\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "#     criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     z = model.encode(x_input, train_data.edge_index)\n",
    "\n",
    "#     # We perform a new round of negative sampling for every training epoch:\n",
    "#     neg_edge_index = negative_sampling(\n",
    "#         edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "#         num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "#     edge_label_index = torch.cat(\n",
    "#         [train_data.edge_label_index, neg_edge_index],\n",
    "#         dim=-1,\n",
    "#     )\n",
    "#     edge_label = torch.cat([\n",
    "#         train_data.edge_label,\n",
    "#         train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "#     ], dim=0)\n",
    "\n",
    "#     out = model.decode(z, edge_label_index).view(-1)\n",
    "#     loss = criterion(out, edge_label)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test(x_input, data, model):\n",
    "#     model.eval()\n",
    "#     z = model.encode(x_input, data.edge_index)\n",
    "#     out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "#     return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "from models.link_prediction import train as train2\n",
    "from models.link_prediction import test as test2\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train2(x_train, train_data, model)\n",
    "    val_auc = test2(x_val, val_data, model)\n",
    "    test_auc = test2(x_test, test_data, model)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "            f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "def eval_link_prediction(x_eval, edge_index, model, adj_matrix, inverted_mask_matrix): \n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    z = model.encode(x_eval, edge_index)\n",
    "    final_edge_index = model.decode_all(z)\n",
    "    predicted_adj = to_dense_adj(final_edge_index, max_num_nodes=num_nodes).squeeze(0).to('cpu')\n",
    "    indices = torch.where(inverted_mask_matrix.to('cpu'))\n",
    "    acc =((adj_matrix[indices[0], indices[1]]) == (predicted_adj[indices[0], indices[1]])).sum() / indices[0].shape[0]\n",
    "\n",
    "    return acc\n",
    "\n",
    "x_train = torch.concatenate((train_data.x, train_data.x_ase), axis=1)\n",
    "x_val = torch.concatenate((val_data.x, val_data.x_ase), axis=1)\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_ase), axis=1)\n",
    "\n",
    "# best_model = train_link_prediction(x_train, x_val, x_test, train_data, val_data, test_data, 9, epochs=101, patience=200)\n",
    "\n",
    "# inverted_mask_matrix = (torch.ones([num_nodes,num_nodes]).squeeze(0) - mask.to('cpu'))\n",
    "x_eval = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "eval_link_prediction(x_eval, data.edge_index, model, adj_matrix, inverted_mask_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7311)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLASE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.9386, Val: 0.5728, Test: 0.5463\n",
      "Epoch: 002, Loss: 0.7005, Val: 0.7178, Test: 0.6992\n",
      "Epoch: 003, Loss: 0.6910, Val: 0.7487, Test: 0.7327\n",
      "Epoch: 004, Loss: 0.6908, Val: 0.7542, Test: 0.7363\n",
      "Epoch: 005, Loss: 0.6768, Val: 0.7554, Test: 0.7316\n",
      "Epoch: 006, Loss: 0.6690, Val: 0.7578, Test: 0.7271\n",
      "Epoch: 007, Loss: 0.6661, Val: 0.7622, Test: 0.7238\n",
      "Epoch: 008, Loss: 0.6600, Val: 0.7696, Test: 0.7268\n",
      "Epoch: 009, Loss: 0.6540, Val: 0.7741, Test: 0.7313\n",
      "Epoch: 010, Loss: 0.6486, Val: 0.7745, Test: 0.7323\n",
      "Epoch: 011, Loss: 0.6400, Val: 0.7693, Test: 0.7279\n",
      "Epoch: 012, Loss: 0.6376, Val: 0.7581, Test: 0.7196\n",
      "Epoch: 013, Loss: 0.6292, Val: 0.7486, Test: 0.7113\n",
      "Epoch: 014, Loss: 0.6280, Val: 0.7436, Test: 0.7054\n",
      "Epoch: 015, Loss: 0.6225, Val: 0.7435, Test: 0.7034\n",
      "Epoch: 016, Loss: 0.6167, Val: 0.7432, Test: 0.7029\n",
      "Epoch: 017, Loss: 0.6087, Val: 0.7443, Test: 0.7037\n",
      "Epoch: 018, Loss: 0.6037, Val: 0.7566, Test: 0.7147\n",
      "Epoch: 019, Loss: 0.6038, Val: 0.7606, Test: 0.7383\n",
      "Epoch: 020, Loss: 0.5915, Val: 0.7562, Test: 0.7483\n",
      "Epoch: 021, Loss: 0.5943, Val: 0.7532, Test: 0.7521\n",
      "Epoch: 022, Loss: 0.5907, Val: 0.7531, Test: 0.7537\n",
      "Epoch: 023, Loss: 0.5872, Val: 0.7519, Test: 0.7549\n",
      "Epoch: 024, Loss: 0.5914, Val: 0.7485, Test: 0.7553\n",
      "Epoch: 025, Loss: 0.5874, Val: 0.7457, Test: 0.7554\n",
      "Epoch: 026, Loss: 0.5824, Val: 0.7465, Test: 0.7573\n",
      "Epoch: 027, Loss: 0.5793, Val: 0.7491, Test: 0.7598\n",
      "Epoch: 028, Loss: 0.5826, Val: 0.7484, Test: 0.7611\n",
      "Epoch: 029, Loss: 0.5757, Val: 0.7450, Test: 0.7613\n",
      "Epoch: 030, Loss: 0.5810, Val: 0.7439, Test: 0.7622\n",
      "Epoch: 031, Loss: 0.5758, Val: 0.7471, Test: 0.7650\n",
      "Epoch: 032, Loss: 0.5763, Val: 0.7498, Test: 0.7666\n",
      "Epoch: 033, Loss: 0.5732, Val: 0.7512, Test: 0.7671\n",
      "Epoch: 034, Loss: 0.5758, Val: 0.7482, Test: 0.7680\n",
      "Epoch: 035, Loss: 0.5781, Val: 0.7462, Test: 0.7685\n",
      "Epoch: 036, Loss: 0.5741, Val: 0.7481, Test: 0.7694\n",
      "Epoch: 037, Loss: 0.5707, Val: 0.7523, Test: 0.7697\n",
      "Epoch: 038, Loss: 0.5736, Val: 0.7530, Test: 0.7698\n",
      "Epoch: 039, Loss: 0.5772, Val: 0.7511, Test: 0.7706\n",
      "Epoch: 040, Loss: 0.5713, Val: 0.7477, Test: 0.7716\n",
      "Epoch: 041, Loss: 0.5708, Val: 0.7468, Test: 0.7720\n",
      "Epoch: 042, Loss: 0.5703, Val: 0.7484, Test: 0.7720\n",
      "Epoch: 043, Loss: 0.5735, Val: 0.7497, Test: 0.7721\n",
      "Epoch: 044, Loss: 0.5701, Val: 0.7482, Test: 0.7724\n",
      "Epoch: 045, Loss: 0.5781, Val: 0.7440, Test: 0.7725\n",
      "Epoch: 046, Loss: 0.5728, Val: 0.7429, Test: 0.7724\n",
      "Epoch: 047, Loss: 0.5729, Val: 0.7470, Test: 0.7734\n",
      "Epoch: 048, Loss: 0.5752, Val: 0.7496, Test: 0.7730\n",
      "Epoch: 049, Loss: 0.5763, Val: 0.7502, Test: 0.7731\n",
      "Epoch: 050, Loss: 0.5698, Val: 0.7477, Test: 0.7740\n",
      "Epoch: 051, Loss: 0.5686, Val: 0.7463, Test: 0.7742\n",
      "Epoch: 052, Loss: 0.5698, Val: 0.7469, Test: 0.7743\n",
      "Epoch: 053, Loss: 0.5719, Val: 0.7487, Test: 0.7734\n",
      "Epoch: 054, Loss: 0.5666, Val: 0.7495, Test: 0.7731\n",
      "Epoch: 055, Loss: 0.5734, Val: 0.7463, Test: 0.7741\n",
      "Epoch: 056, Loss: 0.5708, Val: 0.7454, Test: 0.7747\n",
      "Epoch: 057, Loss: 0.5683, Val: 0.7472, Test: 0.7745\n",
      "Epoch: 058, Loss: 0.5721, Val: 0.7483, Test: 0.7744\n",
      "Epoch: 059, Loss: 0.5802, Val: 0.7476, Test: 0.7751\n",
      "Epoch: 060, Loss: 0.5703, Val: 0.7472, Test: 0.7750\n",
      "Epoch: 061, Loss: 0.5682, Val: 0.7480, Test: 0.7750\n",
      "Epoch: 062, Loss: 0.5695, Val: 0.7489, Test: 0.7749\n",
      "Epoch: 063, Loss: 0.5677, Val: 0.7487, Test: 0.7746\n",
      "Epoch: 064, Loss: 0.5726, Val: 0.7459, Test: 0.7748\n",
      "Epoch: 065, Loss: 0.5654, Val: 0.7458, Test: 0.7746\n",
      "Epoch: 066, Loss: 0.5726, Val: 0.7460, Test: 0.7747\n",
      "Epoch: 067, Loss: 0.5652, Val: 0.7486, Test: 0.7740\n",
      "Epoch: 068, Loss: 0.5710, Val: 0.7490, Test: 0.7740\n",
      "Epoch: 069, Loss: 0.5698, Val: 0.7470, Test: 0.7748\n",
      "Epoch: 070, Loss: 0.5709, Val: 0.7468, Test: 0.7749\n",
      "Epoch: 071, Loss: 0.5706, Val: 0.7470, Test: 0.7748\n",
      "Epoch: 072, Loss: 0.5717, Val: 0.7498, Test: 0.7740\n",
      "Epoch: 073, Loss: 0.5625, Val: 0.7507, Test: 0.7737\n",
      "Epoch: 074, Loss: 0.5657, Val: 0.7500, Test: 0.7742\n",
      "Epoch: 075, Loss: 0.5683, Val: 0.7465, Test: 0.7750\n",
      "Epoch: 076, Loss: 0.5699, Val: 0.7445, Test: 0.7744\n",
      "Epoch: 077, Loss: 0.5756, Val: 0.7448, Test: 0.7744\n",
      "Epoch: 078, Loss: 0.5720, Val: 0.7474, Test: 0.7743\n",
      "Epoch: 079, Loss: 0.5721, Val: 0.7489, Test: 0.7744\n",
      "Epoch: 080, Loss: 0.5667, Val: 0.7475, Test: 0.7753\n",
      "Epoch: 081, Loss: 0.5707, Val: 0.7468, Test: 0.7756\n",
      "Epoch: 082, Loss: 0.5716, Val: 0.7479, Test: 0.7755\n",
      "Epoch: 083, Loss: 0.5689, Val: 0.7494, Test: 0.7754\n",
      "Epoch: 084, Loss: 0.5696, Val: 0.7498, Test: 0.7746\n",
      "Epoch: 085, Loss: 0.5682, Val: 0.7465, Test: 0.7749\n",
      "Epoch: 086, Loss: 0.5634, Val: 0.7456, Test: 0.7749\n",
      "Epoch: 087, Loss: 0.5666, Val: 0.7451, Test: 0.7747\n",
      "Epoch: 088, Loss: 0.5623, Val: 0.7455, Test: 0.7750\n",
      "Epoch: 089, Loss: 0.5665, Val: 0.7461, Test: 0.7755\n",
      "Epoch: 090, Loss: 0.5643, Val: 0.7482, Test: 0.7760\n",
      "Epoch: 091, Loss: 0.5661, Val: 0.7482, Test: 0.7760\n",
      "Epoch: 092, Loss: 0.5650, Val: 0.7474, Test: 0.7759\n",
      "Epoch: 093, Loss: 0.5670, Val: 0.7462, Test: 0.7757\n",
      "Epoch: 094, Loss: 0.5682, Val: 0.7463, Test: 0.7753\n",
      "Epoch: 095, Loss: 0.5695, Val: 0.7457, Test: 0.7753\n",
      "Epoch: 096, Loss: 0.5684, Val: 0.7451, Test: 0.7754\n",
      "Epoch: 097, Loss: 0.5751, Val: 0.7460, Test: 0.7757\n",
      "Epoch: 098, Loss: 0.5677, Val: 0.7482, Test: 0.7759\n",
      "Epoch: 099, Loss: 0.5677, Val: 0.7487, Test: 0.7762\n",
      "Epoch: 100, Loss: 0.5697, Val: 0.7475, Test: 0.7766\n",
      "Final Test: 0.7323\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_glase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_glase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7603)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / (n_L1 + n_L2 + n_L3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss: 0.5424, Val: 0.8472, Test: 0.8335\n",
      "Epoch: 040, Loss: 0.5387, Val: 0.8613, Test: 0.8483\n",
      "Epoch: 060, Loss: 0.5409, Val: 0.8662, Test: 0.8538\n",
      "Epoch: 080, Loss: 0.5392, Val: 0.8642, Test: 0.8516\n",
      "Epoch: 100, Loss: 0.5375, Val: 0.8529, Test: 0.8397\n",
      "Final Test: 0.8835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8329)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_grdpg), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_grdpg), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "            f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_grdpg), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / (n_L1 + n_L2 + n_L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7414)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_mask_matrix = (torch.ones([num_nodes,num_nodes]).squeeze(0) - mask.to('cpu')) \n",
    "indices = torch.where(inverted_mask_matrix)\n",
    "((adj_matrix[indices[0], indices[1]]) == (predicted_adj[indices[0], indices[1]])).sum() / indices[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "ones_features=torch.ones([410, 5])\n",
    "ones_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Train, Val, Test\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "masked_edge_index = masked_adj.nonzero().t().contiguous()\n",
    "\n",
    "data = Data(x=random_features.float(), x_ase=x_ase, x_glase=x_glase, edge_index=masked_edge_index)\n",
    "\n",
    "transform = T.Compose([\n",
    "    # T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6809, Val: 0.7945, Test: 0.7740\n",
      "Epoch: 002, Loss: 0.6746, Val: 0.7971, Test: 0.7769\n",
      "Epoch: 003, Loss: 0.6950, Val: 0.7963, Test: 0.7767\n",
      "Epoch: 004, Loss: 0.6680, Val: 0.7958, Test: 0.7767\n",
      "Epoch: 005, Loss: 0.6728, Val: 0.7960, Test: 0.7770\n",
      "Epoch: 006, Loss: 0.6740, Val: 0.7967, Test: 0.7769\n",
      "Epoch: 007, Loss: 0.6702, Val: 0.7971, Test: 0.7770\n",
      "Epoch: 008, Loss: 0.6666, Val: 0.7976, Test: 0.7770\n",
      "Epoch: 009, Loss: 0.6665, Val: 0.7977, Test: 0.7771\n",
      "Epoch: 010, Loss: 0.6687, Val: 0.7976, Test: 0.7770\n",
      "Epoch: 011, Loss: 0.6626, Val: 0.7970, Test: 0.7770\n",
      "Epoch: 012, Loss: 0.6600, Val: 0.7965, Test: 0.7766\n",
      "Epoch: 013, Loss: 0.6582, Val: 0.7964, Test: 0.7763\n",
      "Epoch: 014, Loss: 0.6561, Val: 0.7963, Test: 0.7763\n",
      "Epoch: 015, Loss: 0.6524, Val: 0.7970, Test: 0.7766\n",
      "Epoch: 016, Loss: 0.6488, Val: 0.7971, Test: 0.7768\n",
      "Epoch: 017, Loss: 0.6482, Val: 0.7968, Test: 0.7768\n",
      "Epoch: 018, Loss: 0.6453, Val: 0.7962, Test: 0.7761\n",
      "Epoch: 019, Loss: 0.6414, Val: 0.7954, Test: 0.7750\n",
      "Epoch: 020, Loss: 0.6410, Val: 0.7946, Test: 0.7740\n",
      "Epoch: 021, Loss: 0.6389, Val: 0.7941, Test: 0.7734\n",
      "Epoch: 022, Loss: 0.6380, Val: 0.7941, Test: 0.7734\n",
      "Epoch: 023, Loss: 0.6368, Val: 0.7941, Test: 0.7736\n",
      "Epoch: 024, Loss: 0.6316, Val: 0.7937, Test: 0.7735\n",
      "Epoch: 025, Loss: 0.6322, Val: 0.7924, Test: 0.7723\n",
      "Epoch: 026, Loss: 0.6282, Val: 0.7901, Test: 0.7705\n",
      "Epoch: 027, Loss: 0.6278, Val: 0.7879, Test: 0.7686\n",
      "Epoch: 028, Loss: 0.6280, Val: 0.7867, Test: 0.7676\n",
      "Epoch: 029, Loss: 0.6200, Val: 0.7880, Test: 0.7687\n",
      "Epoch: 030, Loss: 0.6225, Val: 0.7870, Test: 0.7681\n",
      "Epoch: 031, Loss: 0.6191, Val: 0.7841, Test: 0.7654\n",
      "Epoch: 032, Loss: 0.6198, Val: 0.7795, Test: 0.7606\n",
      "Epoch: 033, Loss: 0.6162, Val: 0.7792, Test: 0.7593\n",
      "Epoch: 034, Loss: 0.6162, Val: 0.7797, Test: 0.7602\n",
      "Epoch: 035, Loss: 0.6167, Val: 0.7796, Test: 0.7597\n",
      "Epoch: 036, Loss: 0.6155, Val: 0.7771, Test: 0.7559\n",
      "Epoch: 037, Loss: 0.6115, Val: 0.7726, Test: 0.7534\n",
      "Epoch: 038, Loss: 0.6128, Val: 0.7725, Test: 0.7533\n",
      "Epoch: 039, Loss: 0.6134, Val: 0.7727, Test: 0.7532\n",
      "Epoch: 040, Loss: 0.6102, Val: 0.7716, Test: 0.7520\n",
      "Epoch: 041, Loss: 0.6098, Val: 0.7685, Test: 0.7504\n",
      "Epoch: 042, Loss: 0.6141, Val: 0.7634, Test: 0.7480\n",
      "Epoch: 043, Loss: 0.6078, Val: 0.7659, Test: 0.7496\n",
      "Epoch: 044, Loss: 0.6050, Val: 0.7683, Test: 0.7503\n",
      "Epoch: 045, Loss: 0.6045, Val: 0.7629, Test: 0.7483\n",
      "Epoch: 046, Loss: 0.6052, Val: 0.7581, Test: 0.7441\n",
      "Epoch: 047, Loss: 0.6005, Val: 0.7597, Test: 0.7460\n",
      "Epoch: 048, Loss: 0.5993, Val: 0.7634, Test: 0.7492\n",
      "Epoch: 049, Loss: 0.6076, Val: 0.7573, Test: 0.7432\n",
      "Epoch: 050, Loss: 0.6031, Val: 0.7543, Test: 0.7386\n",
      "Epoch: 051, Loss: 0.6033, Val: 0.7571, Test: 0.7430\n",
      "Epoch: 052, Loss: 0.6035, Val: 0.7579, Test: 0.7444\n",
      "Epoch: 053, Loss: 0.5991, Val: 0.7564, Test: 0.7415\n",
      "Epoch: 054, Loss: 0.5981, Val: 0.7547, Test: 0.7385\n",
      "Epoch: 055, Loss: 0.5979, Val: 0.7552, Test: 0.7394\n",
      "Epoch: 056, Loss: 0.6024, Val: 0.7555, Test: 0.7399\n",
      "Epoch: 057, Loss: 0.5980, Val: 0.7550, Test: 0.7391\n",
      "Epoch: 058, Loss: 0.6022, Val: 0.7527, Test: 0.7359\n",
      "Epoch: 059, Loss: 0.5947, Val: 0.7543, Test: 0.7376\n",
      "Epoch: 060, Loss: 0.5983, Val: 0.7546, Test: 0.7383\n",
      "Epoch: 061, Loss: 0.5973, Val: 0.7534, Test: 0.7362\n",
      "Epoch: 062, Loss: 0.5967, Val: 0.7524, Test: 0.7357\n",
      "Epoch: 063, Loss: 0.5983, Val: 0.7528, Test: 0.7359\n",
      "Epoch: 064, Loss: 0.5984, Val: 0.7514, Test: 0.7349\n",
      "Epoch: 065, Loss: 0.6021, Val: 0.7502, Test: 0.7339\n",
      "Epoch: 066, Loss: 0.5959, Val: 0.7530, Test: 0.7361\n",
      "Epoch: 067, Loss: 0.5956, Val: 0.7529, Test: 0.7360\n",
      "Epoch: 068, Loss: 0.5951, Val: 0.7499, Test: 0.7335\n",
      "Epoch: 069, Loss: 0.6007, Val: 0.7484, Test: 0.7325\n",
      "Epoch: 070, Loss: 0.6032, Val: 0.7496, Test: 0.7334\n",
      "Epoch: 071, Loss: 0.5987, Val: 0.7513, Test: 0.7349\n",
      "Epoch: 072, Loss: 0.5915, Val: 0.7522, Test: 0.7355\n",
      "Epoch: 073, Loss: 0.6035, Val: 0.7461, Test: 0.7312\n",
      "Epoch: 074, Loss: 0.6034, Val: 0.7478, Test: 0.7320\n",
      "Epoch: 075, Loss: 0.6028, Val: 0.7516, Test: 0.7348\n",
      "Epoch: 076, Loss: 0.5984, Val: 0.7499, Test: 0.7336\n",
      "Epoch: 077, Loss: 0.5924, Val: 0.7488, Test: 0.7330\n",
      "Epoch: 078, Loss: 0.5909, Val: 0.7511, Test: 0.7345\n",
      "Epoch: 079, Loss: 0.5995, Val: 0.7481, Test: 0.7325\n",
      "Epoch: 080, Loss: 0.5948, Val: 0.7491, Test: 0.7331\n",
      "Epoch: 081, Loss: 0.5978, Val: 0.7497, Test: 0.7334\n",
      "Epoch: 082, Loss: 0.5958, Val: 0.7495, Test: 0.7333\n",
      "Epoch: 083, Loss: 0.6003, Val: 0.7482, Test: 0.7326\n",
      "Epoch: 084, Loss: 0.5914, Val: 0.7509, Test: 0.7343\n",
      "Epoch: 085, Loss: 0.6010, Val: 0.7472, Test: 0.7317\n",
      "Epoch: 086, Loss: 0.5918, Val: 0.7493, Test: 0.7331\n",
      "Epoch: 087, Loss: 0.5970, Val: 0.7500, Test: 0.7336\n",
      "Epoch: 088, Loss: 0.6014, Val: 0.7469, Test: 0.7315\n",
      "Epoch: 089, Loss: 0.5973, Val: 0.7492, Test: 0.7330\n",
      "Epoch: 090, Loss: 0.6049, Val: 0.7474, Test: 0.7318\n",
      "Epoch: 091, Loss: 0.6010, Val: 0.7474, Test: 0.7319\n",
      "Epoch: 092, Loss: 0.6019, Val: 0.7491, Test: 0.7329\n",
      "Epoch: 093, Loss: 0.5981, Val: 0.7498, Test: 0.7334\n",
      "Epoch: 094, Loss: 0.5965, Val: 0.7486, Test: 0.7328\n",
      "Epoch: 095, Loss: 0.5989, Val: 0.7470, Test: 0.7317\n",
      "Epoch: 096, Loss: 0.6064, Val: 0.7469, Test: 0.7316\n",
      "Epoch: 097, Loss: 0.5939, Val: 0.7527, Test: 0.7356\n",
      "Epoch: 098, Loss: 0.6036, Val: 0.7470, Test: 0.7317\n",
      "Epoch: 099, Loss: 0.5990, Val: 0.7466, Test: 0.7312\n",
      "Epoch: 100, Loss: 0.6001, Val: 0.7519, Test: 0.7352\n",
      "Final Test: 0.7771\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(5, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5545)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "\n",
    "z = model.encode(data.x, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / 230\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6694, Val: 0.8427, Test: 0.8247\n",
      "Epoch: 002, Loss: 0.6670, Val: 0.8461, Test: 0.8198\n",
      "Epoch: 003, Loss: 0.6483, Val: 0.8730, Test: 0.8557\n",
      "Epoch: 004, Loss: 0.6304, Val: 0.8706, Test: 0.8652\n",
      "Epoch: 005, Loss: 0.6054, Val: 0.8490, Test: 0.8508\n",
      "Epoch: 006, Loss: 0.5772, Val: 0.8428, Test: 0.8379\n",
      "Epoch: 007, Loss: 0.5562, Val: 0.8371, Test: 0.8408\n",
      "Epoch: 008, Loss: 0.5409, Val: 0.8396, Test: 0.8331\n",
      "Epoch: 009, Loss: 0.5562, Val: 0.8187, Test: 0.8304\n",
      "Epoch: 010, Loss: 0.5673, Val: 0.8372, Test: 0.8344\n",
      "Epoch: 011, Loss: 0.5494, Val: 0.8351, Test: 0.8300\n",
      "Epoch: 012, Loss: 0.5546, Val: 0.8293, Test: 0.8346\n",
      "Epoch: 013, Loss: 0.5419, Val: 0.8241, Test: 0.8326\n",
      "Epoch: 014, Loss: 0.5455, Val: 0.8312, Test: 0.8298\n",
      "Epoch: 015, Loss: 0.5385, Val: 0.8334, Test: 0.8291\n",
      "Epoch: 016, Loss: 0.5485, Val: 0.8347, Test: 0.8319\n",
      "Epoch: 017, Loss: 0.5405, Val: 0.8370, Test: 0.8389\n",
      "Epoch: 018, Loss: 0.5395, Val: 0.8387, Test: 0.8411\n",
      "Epoch: 019, Loss: 0.5379, Val: 0.8420, Test: 0.8413\n",
      "Epoch: 020, Loss: 0.5321, Val: 0.8434, Test: 0.8402\n",
      "Epoch: 021, Loss: 0.5367, Val: 0.8419, Test: 0.8406\n",
      "Epoch: 022, Loss: 0.5285, Val: 0.8393, Test: 0.8406\n",
      "Epoch: 023, Loss: 0.5305, Val: 0.8394, Test: 0.8396\n",
      "Epoch: 024, Loss: 0.5324, Val: 0.8394, Test: 0.8375\n",
      "Epoch: 025, Loss: 0.5272, Val: 0.8380, Test: 0.8361\n",
      "Epoch: 026, Loss: 0.5289, Val: 0.8369, Test: 0.8372\n",
      "Epoch: 027, Loss: 0.5339, Val: 0.8362, Test: 0.8372\n",
      "Epoch: 028, Loss: 0.5343, Val: 0.8365, Test: 0.8359\n",
      "Epoch: 029, Loss: 0.5307, Val: 0.8370, Test: 0.8365\n",
      "Epoch: 030, Loss: 0.5255, Val: 0.8385, Test: 0.8380\n",
      "Epoch: 031, Loss: 0.5264, Val: 0.8394, Test: 0.8382\n",
      "Epoch: 032, Loss: 0.5333, Val: 0.8406, Test: 0.8386\n",
      "Epoch: 033, Loss: 0.5338, Val: 0.8401, Test: 0.8414\n",
      "Epoch: 034, Loss: 0.5304, Val: 0.8378, Test: 0.8414\n",
      "Epoch: 035, Loss: 0.5370, Val: 0.8396, Test: 0.8403\n",
      "Epoch: 036, Loss: 0.5270, Val: 0.8402, Test: 0.8384\n",
      "Epoch: 037, Loss: 0.5256, Val: 0.8398, Test: 0.8391\n",
      "Epoch: 038, Loss: 0.5278, Val: 0.8387, Test: 0.8402\n",
      "Epoch: 039, Loss: 0.5293, Val: 0.8394, Test: 0.8409\n",
      "Epoch: 040, Loss: 0.5255, Val: 0.8407, Test: 0.8412\n",
      "Epoch: 041, Loss: 0.5266, Val: 0.8415, Test: 0.8411\n",
      "Epoch: 042, Loss: 0.5260, Val: 0.8415, Test: 0.8435\n",
      "Epoch: 043, Loss: 0.5266, Val: 0.8409, Test: 0.8433\n",
      "Epoch: 044, Loss: 0.5165, Val: 0.8407, Test: 0.8423\n",
      "Epoch: 045, Loss: 0.5242, Val: 0.8406, Test: 0.8405\n",
      "Epoch: 046, Loss: 0.5349, Val: 0.8396, Test: 0.8413\n",
      "Epoch: 047, Loss: 0.5236, Val: 0.8394, Test: 0.8411\n",
      "Epoch: 048, Loss: 0.5253, Val: 0.8397, Test: 0.8420\n",
      "Epoch: 049, Loss: 0.5207, Val: 0.8413, Test: 0.8439\n",
      "Epoch: 050, Loss: 0.5283, Val: 0.8423, Test: 0.8449\n",
      "Epoch: 051, Loss: 0.5120, Val: 0.8430, Test: 0.8454\n",
      "Epoch: 052, Loss: 0.5232, Val: 0.8420, Test: 0.8448\n",
      "Epoch: 053, Loss: 0.5241, Val: 0.8398, Test: 0.8429\n",
      "Epoch: 054, Loss: 0.5202, Val: 0.8399, Test: 0.8421\n",
      "Epoch: 055, Loss: 0.5231, Val: 0.8401, Test: 0.8418\n",
      "Epoch: 056, Loss: 0.5229, Val: 0.8410, Test: 0.8437\n",
      "Epoch: 057, Loss: 0.5227, Val: 0.8418, Test: 0.8455\n",
      "Epoch: 058, Loss: 0.5235, Val: 0.8434, Test: 0.8457\n",
      "Epoch: 059, Loss: 0.5211, Val: 0.8426, Test: 0.8438\n",
      "Epoch: 060, Loss: 0.5212, Val: 0.8387, Test: 0.8423\n",
      "Epoch: 061, Loss: 0.5222, Val: 0.8366, Test: 0.8415\n",
      "Epoch: 062, Loss: 0.5236, Val: 0.8398, Test: 0.8419\n",
      "Epoch: 063, Loss: 0.5172, Val: 0.8426, Test: 0.8440\n",
      "Epoch: 064, Loss: 0.5229, Val: 0.8425, Test: 0.8459\n",
      "Epoch: 065, Loss: 0.5181, Val: 0.8393, Test: 0.8442\n",
      "Epoch: 066, Loss: 0.5192, Val: 0.8365, Test: 0.8404\n",
      "Epoch: 067, Loss: 0.5242, Val: 0.8374, Test: 0.8375\n",
      "Epoch: 068, Loss: 0.5144, Val: 0.8413, Test: 0.8424\n",
      "Epoch: 069, Loss: 0.5176, Val: 0.8424, Test: 0.8468\n",
      "Epoch: 070, Loss: 0.5127, Val: 0.8403, Test: 0.8447\n",
      "Epoch: 071, Loss: 0.5162, Val: 0.8377, Test: 0.8389\n",
      "Epoch: 072, Loss: 0.5206, Val: 0.8352, Test: 0.8350\n",
      "Epoch: 073, Loss: 0.5100, Val: 0.8369, Test: 0.8406\n",
      "Epoch: 074, Loss: 0.5168, Val: 0.8416, Test: 0.8451\n",
      "Epoch: 075, Loss: 0.5227, Val: 0.8432, Test: 0.8438\n",
      "Epoch: 076, Loss: 0.5147, Val: 0.8370, Test: 0.8375\n",
      "Epoch: 077, Loss: 0.5135, Val: 0.8340, Test: 0.8357\n",
      "Epoch: 078, Loss: 0.5137, Val: 0.8375, Test: 0.8403\n",
      "Epoch: 079, Loss: 0.5244, Val: 0.8419, Test: 0.8423\n",
      "Epoch: 080, Loss: 0.5147, Val: 0.8405, Test: 0.8402\n",
      "Epoch: 081, Loss: 0.5160, Val: 0.8349, Test: 0.8369\n",
      "Epoch: 082, Loss: 0.5171, Val: 0.8325, Test: 0.8349\n",
      "Epoch: 083, Loss: 0.5213, Val: 0.8350, Test: 0.8360\n",
      "Epoch: 084, Loss: 0.5139, Val: 0.8397, Test: 0.8391\n",
      "Epoch: 085, Loss: 0.5176, Val: 0.8366, Test: 0.8368\n",
      "Epoch: 086, Loss: 0.5097, Val: 0.8338, Test: 0.8341\n",
      "Epoch: 087, Loss: 0.5115, Val: 0.8325, Test: 0.8341\n",
      "Epoch: 088, Loss: 0.5097, Val: 0.8360, Test: 0.8372\n",
      "Epoch: 089, Loss: 0.5088, Val: 0.8430, Test: 0.8408\n",
      "Epoch: 090, Loss: 0.5093, Val: 0.8414, Test: 0.8392\n",
      "Epoch: 091, Loss: 0.5165, Val: 0.8356, Test: 0.8356\n",
      "Epoch: 092, Loss: 0.5064, Val: 0.8360, Test: 0.8366\n",
      "Epoch: 093, Loss: 0.5091, Val: 0.8383, Test: 0.8386\n",
      "Epoch: 094, Loss: 0.5099, Val: 0.8413, Test: 0.8386\n",
      "Epoch: 095, Loss: 0.5095, Val: 0.8361, Test: 0.8346\n",
      "Epoch: 096, Loss: 0.5176, Val: 0.8313, Test: 0.8330\n",
      "Epoch: 097, Loss: 0.5161, Val: 0.8328, Test: 0.8336\n",
      "Epoch: 098, Loss: 0.5106, Val: 0.8366, Test: 0.8357\n",
      "Epoch: 099, Loss: 0.5068, Val: 0.8365, Test: 0.8345\n",
      "Epoch: 100, Loss: 0.5118, Val: 0.8319, Test: 0.8317\n",
      "Final Test: 0.8557\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_ase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_ase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8357)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / 230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLASE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.8382, Val: 0.6897, Test: 0.7031\n",
      "Epoch: 002, Loss: 0.6994, Val: 0.7442, Test: 0.7525\n",
      "Epoch: 003, Loss: 0.6994, Val: 0.7943, Test: 0.7836\n",
      "Epoch: 004, Loss: 0.6782, Val: 0.8038, Test: 0.7913\n",
      "Epoch: 005, Loss: 0.6596, Val: 0.8057, Test: 0.7909\n",
      "Epoch: 006, Loss: 0.6519, Val: 0.7889, Test: 0.7664\n",
      "Epoch: 007, Loss: 0.6490, Val: 0.7955, Test: 0.7749\n",
      "Epoch: 008, Loss: 0.6417, Val: 0.8108, Test: 0.7998\n",
      "Epoch: 009, Loss: 0.6352, Val: 0.8050, Test: 0.7956\n",
      "Epoch: 010, Loss: 0.6222, Val: 0.7920, Test: 0.7842\n",
      "Epoch: 011, Loss: 0.6096, Val: 0.7921, Test: 0.7866\n",
      "Epoch: 012, Loss: 0.6048, Val: 0.8015, Test: 0.7905\n",
      "Epoch: 013, Loss: 0.5982, Val: 0.8124, Test: 0.8027\n",
      "Epoch: 014, Loss: 0.5842, Val: 0.8129, Test: 0.8052\n",
      "Epoch: 015, Loss: 0.5827, Val: 0.8105, Test: 0.8022\n",
      "Epoch: 016, Loss: 0.5685, Val: 0.8095, Test: 0.8089\n",
      "Epoch: 017, Loss: 0.5680, Val: 0.8197, Test: 0.8168\n",
      "Epoch: 018, Loss: 0.5601, Val: 0.8284, Test: 0.8236\n",
      "Epoch: 019, Loss: 0.5519, Val: 0.8314, Test: 0.8238\n",
      "Epoch: 020, Loss: 0.5501, Val: 0.8363, Test: 0.8329\n",
      "Epoch: 021, Loss: 0.5432, Val: 0.8355, Test: 0.8381\n",
      "Epoch: 022, Loss: 0.5427, Val: 0.8339, Test: 0.8378\n",
      "Epoch: 023, Loss: 0.5417, Val: 0.8371, Test: 0.8367\n",
      "Epoch: 024, Loss: 0.5428, Val: 0.8355, Test: 0.8318\n",
      "Epoch: 025, Loss: 0.5345, Val: 0.8341, Test: 0.8315\n",
      "Epoch: 026, Loss: 0.5386, Val: 0.8324, Test: 0.8325\n",
      "Epoch: 027, Loss: 0.5446, Val: 0.8298, Test: 0.8336\n",
      "Epoch: 028, Loss: 0.5400, Val: 0.8325, Test: 0.8305\n",
      "Epoch: 029, Loss: 0.5448, Val: 0.8318, Test: 0.8285\n",
      "Epoch: 030, Loss: 0.5431, Val: 0.8326, Test: 0.8296\n",
      "Epoch: 031, Loss: 0.5399, Val: 0.8297, Test: 0.8351\n",
      "Epoch: 032, Loss: 0.5385, Val: 0.8306, Test: 0.8361\n",
      "Epoch: 033, Loss: 0.5415, Val: 0.8351, Test: 0.8334\n",
      "Epoch: 034, Loss: 0.5325, Val: 0.8363, Test: 0.8329\n",
      "Epoch: 035, Loss: 0.5383, Val: 0.8367, Test: 0.8360\n",
      "Epoch: 036, Loss: 0.5328, Val: 0.8359, Test: 0.8379\n",
      "Epoch: 037, Loss: 0.5344, Val: 0.8351, Test: 0.8382\n",
      "Epoch: 038, Loss: 0.5347, Val: 0.8365, Test: 0.8379\n",
      "Epoch: 039, Loss: 0.5360, Val: 0.8374, Test: 0.8368\n",
      "Epoch: 040, Loss: 0.5364, Val: 0.8377, Test: 0.8368\n",
      "Epoch: 041, Loss: 0.5357, Val: 0.8379, Test: 0.8375\n",
      "Epoch: 042, Loss: 0.5351, Val: 0.8378, Test: 0.8367\n",
      "Epoch: 043, Loss: 0.5306, Val: 0.8374, Test: 0.8367\n",
      "Epoch: 044, Loss: 0.5339, Val: 0.8374, Test: 0.8355\n",
      "Epoch: 045, Loss: 0.5320, Val: 0.8373, Test: 0.8356\n",
      "Epoch: 046, Loss: 0.5332, Val: 0.8344, Test: 0.8385\n",
      "Epoch: 047, Loss: 0.5310, Val: 0.8351, Test: 0.8381\n",
      "Epoch: 048, Loss: 0.5361, Val: 0.8373, Test: 0.8347\n",
      "Epoch: 049, Loss: 0.5226, Val: 0.8377, Test: 0.8351\n",
      "Epoch: 050, Loss: 0.5310, Val: 0.8355, Test: 0.8385\n",
      "Epoch: 051, Loss: 0.5310, Val: 0.8345, Test: 0.8385\n",
      "Epoch: 052, Loss: 0.5344, Val: 0.8367, Test: 0.8369\n",
      "Epoch: 053, Loss: 0.5405, Val: 0.8370, Test: 0.8361\n",
      "Epoch: 054, Loss: 0.5395, Val: 0.8372, Test: 0.8380\n",
      "Epoch: 055, Loss: 0.5318, Val: 0.8368, Test: 0.8400\n",
      "Epoch: 056, Loss: 0.5276, Val: 0.8392, Test: 0.8398\n",
      "Epoch: 057, Loss: 0.5271, Val: 0.8397, Test: 0.8393\n",
      "Epoch: 058, Loss: 0.5280, Val: 0.8398, Test: 0.8394\n",
      "Epoch: 059, Loss: 0.5278, Val: 0.8388, Test: 0.8412\n",
      "Epoch: 060, Loss: 0.5306, Val: 0.8370, Test: 0.8409\n",
      "Epoch: 061, Loss: 0.5394, Val: 0.8381, Test: 0.8396\n",
      "Epoch: 062, Loss: 0.5346, Val: 0.8382, Test: 0.8374\n",
      "Epoch: 063, Loss: 0.5301, Val: 0.8385, Test: 0.8390\n",
      "Epoch: 064, Loss: 0.5259, Val: 0.8385, Test: 0.8406\n",
      "Epoch: 065, Loss: 0.5335, Val: 0.8381, Test: 0.8417\n",
      "Epoch: 066, Loss: 0.5341, Val: 0.8371, Test: 0.8415\n",
      "Epoch: 067, Loss: 0.5259, Val: 0.8381, Test: 0.8406\n",
      "Epoch: 068, Loss: 0.5257, Val: 0.8388, Test: 0.8376\n",
      "Epoch: 069, Loss: 0.5279, Val: 0.8390, Test: 0.8389\n",
      "Epoch: 070, Loss: 0.5284, Val: 0.8385, Test: 0.8413\n",
      "Epoch: 071, Loss: 0.5318, Val: 0.8383, Test: 0.8422\n",
      "Epoch: 072, Loss: 0.5247, Val: 0.8399, Test: 0.8421\n",
      "Epoch: 073, Loss: 0.5313, Val: 0.8406, Test: 0.8415\n",
      "Epoch: 074, Loss: 0.5235, Val: 0.8403, Test: 0.8419\n",
      "Epoch: 075, Loss: 0.5249, Val: 0.8392, Test: 0.8427\n",
      "Epoch: 076, Loss: 0.5233, Val: 0.8387, Test: 0.8429\n",
      "Epoch: 077, Loss: 0.5222, Val: 0.8398, Test: 0.8422\n",
      "Epoch: 078, Loss: 0.5194, Val: 0.8402, Test: 0.8422\n",
      "Epoch: 079, Loss: 0.5312, Val: 0.8387, Test: 0.8421\n",
      "Epoch: 080, Loss: 0.5297, Val: 0.8390, Test: 0.8425\n",
      "Epoch: 081, Loss: 0.5162, Val: 0.8415, Test: 0.8437\n",
      "Epoch: 082, Loss: 0.5233, Val: 0.8410, Test: 0.8448\n",
      "Epoch: 083, Loss: 0.5206, Val: 0.8397, Test: 0.8447\n",
      "Epoch: 084, Loss: 0.5190, Val: 0.8393, Test: 0.8440\n",
      "Epoch: 085, Loss: 0.5182, Val: 0.8391, Test: 0.8431\n",
      "Epoch: 086, Loss: 0.5206, Val: 0.8409, Test: 0.8417\n",
      "Epoch: 087, Loss: 0.5226, Val: 0.8412, Test: 0.8432\n",
      "Epoch: 088, Loss: 0.5231, Val: 0.8409, Test: 0.8451\n",
      "Epoch: 089, Loss: 0.5222, Val: 0.8409, Test: 0.8456\n",
      "Epoch: 090, Loss: 0.5214, Val: 0.8424, Test: 0.8450\n",
      "Epoch: 091, Loss: 0.5207, Val: 0.8412, Test: 0.8441\n",
      "Epoch: 092, Loss: 0.5197, Val: 0.8385, Test: 0.8429\n",
      "Epoch: 093, Loss: 0.5219, Val: 0.8369, Test: 0.8417\n",
      "Epoch: 094, Loss: 0.5205, Val: 0.8375, Test: 0.8417\n",
      "Epoch: 095, Loss: 0.5228, Val: 0.8409, Test: 0.8431\n",
      "Epoch: 096, Loss: 0.5131, Val: 0.8436, Test: 0.8459\n",
      "Epoch: 097, Loss: 0.5175, Val: 0.8434, Test: 0.8471\n",
      "Epoch: 098, Loss: 0.5179, Val: 0.8407, Test: 0.8447\n",
      "Epoch: 099, Loss: 0.5172, Val: 0.8381, Test: 0.8412\n",
      "Epoch: 100, Loss: 0.5165, Val: 0.8374, Test: 0.8400\n",
      "Final Test: 0.8459\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_glase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_glase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8571)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / 230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sin leyes partidarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sofia/lase/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/sofia/lase/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/sofia/lase/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/sofia/lase/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sofia/lase/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/sofia/lase/lib/python3.10/site-packages/graspologic/models/edge_swaps.py:215: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  _edge_swap_numba = nb.jit(_edge_swap, nopython=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0., -1.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.]])\n",
      "Iteraciones:  100\n",
      "Loss:  tensor(158.3758)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.utils import stochastic_blockmodel_graph, to_dense_adj\n",
    "from graspologic.embed import AdjacencySpectralEmbed \n",
    "from models.RDPG_GD import GRDPG_GD_Armijo\n",
    "from models.GLASE_unshared_normalized import gLASE \n",
    "# from models.GLASE_unshared_normalized_v2 import gLASE_v2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "d = 4\n",
    "n_P1 = 100\n",
    "n_P2 = 80\n",
    "n_L1 = 200\n",
    "n_L2 = 150\n",
    "n_L3 = 60\n",
    "\n",
    "P1_L1 = 0.9 ## Votos de senadores del partido 1 a leyes grupo 1\n",
    "P1_L2 = 0.8 ## Votos de senadores del partido 1 a leyes grupo 2\n",
    "P1_L3 = 0.3 ## Votos de senadores del partido 1 a leyes grupo 3\n",
    "P2_L1 = 0.4 ## Votos de senadores del partido 2 a leyes grupo 1\n",
    "P2_L2 = 0.5 ## Votos de senadores del partido 2 a leyes grupo 2\n",
    "P2_L3 = 0.2 ## Votos de senadores del partido 2 a leyes grupo 3\n",
    "\n",
    "\n",
    "p = [\n",
    "    [0, 0, P1_L1, P1_L2, P1_L3],\n",
    "    [0, 0, P2_L1, P2_L2, P2_L3],\n",
    "    [P1_L1, P2_L1, 0, 0, 0], \n",
    "    [P1_L2, P2_L2, 0, 0, 0], \n",
    "    [P1_L3, P2_L3, 0, 0, 0]\n",
    "    ]\n",
    "\n",
    "n = [n_P1, n_P2, n_L1, n_L2, n_L3]\n",
    "\n",
    "num_nodes = np.sum(n)\n",
    "edge_index = stochastic_blockmodel_graph(n, p)\n",
    "\n",
    "\n",
    "## MASK\n",
    "n_P1_np = 20\n",
    "n_P2_np = 10\n",
    "senadores_no_presentes = list(range(n_P1_np)) + list(range(n_P1,n_P1+n_P2_np))\n",
    "\n",
    "mask = torch.ones([num_nodes,num_nodes]).squeeze(0)\n",
    "for i in senadores_no_presentes:\n",
    "    votos = (torch.rand(1, num_nodes) < 0.7).int()\n",
    "    mask[i,:] = votos\n",
    "    mask[:,i] = votos\n",
    "\n",
    "\n",
    "## ASE \n",
    "adj_matrix = to_dense_adj(edge_index.to('cpu')).squeeze(0)\n",
    "ase = AdjacencySpectralEmbed(n_components=d, diag_aug=True, algorithm='full')\n",
    "masked_adj = adj_matrix*mask\n",
    "x_ase = ase.fit_transform(masked_adj.numpy())\n",
    "x_ase = torch.from_numpy(x_ase)\n",
    "\n",
    "A = to_dense_adj(edge_index.to('cpu'), max_num_nodes=num_nodes).squeeze(0)\n",
    "\n",
    "u, V = torch.linalg.eig(A)\n",
    "\n",
    "list_q=[]\n",
    "for i in range(d):\n",
    "    if u[i].numpy()>0:\n",
    "        list_q.append(1)\n",
    "    else:\n",
    "        list_q.append(-1)\n",
    "        \n",
    "# list_q.sort(reverse=True)\n",
    "q = torch.Tensor(list_q)\n",
    "Q=torch.diag(q)\n",
    "\n",
    "print(Q)\n",
    "\n",
    "\n",
    "torch.norm((x_ase@Q@x_ase.T - to_dense_adj(edge_index).squeeze(0))*mask)\n",
    "\n",
    "\n",
    "x_grdpg, cost, k  = GRDPG_GD_Armijo(x_ase, edge_index, Q, mask.nonzero().t().contiguous())\n",
    "x_grdpg = x_grdpg.detach()\n",
    "print(\"Iteraciones: \", k)\n",
    "print(\"Loss: \", torch.norm((x_grdpg@Q@x_grdpg.T - to_dense_adj(edge_index).squeeze(0))*to_dense_adj(mask.nonzero().t().contiguous()).squeeze(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vecinos = adj_matrix@torch.ones(num_nodes)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize =(15,5))\n",
    "axes[0].scatter(np.arange(n_P1),vecinos[:n_P1])\n",
    "axes[0].set_title('Vecinos senadores P1')\n",
    "axes[0].set_xlabel('# nodo')\n",
    "axes[0].set_ylabel('vecinos')\n",
    "\n",
    "axes[1].scatter(np.arange(n_P2),vecinos[n_P1:n_P1+n_P2])\n",
    "axes[1].set_title('Vecinos senadores P1')\n",
    "axes[1].set_xlabel('# nodo')\n",
    "axes[1].set_ylabel('vecinos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([254., 217., 232., 251., 234., 245., 233., 222., 219., 236., 242., 237.,\n",
       "        236., 247., 222., 240., 243., 237., 249., 237., 358., 329., 341., 331.,\n",
       "        328., 335., 339., 347., 355., 337., 337., 344., 328., 326., 313., 328.,\n",
       "        355., 335., 313., 330., 345., 334., 332., 332., 340., 337., 325., 345.,\n",
       "        326., 336., 343., 343., 331., 334., 334., 334., 345., 347., 327., 336.,\n",
       "        340., 344., 335., 333., 324., 331., 353., 341., 335., 351., 329., 339.,\n",
       "        332., 347., 332., 339., 348., 322., 342., 345., 332., 339., 347., 315.,\n",
       "        338., 332., 341., 335., 336., 339., 350., 354., 336., 338., 341., 342.,\n",
       "        335., 341., 340., 337.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(masked_adj@torch.ones(num_nodes))[:n_P2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random_features=torch.rand([num_nodes, 5])\n",
    "\n",
    "## Split Train, Val, Test\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "masked_edge_index = masked_adj.nonzero().t().contiguous()\n",
    "\n",
    "data = Data(x=random_features.float(), x_ase=x_ase, x_grdpg=x_grdpg, edge_index=masked_edge_index)\n",
    "\n",
    "transform = T.Compose([\n",
    "    # T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7475, Val: 0.6089, Test: 0.6194\n",
      "Epoch: 002, Loss: 0.6932, Val: 0.6925, Test: 0.6934\n",
      "Epoch: 003, Loss: 0.6960, Val: 0.7184, Test: 0.7166\n",
      "Epoch: 004, Loss: 0.6985, Val: 0.7303, Test: 0.7280\n",
      "Epoch: 005, Loss: 0.6941, Val: 0.7419, Test: 0.7401\n",
      "Epoch: 006, Loss: 0.6919, Val: 0.7536, Test: 0.7529\n",
      "Epoch: 007, Loss: 0.6914, Val: 0.7732, Test: 0.7738\n",
      "Epoch: 008, Loss: 0.6911, Val: 0.7959, Test: 0.7977\n",
      "Epoch: 009, Loss: 0.6907, Val: 0.8048, Test: 0.8055\n",
      "Epoch: 010, Loss: 0.6901, Val: 0.8081, Test: 0.8086\n",
      "Epoch: 011, Loss: 0.6890, Val: 0.8106, Test: 0.8107\n",
      "Epoch: 012, Loss: 0.6878, Val: 0.8127, Test: 0.8125\n",
      "Epoch: 013, Loss: 0.6872, Val: 0.8144, Test: 0.8142\n",
      "Epoch: 014, Loss: 0.6867, Val: 0.8160, Test: 0.8156\n",
      "Epoch: 015, Loss: 0.6858, Val: 0.8170, Test: 0.8164\n",
      "Epoch: 016, Loss: 0.6850, Val: 0.8176, Test: 0.8168\n",
      "Epoch: 017, Loss: 0.6845, Val: 0.8177, Test: 0.8170\n",
      "Epoch: 018, Loss: 0.6844, Val: 0.8178, Test: 0.8170\n",
      "Epoch: 019, Loss: 0.6838, Val: 0.8179, Test: 0.8170\n",
      "Epoch: 020, Loss: 0.6839, Val: 0.8178, Test: 0.8168\n",
      "Epoch: 021, Loss: 0.6832, Val: 0.8172, Test: 0.8164\n",
      "Epoch: 022, Loss: 0.6833, Val: 0.8165, Test: 0.8159\n",
      "Epoch: 023, Loss: 0.6831, Val: 0.8156, Test: 0.8153\n",
      "Epoch: 024, Loss: 0.6827, Val: 0.8145, Test: 0.8144\n",
      "Epoch: 025, Loss: 0.6824, Val: 0.8131, Test: 0.8132\n",
      "Epoch: 026, Loss: 0.6821, Val: 0.8116, Test: 0.8117\n",
      "Epoch: 027, Loss: 0.6824, Val: 0.8102, Test: 0.8104\n",
      "Epoch: 028, Loss: 0.6821, Val: 0.8092, Test: 0.8094\n",
      "Epoch: 029, Loss: 0.6821, Val: 0.8082, Test: 0.8086\n",
      "Epoch: 030, Loss: 0.6815, Val: 0.8072, Test: 0.8078\n",
      "Epoch: 031, Loss: 0.6813, Val: 0.8060, Test: 0.8068\n",
      "Epoch: 032, Loss: 0.6811, Val: 0.8047, Test: 0.8055\n",
      "Epoch: 033, Loss: 0.6807, Val: 0.8035, Test: 0.8042\n",
      "Epoch: 034, Loss: 0.6808, Val: 0.8027, Test: 0.8030\n",
      "Epoch: 035, Loss: 0.6807, Val: 0.8019, Test: 0.8019\n",
      "Epoch: 036, Loss: 0.6802, Val: 0.8009, Test: 0.8007\n",
      "Epoch: 037, Loss: 0.6800, Val: 0.7998, Test: 0.7995\n",
      "Epoch: 038, Loss: 0.6798, Val: 0.7990, Test: 0.7986\n",
      "Epoch: 039, Loss: 0.6795, Val: 0.7986, Test: 0.7983\n",
      "Epoch: 040, Loss: 0.6793, Val: 0.7987, Test: 0.7984\n",
      "Epoch: 041, Loss: 0.6785, Val: 0.7991, Test: 0.7988\n",
      "Epoch: 042, Loss: 0.6786, Val: 0.7991, Test: 0.7988\n",
      "Epoch: 043, Loss: 0.6784, Val: 0.7988, Test: 0.7985\n",
      "Epoch: 044, Loss: 0.6781, Val: 0.7987, Test: 0.7983\n",
      "Epoch: 045, Loss: 0.6785, Val: 0.7989, Test: 0.7985\n",
      "Epoch: 046, Loss: 0.6781, Val: 0.7993, Test: 0.7991\n",
      "Epoch: 047, Loss: 0.6779, Val: 0.7997, Test: 0.7994\n",
      "Epoch: 048, Loss: 0.6775, Val: 0.7997, Test: 0.7994\n",
      "Epoch: 049, Loss: 0.6770, Val: 0.7995, Test: 0.7993\n",
      "Epoch: 050, Loss: 0.6769, Val: 0.7994, Test: 0.7991\n",
      "Epoch: 051, Loss: 0.6762, Val: 0.7995, Test: 0.7991\n",
      "Epoch: 052, Loss: 0.6763, Val: 0.7991, Test: 0.7986\n",
      "Epoch: 053, Loss: 0.6760, Val: 0.7984, Test: 0.7977\n",
      "Epoch: 054, Loss: 0.6760, Val: 0.7977, Test: 0.7969\n",
      "Epoch: 055, Loss: 0.6756, Val: 0.7974, Test: 0.7965\n",
      "Epoch: 056, Loss: 0.6756, Val: 0.7967, Test: 0.7957\n",
      "Epoch: 057, Loss: 0.6748, Val: 0.7959, Test: 0.7947\n",
      "Epoch: 058, Loss: 0.6746, Val: 0.7948, Test: 0.7937\n",
      "Epoch: 059, Loss: 0.6741, Val: 0.7940, Test: 0.7929\n",
      "Epoch: 060, Loss: 0.6740, Val: 0.7930, Test: 0.7919\n",
      "Epoch: 061, Loss: 0.6738, Val: 0.7919, Test: 0.7907\n",
      "Epoch: 062, Loss: 0.6738, Val: 0.7905, Test: 0.7893\n",
      "Epoch: 063, Loss: 0.6734, Val: 0.7892, Test: 0.7880\n",
      "Epoch: 064, Loss: 0.6730, Val: 0.7877, Test: 0.7864\n",
      "Epoch: 065, Loss: 0.6721, Val: 0.7861, Test: 0.7848\n",
      "Epoch: 066, Loss: 0.6722, Val: 0.7843, Test: 0.7829\n",
      "Epoch: 067, Loss: 0.6721, Val: 0.7826, Test: 0.7812\n",
      "Epoch: 068, Loss: 0.6715, Val: 0.7811, Test: 0.7797\n",
      "Epoch: 069, Loss: 0.6721, Val: 0.7785, Test: 0.7770\n",
      "Epoch: 070, Loss: 0.6722, Val: 0.7762, Test: 0.7747\n",
      "Epoch: 071, Loss: 0.6709, Val: 0.7745, Test: 0.7729\n",
      "Epoch: 072, Loss: 0.6707, Val: 0.7718, Test: 0.7700\n",
      "Epoch: 073, Loss: 0.6708, Val: 0.7694, Test: 0.7675\n",
      "Epoch: 074, Loss: 0.6705, Val: 0.7680, Test: 0.7660\n",
      "Epoch: 075, Loss: 0.6699, Val: 0.7657, Test: 0.7637\n",
      "Epoch: 076, Loss: 0.6701, Val: 0.7635, Test: 0.7612\n",
      "Epoch: 077, Loss: 0.6698, Val: 0.7624, Test: 0.7601\n",
      "Epoch: 078, Loss: 0.6689, Val: 0.7612, Test: 0.7588\n",
      "Epoch: 079, Loss: 0.6697, Val: 0.7592, Test: 0.7566\n",
      "Epoch: 080, Loss: 0.6693, Val: 0.7584, Test: 0.7558\n",
      "Epoch: 081, Loss: 0.6692, Val: 0.7574, Test: 0.7547\n",
      "Epoch: 082, Loss: 0.6699, Val: 0.7546, Test: 0.7516\n",
      "Epoch: 083, Loss: 0.6689, Val: 0.7532, Test: 0.7502\n",
      "Epoch: 084, Loss: 0.6683, Val: 0.7517, Test: 0.7487\n",
      "Epoch: 085, Loss: 0.6681, Val: 0.7491, Test: 0.7459\n",
      "Epoch: 086, Loss: 0.6679, Val: 0.7476, Test: 0.7443\n",
      "Epoch: 087, Loss: 0.6677, Val: 0.7467, Test: 0.7434\n",
      "Epoch: 088, Loss: 0.6682, Val: 0.7442, Test: 0.7408\n",
      "Epoch: 089, Loss: 0.6677, Val: 0.7424, Test: 0.7390\n",
      "Epoch: 090, Loss: 0.6684, Val: 0.7411, Test: 0.7376\n",
      "Epoch: 091, Loss: 0.6677, Val: 0.7389, Test: 0.7354\n",
      "Epoch: 092, Loss: 0.6671, Val: 0.7370, Test: 0.7334\n",
      "Epoch: 093, Loss: 0.6666, Val: 0.7362, Test: 0.7327\n",
      "Epoch: 094, Loss: 0.6658, Val: 0.7358, Test: 0.7322\n",
      "Epoch: 095, Loss: 0.6667, Val: 0.7342, Test: 0.7306\n",
      "Epoch: 096, Loss: 0.6668, Val: 0.7346, Test: 0.7311\n",
      "Epoch: 097, Loss: 0.6670, Val: 0.7345, Test: 0.7309\n",
      "Epoch: 098, Loss: 0.6664, Val: 0.7331, Test: 0.7294\n",
      "Epoch: 099, Loss: 0.6666, Val: 0.7325, Test: 0.7289\n",
      "Epoch: 100, Loss: 0.6661, Val: 0.7320, Test: 0.7285\n",
      "Final Test: 0.8170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7130)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(5, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "## Predict on entire masked graph\n",
    "\n",
    "z = model.encode(data.x, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / (n_L1 + n_L2 + n_L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7731, Val: 0.2906, Test: 0.2989\n",
      "Epoch: 002, Loss: 0.7067, Val: 0.7826, Test: 0.7837\n",
      "Epoch: 003, Loss: 0.6985, Val: 0.7881, Test: 0.7873\n",
      "Epoch: 004, Loss: 0.6960, Val: 0.7889, Test: 0.7888\n",
      "Epoch: 005, Loss: 0.6951, Val: 0.8030, Test: 0.8043\n",
      "Epoch: 006, Loss: 0.6928, Val: 0.8076, Test: 0.8081\n",
      "Epoch: 007, Loss: 0.6898, Val: 0.8094, Test: 0.8097\n",
      "Epoch: 008, Loss: 0.6885, Val: 0.8106, Test: 0.8107\n",
      "Epoch: 009, Loss: 0.6872, Val: 0.8123, Test: 0.8121\n",
      "Epoch: 010, Loss: 0.6854, Val: 0.8136, Test: 0.8131\n",
      "Epoch: 011, Loss: 0.6844, Val: 0.8135, Test: 0.8131\n",
      "Epoch: 012, Loss: 0.6841, Val: 0.8123, Test: 0.8120\n",
      "Epoch: 013, Loss: 0.6833, Val: 0.8111, Test: 0.8110\n",
      "Epoch: 014, Loss: 0.6832, Val: 0.8102, Test: 0.8104\n",
      "Epoch: 015, Loss: 0.6831, Val: 0.8095, Test: 0.8097\n",
      "Epoch: 016, Loss: 0.6824, Val: 0.8084, Test: 0.8089\n",
      "Epoch: 017, Loss: 0.6820, Val: 0.8071, Test: 0.8079\n",
      "Epoch: 018, Loss: 0.6818, Val: 0.8061, Test: 0.8070\n",
      "Epoch: 019, Loss: 0.6819, Val: 0.8053, Test: 0.8062\n",
      "Epoch: 020, Loss: 0.6815, Val: 0.8042, Test: 0.8050\n",
      "Epoch: 021, Loss: 0.6813, Val: 0.8028, Test: 0.8032\n",
      "Epoch: 022, Loss: 0.6803, Val: 0.8014, Test: 0.8014\n",
      "Epoch: 023, Loss: 0.6807, Val: 0.8004, Test: 0.8001\n",
      "Epoch: 024, Loss: 0.6805, Val: 0.8000, Test: 0.7996\n",
      "Epoch: 025, Loss: 0.6803, Val: 0.7997, Test: 0.7994\n",
      "Epoch: 026, Loss: 0.6795, Val: 0.7994, Test: 0.7991\n",
      "Epoch: 027, Loss: 0.6795, Val: 0.7988, Test: 0.7985\n",
      "Epoch: 028, Loss: 0.6797, Val: 0.7985, Test: 0.7981\n",
      "Epoch: 029, Loss: 0.6796, Val: 0.7989, Test: 0.7986\n",
      "Epoch: 030, Loss: 0.6793, Val: 0.7999, Test: 0.7997\n",
      "Epoch: 031, Loss: 0.6789, Val: 0.8007, Test: 0.8004\n",
      "Epoch: 032, Loss: 0.6786, Val: 0.8006, Test: 0.8003\n",
      "Epoch: 033, Loss: 0.6787, Val: 0.8003, Test: 0.7999\n",
      "Epoch: 034, Loss: 0.6784, Val: 0.8006, Test: 0.8002\n",
      "Epoch: 035, Loss: 0.6780, Val: 0.8011, Test: 0.8007\n",
      "Epoch: 036, Loss: 0.6779, Val: 0.8008, Test: 0.8005\n",
      "Epoch: 037, Loss: 0.6776, Val: 0.8002, Test: 0.7998\n",
      "Epoch: 038, Loss: 0.6778, Val: 0.8002, Test: 0.7997\n",
      "Epoch: 039, Loss: 0.6770, Val: 0.8008, Test: 0.8003\n",
      "Epoch: 040, Loss: 0.6764, Val: 0.8009, Test: 0.8003\n",
      "Epoch: 041, Loss: 0.6763, Val: 0.7999, Test: 0.7992\n",
      "Epoch: 042, Loss: 0.6768, Val: 0.7994, Test: 0.7985\n",
      "Epoch: 043, Loss: 0.6761, Val: 0.8002, Test: 0.7992\n",
      "Epoch: 044, Loss: 0.6759, Val: 0.7999, Test: 0.7988\n",
      "Epoch: 045, Loss: 0.6755, Val: 0.7987, Test: 0.7974\n",
      "Epoch: 046, Loss: 0.6750, Val: 0.7983, Test: 0.7970\n",
      "Epoch: 047, Loss: 0.6749, Val: 0.7981, Test: 0.7969\n",
      "Epoch: 048, Loss: 0.6745, Val: 0.7969, Test: 0.7957\n",
      "Epoch: 049, Loss: 0.6745, Val: 0.7959, Test: 0.7947\n",
      "Epoch: 050, Loss: 0.6741, Val: 0.7959, Test: 0.7947\n",
      "Epoch: 051, Loss: 0.6740, Val: 0.7951, Test: 0.7939\n",
      "Epoch: 052, Loss: 0.6733, Val: 0.7941, Test: 0.7928\n",
      "Epoch: 053, Loss: 0.6738, Val: 0.7929, Test: 0.7916\n",
      "Epoch: 054, Loss: 0.6732, Val: 0.7912, Test: 0.7900\n",
      "Epoch: 055, Loss: 0.6728, Val: 0.7889, Test: 0.7877\n",
      "Epoch: 056, Loss: 0.6729, Val: 0.7867, Test: 0.7856\n",
      "Epoch: 057, Loss: 0.6731, Val: 0.7844, Test: 0.7833\n",
      "Epoch: 058, Loss: 0.6724, Val: 0.7823, Test: 0.7811\n",
      "Epoch: 059, Loss: 0.6719, Val: 0.7810, Test: 0.7798\n",
      "Epoch: 060, Loss: 0.6718, Val: 0.7809, Test: 0.7797\n",
      "Epoch: 061, Loss: 0.6715, Val: 0.7809, Test: 0.7797\n",
      "Epoch: 062, Loss: 0.6716, Val: 0.7803, Test: 0.7791\n",
      "Epoch: 063, Loss: 0.6714, Val: 0.7779, Test: 0.7767\n",
      "Epoch: 064, Loss: 0.6706, Val: 0.7744, Test: 0.7730\n",
      "Epoch: 065, Loss: 0.6708, Val: 0.7701, Test: 0.7685\n",
      "Epoch: 066, Loss: 0.6708, Val: 0.7662, Test: 0.7643\n",
      "Epoch: 067, Loss: 0.6706, Val: 0.7636, Test: 0.7616\n",
      "Epoch: 068, Loss: 0.6704, Val: 0.7630, Test: 0.7609\n",
      "Epoch: 069, Loss: 0.6706, Val: 0.7618, Test: 0.7596\n",
      "Epoch: 070, Loss: 0.6696, Val: 0.7624, Test: 0.7602\n",
      "Epoch: 071, Loss: 0.6700, Val: 0.7623, Test: 0.7600\n",
      "Epoch: 072, Loss: 0.6706, Val: 0.7585, Test: 0.7561\n",
      "Epoch: 073, Loss: 0.6697, Val: 0.7565, Test: 0.7538\n",
      "Epoch: 074, Loss: 0.6694, Val: 0.7541, Test: 0.7512\n",
      "Epoch: 075, Loss: 0.6693, Val: 0.7512, Test: 0.7483\n",
      "Epoch: 076, Loss: 0.6684, Val: 0.7518, Test: 0.7488\n",
      "Epoch: 077, Loss: 0.6692, Val: 0.7504, Test: 0.7473\n",
      "Epoch: 078, Loss: 0.6683, Val: 0.7486, Test: 0.7456\n",
      "Epoch: 079, Loss: 0.6690, Val: 0.7473, Test: 0.7441\n",
      "Epoch: 080, Loss: 0.6694, Val: 0.7437, Test: 0.7402\n",
      "Epoch: 081, Loss: 0.6676, Val: 0.7420, Test: 0.7387\n",
      "Epoch: 082, Loss: 0.6685, Val: 0.7435, Test: 0.7401\n",
      "Epoch: 083, Loss: 0.6696, Val: 0.7414, Test: 0.7380\n",
      "Epoch: 084, Loss: 0.6679, Val: 0.7383, Test: 0.7349\n",
      "Epoch: 085, Loss: 0.6695, Val: 0.7378, Test: 0.7340\n",
      "Epoch: 086, Loss: 0.6682, Val: 0.7364, Test: 0.7327\n",
      "Epoch: 087, Loss: 0.6680, Val: 0.7338, Test: 0.7304\n",
      "Epoch: 088, Loss: 0.6685, Val: 0.7360, Test: 0.7324\n",
      "Epoch: 089, Loss: 0.6664, Val: 0.7380, Test: 0.7342\n",
      "Epoch: 090, Loss: 0.6674, Val: 0.7383, Test: 0.7348\n",
      "Epoch: 091, Loss: 0.6667, Val: 0.7372, Test: 0.7336\n",
      "Epoch: 092, Loss: 0.6670, Val: 0.7362, Test: 0.7325\n",
      "Epoch: 093, Loss: 0.6669, Val: 0.7324, Test: 0.7287\n",
      "Epoch: 094, Loss: 0.6667, Val: 0.7277, Test: 0.7244\n",
      "Epoch: 095, Loss: 0.6671, Val: 0.7263, Test: 0.7230\n",
      "Epoch: 096, Loss: 0.6661, Val: 0.7270, Test: 0.7232\n",
      "Epoch: 097, Loss: 0.6665, Val: 0.7282, Test: 0.7248\n",
      "Epoch: 098, Loss: 0.6665, Val: 0.7288, Test: 0.7254\n",
      "Epoch: 099, Loss: 0.6669, Val: 0.7293, Test: 0.7256\n",
      "Epoch: 100, Loss: 0.6663, Val: 0.7271, Test: 0.7233\n",
      "Final Test: 0.8131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7174)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_grdpg), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_grdpg), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_grdpg), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_grdpg), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / (n_L1 + n_L2 + n_L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.9432, Val: 0.2887, Test: 0.2918\n",
      "Epoch: 002, Loss: 0.7201, Val: 0.7597, Test: 0.7645\n",
      "Epoch: 003, Loss: 0.7036, Val: 0.7615, Test: 0.7662\n",
      "Epoch: 004, Loss: 0.7148, Val: 0.7517, Test: 0.7568\n",
      "Epoch: 005, Loss: 0.7144, Val: 0.7481, Test: 0.7535\n",
      "Epoch: 006, Loss: 0.7056, Val: 0.7617, Test: 0.7666\n",
      "Epoch: 007, Loss: 0.6977, Val: 0.7790, Test: 0.7849\n",
      "Epoch: 008, Loss: 0.6941, Val: 0.7875, Test: 0.7947\n",
      "Epoch: 009, Loss: 0.6923, Val: 0.7884, Test: 0.7959\n",
      "Epoch: 010, Loss: 0.6915, Val: 0.7881, Test: 0.7956\n",
      "Epoch: 011, Loss: 0.6910, Val: 0.7877, Test: 0.7955\n",
      "Epoch: 012, Loss: 0.6907, Val: 0.7876, Test: 0.7953\n",
      "Epoch: 013, Loss: 0.6904, Val: 0.7876, Test: 0.7953\n",
      "Epoch: 014, Loss: 0.6900, Val: 0.7878, Test: 0.7954\n",
      "Epoch: 015, Loss: 0.6896, Val: 0.7880, Test: 0.7955\n",
      "Epoch: 016, Loss: 0.6888, Val: 0.7882, Test: 0.7956\n",
      "Epoch: 017, Loss: 0.6882, Val: 0.7883, Test: 0.7957\n",
      "Epoch: 018, Loss: 0.6879, Val: 0.7884, Test: 0.7958\n",
      "Epoch: 019, Loss: 0.6875, Val: 0.7885, Test: 0.7958\n",
      "Epoch: 020, Loss: 0.6875, Val: 0.7885, Test: 0.7959\n",
      "Epoch: 021, Loss: 0.6874, Val: 0.7886, Test: 0.7959\n",
      "Epoch: 022, Loss: 0.6875, Val: 0.7886, Test: 0.7959\n",
      "Epoch: 023, Loss: 0.6873, Val: 0.7886, Test: 0.7959\n",
      "Epoch: 024, Loss: 0.6875, Val: 0.7886, Test: 0.7959\n",
      "Epoch: 025, Loss: 0.6874, Val: 0.7886, Test: 0.7960\n",
      "Epoch: 026, Loss: 0.6869, Val: 0.7887, Test: 0.7960\n",
      "Epoch: 027, Loss: 0.6873, Val: 0.7887, Test: 0.7960\n",
      "Epoch: 028, Loss: 0.6870, Val: 0.7887, Test: 0.7960\n",
      "Epoch: 029, Loss: 0.6871, Val: 0.7888, Test: 0.7961\n",
      "Epoch: 030, Loss: 0.6873, Val: 0.7888, Test: 0.7961\n",
      "Epoch: 031, Loss: 0.6870, Val: 0.7888, Test: 0.7961\n",
      "Epoch: 032, Loss: 0.6867, Val: 0.7889, Test: 0.7962\n",
      "Epoch: 033, Loss: 0.6866, Val: 0.7889, Test: 0.7962\n",
      "Epoch: 034, Loss: 0.6866, Val: 0.7889, Test: 0.7962\n",
      "Epoch: 035, Loss: 0.6867, Val: 0.7889, Test: 0.7962\n",
      "Epoch: 036, Loss: 0.6867, Val: 0.7890, Test: 0.7962\n",
      "Epoch: 037, Loss: 0.6864, Val: 0.7890, Test: 0.7962\n",
      "Epoch: 038, Loss: 0.6864, Val: 0.7890, Test: 0.7963\n",
      "Epoch: 039, Loss: 0.6863, Val: 0.7890, Test: 0.7963\n",
      "Epoch: 040, Loss: 0.6864, Val: 0.7890, Test: 0.7963\n",
      "Epoch: 041, Loss: 0.6863, Val: 0.7890, Test: 0.7963\n",
      "Epoch: 042, Loss: 0.6861, Val: 0.7890, Test: 0.7963\n",
      "Epoch: 043, Loss: 0.6861, Val: 0.7890, Test: 0.7963\n",
      "Epoch: 044, Loss: 0.6860, Val: 0.7890, Test: 0.7963\n",
      "Epoch: 045, Loss: 0.6859, Val: 0.7890, Test: 0.7963\n",
      "Epoch: 046, Loss: 0.6859, Val: 0.7890, Test: 0.7963\n",
      "Epoch: 047, Loss: 0.6860, Val: 0.7889, Test: 0.7962\n",
      "Epoch: 048, Loss: 0.6859, Val: 0.7888, Test: 0.7961\n",
      "Epoch: 049, Loss: 0.6856, Val: 0.7886, Test: 0.7960\n",
      "Epoch: 050, Loss: 0.6857, Val: 0.7884, Test: 0.7959\n",
      "Epoch: 051, Loss: 0.6856, Val: 0.7882, Test: 0.7957\n",
      "Epoch: 052, Loss: 0.6859, Val: 0.7880, Test: 0.7955\n",
      "Epoch: 053, Loss: 0.6853, Val: 0.7877, Test: 0.7952\n",
      "Epoch: 054, Loss: 0.6855, Val: 0.7874, Test: 0.7949\n",
      "Epoch: 055, Loss: 0.6852, Val: 0.7870, Test: 0.7945\n",
      "Epoch: 056, Loss: 0.6852, Val: 0.7865, Test: 0.7939\n",
      "Epoch: 057, Loss: 0.6854, Val: 0.7857, Test: 0.7931\n",
      "Epoch: 058, Loss: 0.6850, Val: 0.7847, Test: 0.7919\n",
      "Epoch: 059, Loss: 0.6851, Val: 0.7834, Test: 0.7906\n",
      "Epoch: 060, Loss: 0.6849, Val: 0.7820, Test: 0.7891\n",
      "Epoch: 061, Loss: 0.6847, Val: 0.7807, Test: 0.7876\n",
      "Epoch: 062, Loss: 0.6850, Val: 0.7792, Test: 0.7861\n",
      "Epoch: 063, Loss: 0.6848, Val: 0.7778, Test: 0.7846\n",
      "Epoch: 064, Loss: 0.6848, Val: 0.7765, Test: 0.7832\n",
      "Epoch: 065, Loss: 0.6846, Val: 0.7752, Test: 0.7818\n",
      "Epoch: 066, Loss: 0.6844, Val: 0.7739, Test: 0.7804\n",
      "Epoch: 067, Loss: 0.6846, Val: 0.7727, Test: 0.7790\n",
      "Epoch: 068, Loss: 0.6847, Val: 0.7713, Test: 0.7775\n",
      "Epoch: 069, Loss: 0.6845, Val: 0.7701, Test: 0.7762\n",
      "Epoch: 070, Loss: 0.6845, Val: 0.7690, Test: 0.7749\n",
      "Epoch: 071, Loss: 0.6840, Val: 0.7682, Test: 0.7740\n",
      "Epoch: 072, Loss: 0.6840, Val: 0.7676, Test: 0.7733\n",
      "Epoch: 073, Loss: 0.6840, Val: 0.7670, Test: 0.7726\n",
      "Epoch: 074, Loss: 0.6840, Val: 0.7665, Test: 0.7720\n",
      "Epoch: 075, Loss: 0.6837, Val: 0.7661, Test: 0.7715\n",
      "Epoch: 076, Loss: 0.6837, Val: 0.7657, Test: 0.7710\n",
      "Epoch: 077, Loss: 0.6832, Val: 0.7653, Test: 0.7706\n",
      "Epoch: 078, Loss: 0.6833, Val: 0.7650, Test: 0.7703\n",
      "Epoch: 079, Loss: 0.6834, Val: 0.7647, Test: 0.7700\n",
      "Epoch: 080, Loss: 0.6831, Val: 0.7645, Test: 0.7697\n",
      "Epoch: 081, Loss: 0.6827, Val: 0.7642, Test: 0.7695\n",
      "Epoch: 082, Loss: 0.6827, Val: 0.7640, Test: 0.7692\n",
      "Epoch: 083, Loss: 0.6828, Val: 0.7637, Test: 0.7690\n",
      "Epoch: 084, Loss: 0.6828, Val: 0.7634, Test: 0.7686\n",
      "Epoch: 085, Loss: 0.6825, Val: 0.7630, Test: 0.7683\n",
      "Epoch: 086, Loss: 0.6825, Val: 0.7627, Test: 0.7680\n",
      "Epoch: 087, Loss: 0.6829, Val: 0.7623, Test: 0.7676\n",
      "Epoch: 088, Loss: 0.6822, Val: 0.7620, Test: 0.7673\n",
      "Epoch: 089, Loss: 0.6821, Val: 0.7616, Test: 0.7669\n",
      "Epoch: 090, Loss: 0.6823, Val: 0.7611, Test: 0.7664\n",
      "Epoch: 091, Loss: 0.6817, Val: 0.7607, Test: 0.7660\n",
      "Epoch: 092, Loss: 0.6818, Val: 0.7603, Test: 0.7656\n",
      "Epoch: 093, Loss: 0.6819, Val: 0.7598, Test: 0.7650\n",
      "Epoch: 094, Loss: 0.6813, Val: 0.7594, Test: 0.7645\n",
      "Epoch: 095, Loss: 0.6811, Val: 0.7591, Test: 0.7642\n",
      "Epoch: 096, Loss: 0.6812, Val: 0.7587, Test: 0.7638\n",
      "Epoch: 097, Loss: 0.6813, Val: 0.7580, Test: 0.7631\n",
      "Epoch: 098, Loss: 0.6806, Val: 0.7576, Test: 0.7626\n",
      "Epoch: 099, Loss: 0.6805, Val: 0.7572, Test: 0.7623\n",
      "Epoch: 100, Loss: 0.6803, Val: 0.7568, Test: 0.7618\n",
      "Final Test: 0.7963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6380)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_ase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_ase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / (n_L1 + n_L2 + n_L3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pycountry_convert as pc\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from cycler import cycler\n",
    "\n",
    "resolutions_issues = {'me': 'Palestinian conflict', \n",
    "                      'nu': 'Nuclear weapons and nuclear material', \n",
    "                      'di': 'Arms control and disarmament',\n",
    "                      'co': 'Colonialism',\n",
    "                      'hr': 'Human rights',\n",
    "                      'ec': 'Economic Development',\n",
    "                      'N/A': 'Not specified'}\n",
    "\n",
    "resolutions_issues_color = {'me': 'salmon', \n",
    "                            'nu': 'yellow', \n",
    "                            'di': 'teal',\n",
    "                            'co': 'orchid',\n",
    "                            'hr': 'navy',\n",
    "                            'ec': 'orange',\n",
    "                            'N/A': 'black'}\n",
    "\n",
    "continents_colors = {'North America': 'yellow',\n",
    "                     'South America': 'forestgreen',\n",
    "                     'Europe': 'royalblue',\n",
    "                     'Africa': 'plum',\n",
    "                     'Asia': 'darkorange',\n",
    "                     'Oceania': 'firebrick'}\n",
    "\n",
    "cycler_colors = ['royalblue','firebrick','forestgreen','olive']\n",
    "\n",
    "\n",
    "def load_un_dataset(un_data_path, initial_year=1946, final_year=2018, remove_nonmembers=True, remove_nonpresent=False, unknown_votes=False):\n",
    "    \n",
    "    if os.path.isdir(os.path.dirname(un_data_path)):\n",
    "        if not os.path.exists(un_data_path):\n",
    "            download_un_dataset(un_data_path)\n",
    "    else:\n",
    "        raise Exception(\"Provided path for UN dataset is not reachable\")\n",
    "    \n",
    "    # Load data    \n",
    "    votes_df = pd.read_csv(un_data_path, low_memory=False, encoding='latin-1', index_col=0)\n",
    "    # Keep only desired years\n",
    "    votes_df = votes_df[votes_df.year>=initial_year]\n",
    "    votes_df = votes_df[votes_df.year<=final_year]\n",
    "    \n",
    "    if remove_nonmembers:\n",
    "        # Remove votes by nonmembers\n",
    "        votes_df = votes_df[votes_df.vote!=9]\n",
    "    \n",
    "    if remove_nonpresent:\n",
    "        # Remove votes by nonmembers\n",
    "        votes_df = votes_df[votes_df.vote!=8]\n",
    "        \n",
    "    # Edges in graph represent an affirmative vote\n",
    "    votes_df['weight'] = (votes_df.vote==1)\n",
    "    \n",
    "    if unknown_votes:\n",
    "        # Voters preference is assumed unknown if it is an abstention or voter is not present\n",
    "        votes_df['unknown'] =  (votes_df.vote==2) | (votes_df.vote==8)\n",
    "    \n",
    "    votes_df['res_features'] = votes_df[['me','nu','di','co','hr','ec']].apply(lambda row: np.array(row), axis=1)\n",
    "\n",
    "    return votes_df\n",
    "\n",
    "        \n",
    "def download_un_dataset(filename='UNVotes-1.csv', data_url='https://dataverse.harvard.edu/api/access/datafile/6358426'):\n",
    "    # Code from https://gist.github.com/yanqd0/c13ed29e29432e3cf3e7c38467f42f51\n",
    "    response = requests.get(data_url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length',0))\n",
    "    with open(filename, \"wb\") as f, tqdm(desc='Downloading UN dataset', total=total_size, unit='B', unit_divisor=1024, unit_scale=True) as pbar:\n",
    "        for un_data in response.iter_content(chunk_size=1024):\n",
    "            size = f.write(un_data)\n",
    "            pbar.update(size)\n",
    "            \n",
    "def get_continents_dict(votes_df):\n",
    "    continents_dict = {}\n",
    "    countries = votes_df.Country.unique()\n",
    "    for country in countries: \n",
    "        try:\n",
    "            continent_code = pc.country_alpha2_to_continent_code(pc.country_alpha3_to_country_alpha2(country))\n",
    "            continents_dict[country] = pc.convert_continent_code_to_continent_name(continent_code)\n",
    "        except:\n",
    "            continue\n",
    "            # print(pais)\n",
    "            \n",
    "    continents_dict['DDR'] = 'Europe'\n",
    "    continents_dict['CSK'] = 'Europe'\n",
    "    continents_dict['YUG'] = 'Europe'\n",
    "    continents_dict['EAZ'] = 'Africa'\n",
    "    continents_dict['YAR'] = 'Asia'\n",
    "    continents_dict['TLS'] = 'Asia'\n",
    "    \n",
    "    return continents_dict\n",
    "\n",
    "def get_countries_name_conversion_dict(votes_df):\n",
    "    countries = votes_df.Countryname.unique()\n",
    "    conversion_dict = {}\n",
    "    for country in countries: \n",
    "        conversion_dict[country] = votes_df[votes_df.Countryname==country].Country.unique()[0]\n",
    "        \n",
    "    return conversion_dict\n",
    "\n",
    "def create_un_graphs(votes_df):\n",
    "    \n",
    "    continents_dict = get_continents_dict(votes_df)\n",
    "    conversion_dict = get_countries_name_conversion_dict(votes_df)\n",
    "    \n",
    "    all_graphs = {}\n",
    "    \n",
    "    initial_year = votes_df.year.min()\n",
    "    final_year = votes_df.year.max()\n",
    "    \n",
    "    edge_attr = ['weight', 'unknown'] if 'unknown' in votes_df.columns else 'weight'    \n",
    "    \n",
    "    for year in range(initial_year,final_year+1):\n",
    "        \n",
    "        votes_df_year= votes_df[votes_df.year==year]\n",
    "        \n",
    "        g = nx.from_pandas_edgelist(votes_df_year,source='Countryname',target='resid',edge_attr=edge_attr,create_using=nx.DiGraph())\n",
    "        if g.number_of_edges()>0:\n",
    "            \n",
    "            countries_list = votes_df_year.Countryname.unique()\n",
    "            \n",
    "            # Add country's code and continent as graph attributes\n",
    "            countries_codes = {}\n",
    "            countries_continents = {}\n",
    "            nodes_colors = {}\n",
    "            node_types = {}\n",
    "            for country in countries_list:\n",
    "                countries_codes[country] = conversion_dict[country]\n",
    "                countries_continents[country] = continents_dict[conversion_dict[country]]\n",
    "                nodes_colors[country] = continents_colors[countries_continents[country]]\n",
    "                node_types[country] = \"country\"\n",
    "                \n",
    "            nx.set_node_attributes(g, countries_codes, name='country code')\n",
    "            nx.set_node_attributes(g, countries_continents, name='continent')\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Add resolution's issue as graph attribute\n",
    "            resolutions_list = votes_df_year.resid.unique()\n",
    "            resolutions_issues_dict = {}\n",
    "            important_resolutions_dict = {}\n",
    "            resolutions_features = {}\n",
    "\n",
    "            \n",
    "            for resolution_id in resolutions_list:\n",
    "                df_votes_sum = votes_df_year[votes_df_year.resid==resolution_id][['me','nu','di','co','hr','ec']].sum()\n",
    "                if df_votes_sum.max()>0:\n",
    "                    resolutions_issues_dict[resolution_id] = df_votes_sum.idxmax()\n",
    "                else:\n",
    "                    resolutions_issues_dict[resolution_id] = 'N/A'\n",
    "                    \n",
    "                nodes_colors[resolution_id] = resolutions_issues_color[resolutions_issues_dict[resolution_id]]\n",
    "                resolutions_features[resolution_id] = votes_df_year[votes_df_year.resid==resolution_id]['res_features'].mean()\n",
    "                node_types[resolution_id] = \"resolution\"\n",
    "                \n",
    "                important_vote = votes_df_year[votes_df_year.resid==resolution_id]['importantvote'].max()\n",
    "                if important_vote > 0:\n",
    "                    important_resolutions_dict[resolution_id] = True\n",
    "                else:\n",
    "                    important_resolutions_dict[resolution_id] = False\n",
    "            \n",
    "            nx.set_node_attributes(g, resolutions_issues_dict,name='issue code')\n",
    "            nx.set_node_attributes(g, nodes_colors, name='color')\n",
    "            nx.set_node_attributes(g, important_resolutions_dict, name='important vote')\n",
    "            nx.set_node_attributes(g, node_types, name='type') \n",
    "            nx.set_node_attributes(g, resolutions_features, name='res_features') \n",
    "                \n",
    "            all_graphs[year] = g\n",
    "            \n",
    "    return all_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rcid</th>\n",
       "      <th>ccode</th>\n",
       "      <th>member</th>\n",
       "      <th>vote</th>\n",
       "      <th>Country</th>\n",
       "      <th>Countryname</th>\n",
       "      <th>year</th>\n",
       "      <th>session</th>\n",
       "      <th>abstain</th>\n",
       "      <th>yes</th>\n",
       "      <th>...</th>\n",
       "      <th>nu</th>\n",
       "      <th>di</th>\n",
       "      <th>hr</th>\n",
       "      <th>co</th>\n",
       "      <th>ec</th>\n",
       "      <th>ident</th>\n",
       "      <th>resid</th>\n",
       "      <th>weight</th>\n",
       "      <th>unknown</th>\n",
       "      <th>res_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>USA</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>CUB</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>HTI</td>\n",
       "      <td>Haiti</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DOM</td>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199018</th>\n",
       "      <td>6102</td>\n",
       "      <td>552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2018</td>\n",
       "      <td>73</td>\n",
       "      <td>12.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73090</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199019</th>\n",
       "      <td>6138</td>\n",
       "      <td>552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2018</td>\n",
       "      <td>73</td>\n",
       "      <td>33.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73091</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199020</th>\n",
       "      <td>6139</td>\n",
       "      <td>552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2018</td>\n",
       "      <td>73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73092</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199021</th>\n",
       "      <td>6097</td>\n",
       "      <td>552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2018</td>\n",
       "      <td>73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73093</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199022</th>\n",
       "      <td>6098</td>\n",
       "      <td>552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2018</td>\n",
       "      <td>73</td>\n",
       "      <td>29.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73094</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>943616 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rcid  ccode  member  vote Country               Countryname  year  \\\n",
       "1           3      2     1.0     1     USA  United States of America  1946   \n",
       "2           3     20     1.0     3     CAN                    Canada  1946   \n",
       "4           3     40     1.0     1     CUB                      Cuba  1946   \n",
       "5           3     41     1.0     1     HTI                     Haiti  1946   \n",
       "6           3     42     1.0     1     DOM        Dominican Republic  1946   \n",
       "...       ...    ...     ...   ...     ...                       ...   ...   \n",
       "1199018  6102    552     1.0     1     ZWE                  Zimbabwe  2018   \n",
       "1199019  6138    552     1.0     1     ZWE                  Zimbabwe  2018   \n",
       "1199020  6139    552     1.0     1     ZWE                  Zimbabwe  2018   \n",
       "1199021  6097    552     1.0     1     ZWE                  Zimbabwe  2018   \n",
       "1199022  6098    552     1.0     2     ZWE                  Zimbabwe  2018   \n",
       "\n",
       "         session  abstain    yes  ...  nu  di hr co  ec  ident  resid weight  \\\n",
       "1              1      4.0   29.0  ...   0   0  0  0   0    0.0   1001   True   \n",
       "2              1      4.0   29.0  ...   0   0  0  0   0    0.0   1001  False   \n",
       "4              1      4.0   29.0  ...   0   0  0  0   0    0.0   1001   True   \n",
       "5              1      4.0   29.0  ...   0   0  0  0   0    0.0   1001   True   \n",
       "6              1      4.0   29.0  ...   0   0  0  0   0    0.0   1001   True   \n",
       "...          ...      ...    ...  ...  ..  .. .. ..  ..    ...    ...    ...   \n",
       "1199018       73     12.0  156.0  ...   0   0  0  0   0    0.0  73090   True   \n",
       "1199019       73     33.0   94.0  ...   0   0  0  0   0    0.0  73091   True   \n",
       "1199020       73      0.0  188.0  ...   0   0  0  0   1    0.0  73092   True   \n",
       "1199021       73      1.0  180.0  ...   0   0  0  0   0    0.0  73093   True   \n",
       "1199022       73     29.0  151.0  ...   0   0  0  0   1    0.0  73094  False   \n",
       "\n",
       "         unknown        res_features  \n",
       "1          False  [0, 0, 0, 0, 0, 0]  \n",
       "2          False  [0, 0, 0, 0, 0, 0]  \n",
       "4          False  [0, 0, 0, 0, 0, 0]  \n",
       "5          False  [0, 0, 0, 0, 0, 0]  \n",
       "6          False  [0, 0, 0, 0, 0, 0]  \n",
       "...          ...                 ...  \n",
       "1199018    False  [0, 0, 0, 0, 0, 0]  \n",
       "1199019    False  [0, 0, 0, 0, 0, 0]  \n",
       "1199020    False  [0, 0, 0, 0, 0, 1]  \n",
       "1199021    False  [0, 0, 0, 0, 0, 0]  \n",
       "1199022     True  [0, 0, 0, 0, 0, 1]  \n",
       "\n",
       "[943616 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes_df = load_un_dataset('data/UNVotes-1.csv', unknown_votes=True)\n",
    "votes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rcid', 'ccode', 'member', 'vote', 'Country', 'Countryname', 'year',\n",
       "       'session', 'abstain', 'yes', 'no', 'importantvote', 'date', 'unres',\n",
       "       'amend', 'para', 'short', 'descr', 'me', 'nu', 'di', 'hr', 'co', 'ec',\n",
       "       'ident', 'resid', 'weight', 'unknown', 'res_features'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graphs = create_un_graphs(votes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = all_graphs[2018].to_undirected()\n",
    "# rename nodes\n",
    "mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "G_ = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "country_indexes = []\n",
    "res_indexes = []\n",
    "features = []\n",
    "\n",
    "for node, data in G_.nodes(data=True):\n",
    "    if data[\"type\"] == \"country\":\n",
    "        country_indexes.append(node)\n",
    "        features.append(np.ones(6))\n",
    "    else:\n",
    "        res_indexes.append(node)\n",
    "        features.append(data['res_features'])\n",
    "\n",
    "\n",
    "unknown_edges = []\n",
    "\n",
    "for u, v, data in G_.edges(data=True):\n",
    "    if data['unknown']:\n",
    "        unknown_edges.append((u,v))\n",
    "        unknown_edges.append((v,u))\n",
    "    \n",
    "\n",
    "\n",
    "adj_matrix = nx.adjacency_matrix(G).todense().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "num_nodes = adj_matrix.shape[0]\n",
    "\n",
    "edge_index = torch.tensor(adj_matrix).nonzero().t().contiguous()\n",
    "\n",
    "mask = torch.ones([num_nodes,num_nodes]).squeeze(0)\n",
    "\n",
    "random.seed(42)\n",
    "# not_present_countries = random.sample(country_indexes, 10)\n",
    "\n",
    "for i in country_indexes:\n",
    "    votos = (torch.rand(1, num_nodes) < 0.3).int()\n",
    "    mask[i,:] = votos\n",
    "    mask[:,i] = votos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.tensor(features)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1.]])\n",
      "Iteraciones:  100\n",
      "Loss:  tensor(16.3506)\n"
     ]
    }
   ],
   "source": [
    "## ASE \n",
    "\n",
    "d = 6\n",
    "adj_matrix = to_dense_adj(edge_index.to('cpu')).squeeze(0)\n",
    "ase = AdjacencySpectralEmbed(n_components=d, diag_aug=True, algorithm='full')\n",
    "masked_adj = adj_matrix*mask\n",
    "x_ase = ase.fit_transform(masked_adj.numpy())\n",
    "x_ase = torch.from_numpy(x_ase)\n",
    "\n",
    "A = to_dense_adj(edge_index.to('cpu'), max_num_nodes=num_nodes).squeeze(0)\n",
    "\n",
    "u, V = torch.linalg.eig(A)\n",
    "\n",
    "list_q=[]\n",
    "for i in range(d):\n",
    "    if u[i].numpy()>0:\n",
    "        list_q.append(1)\n",
    "    else:\n",
    "        list_q.append(-1)\n",
    "        \n",
    "# list_q.sort(reverse=True)\n",
    "q = torch.Tensor(list_q)\n",
    "Q=torch.diag(q)\n",
    "\n",
    "print(Q)\n",
    "\n",
    "\n",
    "torch.norm((x_ase@Q@x_ase.T - to_dense_adj(edge_index).squeeze(0))*mask)\n",
    "\n",
    "\n",
    "x_grdpg, cost, k  = GRDPG_GD_Armijo(x_ase, edge_index, Q, mask.nonzero().t().contiguous())\n",
    "x_grdpg = x_grdpg.detach()\n",
    "print(\"Iteraciones: \", k)\n",
    "print(\"Loss: \", torch.norm((x_grdpg@Q@x_grdpg.T - to_dense_adj(edge_index).squeeze(0))*to_dense_adj(mask.nonzero().t().contiguous()).squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(56.0560, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(23.0081, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(21.7985, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(26.5601, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(21.6407, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(21.0882, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(20.8992, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(20.5893, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(20.2220, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(20.0568, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gd_steps = 10\n",
    "lr = 1e-2\n",
    "device = 'cuda'\n",
    "model = gLASE(d,d, gd_steps)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "## Initialization\n",
    "for step in range(gd_steps):\n",
    "    model.gd[step].lin1.weight.data = (torch.eye(d,d)*lr).to(device)#torch.nn.init.xavier_uniform_(model.gd[step].lin1.weight)*lr\n",
    "    model.gd[step].lin2.weight.data = (torch.eye(d,d)*lr).to(device)#torch.nn.init.xavier_uniform_(model.gd[step].lin2.weight)*lr\n",
    "    \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Define ATT mask\n",
    "edge_index_2 = torch.ones([num_nodes,num_nodes],).nonzero().t().contiguous().to(device)\n",
    "mask = mask.to(device)\n",
    "x_ase = x_ase.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "Q = Q.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x_ase, edge_index, edge_index_2, Q, mask.nonzero().t().contiguous())\n",
    "    loss = torch.norm((out@Q@out.T - to_dense_adj(edge_index).squeeze(0))*mask)\n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "\n",
    "    if epoch % 100 ==0:\n",
    "        print(loss)\n",
    "\n",
    "\n",
    "x_glase = out.detach().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Train, Val, Test\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "masked_edge_index = masked_adj.nonzero().t().contiguous()\n",
    "\n",
    "data = Data(x=features.float(), x_ase=x_ase, x_glase=x_glase, edge_index=masked_edge_index)\n",
    "# torch.manual_seed(42)\n",
    "# random_features=torch.rand([288, 6])\n",
    "# random_features\n",
    "\n",
    "data = Data(x=features.float(), x_ase=x_ase, x_glase=x_glase, edge_index=masked_edge_index)\n",
    "\n",
    "\n",
    "transform = T.Compose([\n",
    "    # T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.1535, Val: 0.3751, Test: 0.3485\n",
      "Epoch: 002, Loss: 0.7430, Val: 0.3933, Test: 0.3717\n",
      "Epoch: 003, Loss: 0.7176, Val: 0.4130, Test: 0.3915\n",
      "Epoch: 004, Loss: 0.7347, Val: 0.4162, Test: 0.3878\n",
      "Epoch: 005, Loss: 0.7339, Val: 0.4079, Test: 0.3785\n",
      "Epoch: 006, Loss: 0.7240, Val: 0.4022, Test: 0.3710\n",
      "Epoch: 007, Loss: 0.7141, Val: 0.4022, Test: 0.3639\n",
      "Epoch: 008, Loss: 0.7069, Val: 0.4091, Test: 0.3573\n",
      "Epoch: 009, Loss: 0.7030, Val: 0.4162, Test: 0.3424\n",
      "Epoch: 010, Loss: 0.7000, Val: 0.4208, Test: 0.3247\n",
      "Epoch: 011, Loss: 0.6993, Val: 0.4334, Test: 0.3181\n",
      "Epoch: 012, Loss: 0.6969, Val: 0.4635, Test: 0.3412\n",
      "Epoch: 013, Loss: 0.6963, Val: 0.5278, Test: 0.3952\n",
      "Epoch: 014, Loss: 0.6954, Val: 0.6175, Test: 0.4778\n",
      "Epoch: 015, Loss: 0.6946, Val: 0.6795, Test: 0.5387\n",
      "Epoch: 016, Loss: 0.6940, Val: 0.7073, Test: 0.5778\n",
      "Epoch: 017, Loss: 0.6932, Val: 0.7202, Test: 0.6027\n",
      "Epoch: 018, Loss: 0.6920, Val: 0.7227, Test: 0.6140\n",
      "Epoch: 019, Loss: 0.6919, Val: 0.7218, Test: 0.6180\n",
      "Epoch: 020, Loss: 0.6922, Val: 0.7215, Test: 0.6183\n",
      "Epoch: 021, Loss: 0.6908, Val: 0.7220, Test: 0.6210\n",
      "Epoch: 022, Loss: 0.6912, Val: 0.7234, Test: 0.6218\n",
      "Epoch: 023, Loss: 0.6904, Val: 0.7245, Test: 0.6229\n",
      "Epoch: 024, Loss: 0.6895, Val: 0.7259, Test: 0.6236\n",
      "Epoch: 025, Loss: 0.6881, Val: 0.7287, Test: 0.6251\n",
      "Epoch: 026, Loss: 0.6887, Val: 0.7300, Test: 0.6250\n",
      "Epoch: 027, Loss: 0.6885, Val: 0.7309, Test: 0.6247\n",
      "Epoch: 028, Loss: 0.6869, Val: 0.7314, Test: 0.6248\n",
      "Epoch: 029, Loss: 0.6887, Val: 0.7332, Test: 0.6251\n",
      "Epoch: 030, Loss: 0.6879, Val: 0.7335, Test: 0.6248\n",
      "Epoch: 031, Loss: 0.6893, Val: 0.7365, Test: 0.6239\n",
      "Epoch: 032, Loss: 0.6878, Val: 0.7378, Test: 0.6236\n",
      "Epoch: 033, Loss: 0.6889, Val: 0.7362, Test: 0.6225\n",
      "Epoch: 034, Loss: 0.6873, Val: 0.7339, Test: 0.6210\n",
      "Epoch: 035, Loss: 0.6878, Val: 0.7335, Test: 0.6198\n",
      "Epoch: 036, Loss: 0.6870, Val: 0.7348, Test: 0.6196\n",
      "Epoch: 037, Loss: 0.6879, Val: 0.7328, Test: 0.6197\n",
      "Epoch: 038, Loss: 0.6870, Val: 0.7303, Test: 0.6213\n",
      "Epoch: 039, Loss: 0.6873, Val: 0.7305, Test: 0.6220\n",
      "Epoch: 040, Loss: 0.6879, Val: 0.7321, Test: 0.6216\n",
      "Epoch: 041, Loss: 0.6859, Val: 0.7316, Test: 0.6206\n",
      "Epoch: 042, Loss: 0.6874, Val: 0.7323, Test: 0.6198\n",
      "Epoch: 043, Loss: 0.6861, Val: 0.7335, Test: 0.6204\n",
      "Epoch: 044, Loss: 0.6883, Val: 0.7323, Test: 0.6198\n",
      "Epoch: 045, Loss: 0.6867, Val: 0.7323, Test: 0.6187\n",
      "Epoch: 046, Loss: 0.6869, Val: 0.7298, Test: 0.6164\n",
      "Epoch: 047, Loss: 0.6852, Val: 0.7312, Test: 0.6176\n",
      "Epoch: 048, Loss: 0.6868, Val: 0.7323, Test: 0.6188\n",
      "Epoch: 049, Loss: 0.6863, Val: 0.7335, Test: 0.6194\n",
      "Epoch: 050, Loss: 0.6869, Val: 0.7321, Test: 0.6193\n",
      "Epoch: 051, Loss: 0.6858, Val: 0.7307, Test: 0.6176\n",
      "Epoch: 052, Loss: 0.6861, Val: 0.7321, Test: 0.6178\n",
      "Epoch: 053, Loss: 0.6845, Val: 0.7309, Test: 0.6171\n",
      "Epoch: 054, Loss: 0.6863, Val: 0.7323, Test: 0.6172\n",
      "Epoch: 055, Loss: 0.6853, Val: 0.7314, Test: 0.6153\n",
      "Epoch: 056, Loss: 0.6860, Val: 0.7284, Test: 0.6153\n",
      "Epoch: 057, Loss: 0.6855, Val: 0.7261, Test: 0.6124\n",
      "Epoch: 058, Loss: 0.6846, Val: 0.7300, Test: 0.6143\n",
      "Epoch: 059, Loss: 0.6836, Val: 0.7296, Test: 0.6153\n",
      "Epoch: 060, Loss: 0.6854, Val: 0.7298, Test: 0.6159\n",
      "Epoch: 061, Loss: 0.6850, Val: 0.7291, Test: 0.6164\n",
      "Epoch: 062, Loss: 0.6846, Val: 0.7289, Test: 0.6154\n",
      "Epoch: 063, Loss: 0.6858, Val: 0.7289, Test: 0.6139\n",
      "Epoch: 064, Loss: 0.6851, Val: 0.7273, Test: 0.6102\n",
      "Epoch: 065, Loss: 0.6841, Val: 0.7231, Test: 0.6053\n",
      "Epoch: 066, Loss: 0.6846, Val: 0.7195, Test: 0.6027\n",
      "Epoch: 067, Loss: 0.6832, Val: 0.7195, Test: 0.6045\n",
      "Epoch: 068, Loss: 0.6838, Val: 0.7229, Test: 0.6107\n",
      "Epoch: 069, Loss: 0.6845, Val: 0.7215, Test: 0.6112\n",
      "Epoch: 070, Loss: 0.6835, Val: 0.7199, Test: 0.6119\n",
      "Epoch: 071, Loss: 0.6817, Val: 0.7167, Test: 0.6101\n",
      "Epoch: 072, Loss: 0.6846, Val: 0.7167, Test: 0.6045\n",
      "Epoch: 073, Loss: 0.6829, Val: 0.7107, Test: 0.6028\n",
      "Epoch: 074, Loss: 0.6836, Val: 0.7073, Test: 0.5987\n",
      "Epoch: 075, Loss: 0.6830, Val: 0.7066, Test: 0.5978\n",
      "Epoch: 076, Loss: 0.6824, Val: 0.7080, Test: 0.5995\n",
      "Epoch: 077, Loss: 0.6820, Val: 0.7101, Test: 0.6031\n",
      "Epoch: 078, Loss: 0.6810, Val: 0.7084, Test: 0.6078\n",
      "Epoch: 079, Loss: 0.6814, Val: 0.7096, Test: 0.6072\n",
      "Epoch: 080, Loss: 0.6809, Val: 0.7075, Test: 0.6071\n",
      "Epoch: 081, Loss: 0.6771, Val: 0.7098, Test: 0.6096\n",
      "Epoch: 082, Loss: 0.6815, Val: 0.7048, Test: 0.6061\n",
      "Epoch: 083, Loss: 0.6807, Val: 0.6995, Test: 0.5974\n",
      "Epoch: 084, Loss: 0.6789, Val: 0.6926, Test: 0.5923\n",
      "Epoch: 085, Loss: 0.6802, Val: 0.6919, Test: 0.5913\n",
      "Epoch: 086, Loss: 0.6813, Val: 0.6961, Test: 0.5931\n",
      "Epoch: 087, Loss: 0.6832, Val: 0.6974, Test: 0.5950\n",
      "Epoch: 088, Loss: 0.6809, Val: 0.7011, Test: 0.5964\n",
      "Epoch: 089, Loss: 0.6807, Val: 0.7002, Test: 0.5978\n",
      "Epoch: 090, Loss: 0.6823, Val: 0.6974, Test: 0.5962\n",
      "Epoch: 091, Loss: 0.6785, Val: 0.6972, Test: 0.5945\n",
      "Epoch: 092, Loss: 0.6801, Val: 0.6926, Test: 0.5931\n",
      "Epoch: 093, Loss: 0.6807, Val: 0.6864, Test: 0.5919\n",
      "Epoch: 094, Loss: 0.6776, Val: 0.6857, Test: 0.5919\n",
      "Epoch: 095, Loss: 0.6812, Val: 0.6820, Test: 0.5906\n",
      "Epoch: 096, Loss: 0.6776, Val: 0.6811, Test: 0.5904\n",
      "Epoch: 097, Loss: 0.6793, Val: 0.6795, Test: 0.5906\n",
      "Epoch: 098, Loss: 0.6786, Val: 0.6786, Test: 0.5902\n",
      "Epoch: 099, Loss: 0.6804, Val: 0.6749, Test: 0.5884\n",
      "Epoch: 100, Loss: 0.6825, Val: 0.6685, Test: 0.5860\n",
      "Final Test: 0.6236\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(6, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5518)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "z = model.encode(data.x, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "inverted_mask_matrix = torch.ones([num_nodes,num_nodes]).squeeze(0) - mask.to('cpu')\n",
    "\n",
    "(adj_matrix*inverted_mask_matrix == predicted_adj*inverted_mask_matrix).sum() / adj_matrix.shape[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.2692, Val: 0.3781, Test: 0.3453\n",
      "Epoch: 002, Loss: 0.7576, Val: 0.4183, Test: 0.3903\n",
      "Epoch: 003, Loss: 0.7321, Val: 0.4477, Test: 0.4240\n",
      "Epoch: 004, Loss: 0.7378, Val: 0.4500, Test: 0.4237\n",
      "Epoch: 005, Loss: 0.7303, Val: 0.4463, Test: 0.4169\n",
      "Epoch: 006, Loss: 0.7177, Val: 0.4470, Test: 0.4086\n",
      "Epoch: 007, Loss: 0.7083, Val: 0.4405, Test: 0.4025\n",
      "Epoch: 008, Loss: 0.7026, Val: 0.4394, Test: 0.3883\n",
      "Epoch: 009, Loss: 0.7010, Val: 0.4461, Test: 0.3746\n",
      "Epoch: 010, Loss: 0.6993, Val: 0.4667, Test: 0.3799\n",
      "Epoch: 011, Loss: 0.6980, Val: 0.5211, Test: 0.4205\n",
      "Epoch: 012, Loss: 0.6961, Val: 0.5712, Test: 0.4763\n",
      "Epoch: 013, Loss: 0.6956, Val: 0.6084, Test: 0.5244\n",
      "Epoch: 014, Loss: 0.6938, Val: 0.6439, Test: 0.5735\n",
      "Epoch: 015, Loss: 0.6928, Val: 0.6646, Test: 0.6085\n",
      "Epoch: 016, Loss: 0.6923, Val: 0.6745, Test: 0.6212\n",
      "Epoch: 017, Loss: 0.6912, Val: 0.6742, Test: 0.6260\n",
      "Epoch: 018, Loss: 0.6905, Val: 0.6765, Test: 0.6309\n",
      "Epoch: 019, Loss: 0.6903, Val: 0.6772, Test: 0.6326\n",
      "Epoch: 020, Loss: 0.6901, Val: 0.6758, Test: 0.6337\n",
      "Epoch: 021, Loss: 0.6909, Val: 0.6761, Test: 0.6333\n",
      "Epoch: 022, Loss: 0.6886, Val: 0.6786, Test: 0.6344\n",
      "Epoch: 023, Loss: 0.6882, Val: 0.6765, Test: 0.6330\n",
      "Epoch: 024, Loss: 0.6879, Val: 0.6747, Test: 0.6331\n",
      "Epoch: 025, Loss: 0.6892, Val: 0.6745, Test: 0.6351\n",
      "Epoch: 026, Loss: 0.6882, Val: 0.6745, Test: 0.6362\n",
      "Epoch: 027, Loss: 0.6876, Val: 0.6742, Test: 0.6371\n",
      "Epoch: 028, Loss: 0.6871, Val: 0.6715, Test: 0.6362\n",
      "Epoch: 029, Loss: 0.6860, Val: 0.6669, Test: 0.6354\n",
      "Epoch: 030, Loss: 0.6851, Val: 0.6635, Test: 0.6327\n",
      "Epoch: 031, Loss: 0.6847, Val: 0.6600, Test: 0.6309\n",
      "Epoch: 032, Loss: 0.6865, Val: 0.6600, Test: 0.6290\n",
      "Epoch: 033, Loss: 0.6854, Val: 0.6623, Test: 0.6269\n",
      "Epoch: 034, Loss: 0.6839, Val: 0.6616, Test: 0.6255\n",
      "Epoch: 035, Loss: 0.6851, Val: 0.6605, Test: 0.6227\n",
      "Epoch: 036, Loss: 0.6858, Val: 0.6570, Test: 0.6182\n",
      "Epoch: 037, Loss: 0.6838, Val: 0.6547, Test: 0.6143\n",
      "Epoch: 038, Loss: 0.6850, Val: 0.6513, Test: 0.6129\n",
      "Epoch: 039, Loss: 0.6820, Val: 0.6492, Test: 0.6114\n",
      "Epoch: 040, Loss: 0.6811, Val: 0.6462, Test: 0.6114\n",
      "Epoch: 041, Loss: 0.6830, Val: 0.6449, Test: 0.6100\n",
      "Epoch: 042, Loss: 0.6821, Val: 0.6435, Test: 0.6058\n",
      "Epoch: 043, Loss: 0.6837, Val: 0.6407, Test: 0.6018\n",
      "Epoch: 044, Loss: 0.6821, Val: 0.6350, Test: 0.5985\n",
      "Epoch: 045, Loss: 0.6830, Val: 0.6309, Test: 0.5942\n",
      "Epoch: 046, Loss: 0.6797, Val: 0.6244, Test: 0.5894\n",
      "Epoch: 047, Loss: 0.6802, Val: 0.6159, Test: 0.5903\n",
      "Epoch: 048, Loss: 0.6803, Val: 0.6141, Test: 0.5950\n",
      "Epoch: 049, Loss: 0.6766, Val: 0.6146, Test: 0.5971\n",
      "Epoch: 050, Loss: 0.6769, Val: 0.6155, Test: 0.5979\n",
      "Epoch: 051, Loss: 0.6748, Val: 0.6157, Test: 0.5969\n",
      "Epoch: 052, Loss: 0.6775, Val: 0.6244, Test: 0.5950\n",
      "Epoch: 053, Loss: 0.6719, Val: 0.6272, Test: 0.5925\n",
      "Epoch: 054, Loss: 0.6729, Val: 0.6272, Test: 0.5917\n",
      "Epoch: 055, Loss: 0.6701, Val: 0.6329, Test: 0.5907\n",
      "Epoch: 056, Loss: 0.6689, Val: 0.6336, Test: 0.5899\n",
      "Epoch: 057, Loss: 0.6666, Val: 0.6352, Test: 0.5958\n",
      "Epoch: 058, Loss: 0.6632, Val: 0.6201, Test: 0.6067\n",
      "Epoch: 059, Loss: 0.6614, Val: 0.6152, Test: 0.6113\n",
      "Epoch: 060, Loss: 0.6548, Val: 0.6283, Test: 0.6059\n",
      "Epoch: 061, Loss: 0.6550, Val: 0.6263, Test: 0.6115\n",
      "Epoch: 062, Loss: 0.6499, Val: 0.6127, Test: 0.6136\n",
      "Epoch: 063, Loss: 0.6466, Val: 0.6244, Test: 0.6077\n",
      "Epoch: 064, Loss: 0.6458, Val: 0.6169, Test: 0.6068\n",
      "Epoch: 065, Loss: 0.6421, Val: 0.5907, Test: 0.6124\n",
      "Epoch: 066, Loss: 0.6335, Val: 0.5902, Test: 0.6117\n",
      "Epoch: 067, Loss: 0.6356, Val: 0.6111, Test: 0.5952\n",
      "Epoch: 068, Loss: 0.6303, Val: 0.6109, Test: 0.5926\n",
      "Epoch: 069, Loss: 0.6368, Val: 0.5978, Test: 0.6018\n",
      "Epoch: 070, Loss: 0.6222, Val: 0.5758, Test: 0.6114\n",
      "Epoch: 071, Loss: 0.6185, Val: 0.5790, Test: 0.6087\n",
      "Epoch: 072, Loss: 0.6201, Val: 0.6065, Test: 0.5930\n",
      "Epoch: 073, Loss: 0.6180, Val: 0.6056, Test: 0.5981\n",
      "Epoch: 074, Loss: 0.6147, Val: 0.5930, Test: 0.6104\n",
      "Epoch: 075, Loss: 0.6199, Val: 0.5902, Test: 0.6137\n",
      "Epoch: 076, Loss: 0.6122, Val: 0.5978, Test: 0.6105\n",
      "Epoch: 077, Loss: 0.5953, Val: 0.6035, Test: 0.6059\n",
      "Epoch: 078, Loss: 0.6191, Val: 0.6118, Test: 0.6053\n",
      "Epoch: 079, Loss: 0.6159, Val: 0.6102, Test: 0.6090\n",
      "Epoch: 080, Loss: 0.6068, Val: 0.5783, Test: 0.6258\n",
      "Epoch: 081, Loss: 0.6122, Val: 0.5909, Test: 0.6214\n",
      "Epoch: 082, Loss: 0.5958, Val: 0.6097, Test: 0.6106\n",
      "Epoch: 083, Loss: 0.6147, Val: 0.6104, Test: 0.6135\n",
      "Epoch: 084, Loss: 0.6101, Val: 0.5898, Test: 0.6264\n",
      "Epoch: 085, Loss: 0.6160, Val: 0.5854, Test: 0.6284\n",
      "Epoch: 086, Loss: 0.6054, Val: 0.6010, Test: 0.6240\n",
      "Epoch: 087, Loss: 0.6046, Val: 0.6169, Test: 0.6161\n",
      "Epoch: 088, Loss: 0.6163, Val: 0.6129, Test: 0.6173\n",
      "Epoch: 089, Loss: 0.6062, Val: 0.6051, Test: 0.6219\n",
      "Epoch: 090, Loss: 0.6114, Val: 0.6008, Test: 0.6251\n",
      "Epoch: 091, Loss: 0.6123, Val: 0.5960, Test: 0.6271\n",
      "Epoch: 092, Loss: 0.6136, Val: 0.5944, Test: 0.6271\n",
      "Epoch: 093, Loss: 0.6118, Val: 0.5971, Test: 0.6274\n",
      "Epoch: 094, Loss: 0.6090, Val: 0.5985, Test: 0.6261\n",
      "Epoch: 095, Loss: 0.6067, Val: 0.5923, Test: 0.6285\n",
      "Epoch: 096, Loss: 0.6125, Val: 0.5941, Test: 0.6270\n",
      "Epoch: 097, Loss: 0.6071, Val: 0.5999, Test: 0.6246\n",
      "Epoch: 098, Loss: 0.6088, Val: 0.6026, Test: 0.6224\n",
      "Epoch: 099, Loss: 0.6047, Val: 0.6061, Test: 0.6178\n",
      "Epoch: 100, Loss: 0.6160, Val: 0.6077, Test: 0.6187\n",
      "Final Test: 0.6344\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(12, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_ase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6060)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "inverted_mask_matrix = torch.ones([num_nodes,num_nodes]).squeeze(0) - mask.to('cpu')\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "\n",
    "(adj_matrix*inverted_mask_matrix == predicted_adj*inverted_mask_matrix).sum() / adj_matrix.shape[0]**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLASE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.9136, Val: 0.2950, Test: 0.1806\n",
      "Epoch: 002, Loss: 0.7345, Val: 0.5354, Test: 0.4638\n",
      "Epoch: 003, Loss: 0.7256, Val: 0.5452, Test: 0.4712\n",
      "Epoch: 004, Loss: 0.7168, Val: 0.5016, Test: 0.4162\n",
      "Epoch: 005, Loss: 0.7059, Val: 0.4904, Test: 0.3919\n",
      "Epoch: 006, Loss: 0.7010, Val: 0.5142, Test: 0.4106\n",
      "Epoch: 007, Loss: 0.6974, Val: 0.5627, Test: 0.4644\n",
      "Epoch: 008, Loss: 0.6954, Val: 0.6405, Test: 0.5425\n",
      "Epoch: 009, Loss: 0.6935, Val: 0.6983, Test: 0.6022\n",
      "Epoch: 010, Loss: 0.6918, Val: 0.7176, Test: 0.6288\n",
      "Epoch: 011, Loss: 0.6915, Val: 0.7220, Test: 0.6380\n",
      "Epoch: 012, Loss: 0.6903, Val: 0.7208, Test: 0.6382\n",
      "Epoch: 013, Loss: 0.6905, Val: 0.7204, Test: 0.6376\n",
      "Epoch: 014, Loss: 0.6879, Val: 0.7204, Test: 0.6354\n",
      "Epoch: 015, Loss: 0.6906, Val: 0.7213, Test: 0.6331\n",
      "Epoch: 016, Loss: 0.6874, Val: 0.7202, Test: 0.6303\n",
      "Epoch: 017, Loss: 0.6891, Val: 0.7225, Test: 0.6291\n",
      "Epoch: 018, Loss: 0.6886, Val: 0.7218, Test: 0.6278\n",
      "Epoch: 019, Loss: 0.6875, Val: 0.7234, Test: 0.6287\n",
      "Epoch: 020, Loss: 0.6882, Val: 0.7254, Test: 0.6298\n",
      "Epoch: 021, Loss: 0.6883, Val: 0.7277, Test: 0.6301\n",
      "Epoch: 022, Loss: 0.6872, Val: 0.7305, Test: 0.6307\n",
      "Epoch: 023, Loss: 0.6859, Val: 0.7321, Test: 0.6299\n",
      "Epoch: 024, Loss: 0.6864, Val: 0.7348, Test: 0.6292\n",
      "Epoch: 025, Loss: 0.6869, Val: 0.7353, Test: 0.6291\n",
      "Epoch: 026, Loss: 0.6853, Val: 0.7385, Test: 0.6280\n",
      "Epoch: 027, Loss: 0.6848, Val: 0.7392, Test: 0.6271\n",
      "Epoch: 028, Loss: 0.6836, Val: 0.7408, Test: 0.6266\n",
      "Epoch: 029, Loss: 0.6849, Val: 0.7436, Test: 0.6263\n",
      "Epoch: 030, Loss: 0.6824, Val: 0.7459, Test: 0.6251\n",
      "Epoch: 031, Loss: 0.6826, Val: 0.7456, Test: 0.6255\n",
      "Epoch: 032, Loss: 0.6806, Val: 0.7454, Test: 0.6252\n",
      "Epoch: 033, Loss: 0.6819, Val: 0.7449, Test: 0.6259\n",
      "Epoch: 034, Loss: 0.6785, Val: 0.7408, Test: 0.6243\n",
      "Epoch: 035, Loss: 0.6803, Val: 0.7378, Test: 0.6239\n",
      "Epoch: 036, Loss: 0.6786, Val: 0.7275, Test: 0.6216\n",
      "Epoch: 037, Loss: 0.6786, Val: 0.7045, Test: 0.6220\n",
      "Epoch: 038, Loss: 0.6787, Val: 0.6811, Test: 0.6257\n",
      "Epoch: 039, Loss: 0.6751, Val: 0.6651, Test: 0.6279\n",
      "Epoch: 040, Loss: 0.6698, Val: 0.6623, Test: 0.6294\n",
      "Epoch: 041, Loss: 0.6693, Val: 0.6683, Test: 0.6154\n",
      "Epoch: 042, Loss: 0.6701, Val: 0.6524, Test: 0.6124\n",
      "Epoch: 043, Loss: 0.6696, Val: 0.6162, Test: 0.6210\n",
      "Epoch: 044, Loss: 0.6617, Val: 0.5960, Test: 0.6260\n",
      "Epoch: 045, Loss: 0.6610, Val: 0.5865, Test: 0.6239\n",
      "Epoch: 046, Loss: 0.6613, Val: 0.5859, Test: 0.6116\n",
      "Epoch: 047, Loss: 0.6628, Val: 0.5969, Test: 0.5987\n",
      "Epoch: 048, Loss: 0.6594, Val: 0.5980, Test: 0.5898\n",
      "Epoch: 049, Loss: 0.6676, Val: 0.5872, Test: 0.5950\n",
      "Epoch: 050, Loss: 0.6625, Val: 0.5794, Test: 0.6013\n",
      "Epoch: 051, Loss: 0.6594, Val: 0.5863, Test: 0.5953\n",
      "Epoch: 052, Loss: 0.6598, Val: 0.5932, Test: 0.5961\n",
      "Epoch: 053, Loss: 0.6696, Val: 0.5838, Test: 0.6038\n",
      "Epoch: 054, Loss: 0.6631, Val: 0.5769, Test: 0.6036\n",
      "Epoch: 055, Loss: 0.6531, Val: 0.5904, Test: 0.6010\n",
      "Epoch: 056, Loss: 0.6548, Val: 0.6155, Test: 0.5908\n",
      "Epoch: 057, Loss: 0.6621, Val: 0.6175, Test: 0.5913\n",
      "Epoch: 058, Loss: 0.6535, Val: 0.6198, Test: 0.5969\n",
      "Epoch: 059, Loss: 0.6542, Val: 0.5992, Test: 0.6045\n",
      "Epoch: 060, Loss: 0.6512, Val: 0.5960, Test: 0.6025\n",
      "Epoch: 061, Loss: 0.6580, Val: 0.6006, Test: 0.6011\n",
      "Epoch: 062, Loss: 0.6502, Val: 0.6125, Test: 0.5957\n",
      "Epoch: 063, Loss: 0.6535, Val: 0.6292, Test: 0.5895\n",
      "Epoch: 064, Loss: 0.6564, Val: 0.6274, Test: 0.5892\n",
      "Epoch: 065, Loss: 0.6587, Val: 0.6088, Test: 0.5960\n",
      "Epoch: 066, Loss: 0.6550, Val: 0.5976, Test: 0.6026\n",
      "Epoch: 067, Loss: 0.6531, Val: 0.6022, Test: 0.6004\n",
      "Epoch: 068, Loss: 0.6469, Val: 0.6045, Test: 0.5901\n",
      "Epoch: 069, Loss: 0.6567, Val: 0.6077, Test: 0.5871\n",
      "Epoch: 070, Loss: 0.6526, Val: 0.6180, Test: 0.5968\n",
      "Epoch: 071, Loss: 0.6487, Val: 0.6116, Test: 0.6042\n",
      "Epoch: 072, Loss: 0.6542, Val: 0.5978, Test: 0.5983\n",
      "Epoch: 073, Loss: 0.6442, Val: 0.6012, Test: 0.5880\n",
      "Epoch: 074, Loss: 0.6321, Val: 0.6182, Test: 0.5868\n",
      "Epoch: 075, Loss: 0.6511, Val: 0.6290, Test: 0.5899\n",
      "Epoch: 076, Loss: 0.6555, Val: 0.6198, Test: 0.5942\n",
      "Epoch: 077, Loss: 0.6536, Val: 0.6049, Test: 0.5942\n",
      "Epoch: 078, Loss: 0.6426, Val: 0.5941, Test: 0.5960\n",
      "Epoch: 079, Loss: 0.6519, Val: 0.5932, Test: 0.5914\n",
      "Epoch: 080, Loss: 0.6518, Val: 0.6125, Test: 0.5880\n",
      "Epoch: 081, Loss: 0.6531, Val: 0.6191, Test: 0.5907\n",
      "Epoch: 082, Loss: 0.6409, Val: 0.6221, Test: 0.5949\n",
      "Epoch: 083, Loss: 0.6565, Val: 0.6102, Test: 0.5931\n",
      "Epoch: 084, Loss: 0.6507, Val: 0.5999, Test: 0.5915\n",
      "Epoch: 085, Loss: 0.6404, Val: 0.5996, Test: 0.5926\n",
      "Epoch: 086, Loss: 0.6484, Val: 0.6166, Test: 0.5880\n",
      "Epoch: 087, Loss: 0.6382, Val: 0.6361, Test: 0.5863\n",
      "Epoch: 088, Loss: 0.6376, Val: 0.6407, Test: 0.5817\n",
      "Epoch: 089, Loss: 0.6547, Val: 0.6152, Test: 0.5688\n",
      "Epoch: 090, Loss: 0.6483, Val: 0.5964, Test: 0.5830\n",
      "Epoch: 091, Loss: 0.6543, Val: 0.5957, Test: 0.5988\n",
      "Epoch: 092, Loss: 0.6379, Val: 0.6079, Test: 0.5933\n",
      "Epoch: 093, Loss: 0.6435, Val: 0.6182, Test: 0.5862\n",
      "Epoch: 094, Loss: 0.6347, Val: 0.6244, Test: 0.5773\n",
      "Epoch: 095, Loss: 0.6384, Val: 0.6175, Test: 0.5677\n",
      "Epoch: 096, Loss: 0.6397, Val: 0.6045, Test: 0.5813\n",
      "Epoch: 097, Loss: 0.6421, Val: 0.6061, Test: 0.5934\n",
      "Epoch: 098, Loss: 0.6442, Val: 0.6162, Test: 0.5897\n",
      "Epoch: 099, Loss: 0.6391, Val: 0.6283, Test: 0.5807\n",
      "Epoch: 100, Loss: 0.6342, Val: 0.6214, Test: 0.5894\n",
      "Final Test: 0.6251\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(12, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_glase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_glase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5943)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "# (adj_matrix[not_present_countries][:,res_indexes]==predicted_adj[not_present_countries][:,res_indexes]).sum() / len(not_present_countries) / len(res_indexes)\n",
    "\n",
    "(adj_matrix*inverted_mask_matrix == predicted_adj*inverted_mask_matrix).sum() / adj_matrix.shape[0]**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
