{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction on Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6930, Val: 0.6833, Test: 0.7031\n",
      "Epoch: 002, Loss: 0.6833, Val: 0.6812, Test: 0.6972\n",
      "Epoch: 003, Loss: 0.7039, Val: 0.6885, Test: 0.7066\n",
      "Epoch: 004, Loss: 0.6784, Val: 0.6980, Test: 0.7239\n",
      "Epoch: 005, Loss: 0.6852, Val: 0.7068, Test: 0.7394\n",
      "Epoch: 006, Loss: 0.6878, Val: 0.7131, Test: 0.7451\n",
      "Epoch: 007, Loss: 0.6883, Val: 0.7073, Test: 0.7348\n",
      "Epoch: 008, Loss: 0.6870, Val: 0.6965, Test: 0.7197\n",
      "Epoch: 009, Loss: 0.6830, Val: 0.6891, Test: 0.7076\n",
      "Epoch: 010, Loss: 0.6761, Val: 0.6857, Test: 0.6987\n",
      "Epoch: 011, Loss: 0.6698, Val: 0.6835, Test: 0.6920\n",
      "Epoch: 012, Loss: 0.6734, Val: 0.6832, Test: 0.6921\n",
      "Epoch: 013, Loss: 0.6685, Val: 0.6930, Test: 0.7060\n",
      "Epoch: 014, Loss: 0.6586, Val: 0.7174, Test: 0.7335\n",
      "Epoch: 015, Loss: 0.6553, Val: 0.7420, Test: 0.7504\n",
      "Epoch: 016, Loss: 0.6487, Val: 0.7464, Test: 0.7467\n",
      "Epoch: 017, Loss: 0.6378, Val: 0.7380, Test: 0.7356\n",
      "Epoch: 018, Loss: 0.6248, Val: 0.7408, Test: 0.7335\n",
      "Epoch: 019, Loss: 0.6165, Val: 0.7703, Test: 0.7496\n",
      "Epoch: 020, Loss: 0.5982, Val: 0.7870, Test: 0.7649\n",
      "Epoch: 021, Loss: 0.5883, Val: 0.7852, Test: 0.7658\n",
      "Epoch: 022, Loss: 0.5790, Val: 0.7852, Test: 0.7584\n",
      "Epoch: 023, Loss: 0.5668, Val: 0.7860, Test: 0.7576\n",
      "Epoch: 024, Loss: 0.5702, Val: 0.7848, Test: 0.7650\n",
      "Epoch: 025, Loss: 0.5696, Val: 0.7893, Test: 0.7669\n",
      "Epoch: 026, Loss: 0.5580, Val: 0.7957, Test: 0.7701\n",
      "Epoch: 027, Loss: 0.5552, Val: 0.8007, Test: 0.7791\n",
      "Epoch: 028, Loss: 0.5541, Val: 0.8095, Test: 0.7890\n",
      "Epoch: 029, Loss: 0.5460, Val: 0.8213, Test: 0.7981\n",
      "Epoch: 030, Loss: 0.5369, Val: 0.8302, Test: 0.8084\n",
      "Epoch: 031, Loss: 0.5329, Val: 0.8441, Test: 0.8269\n",
      "Epoch: 032, Loss: 0.5280, Val: 0.8562, Test: 0.8422\n",
      "Epoch: 033, Loss: 0.5219, Val: 0.8614, Test: 0.8500\n",
      "Epoch: 034, Loss: 0.5167, Val: 0.8639, Test: 0.8533\n",
      "Epoch: 035, Loss: 0.4996, Val: 0.8636, Test: 0.8537\n",
      "Epoch: 036, Loss: 0.5076, Val: 0.8647, Test: 0.8566\n",
      "Epoch: 037, Loss: 0.5070, Val: 0.8638, Test: 0.8581\n",
      "Epoch: 038, Loss: 0.5009, Val: 0.8699, Test: 0.8638\n",
      "Epoch: 039, Loss: 0.4978, Val: 0.8790, Test: 0.8738\n",
      "Epoch: 040, Loss: 0.4914, Val: 0.8876, Test: 0.8832\n",
      "Epoch: 041, Loss: 0.4830, Val: 0.8948, Test: 0.8911\n",
      "Epoch: 042, Loss: 0.4730, Val: 0.9013, Test: 0.8961\n",
      "Epoch: 043, Loss: 0.4687, Val: 0.9071, Test: 0.9004\n",
      "Epoch: 044, Loss: 0.4658, Val: 0.9127, Test: 0.9046\n",
      "Epoch: 045, Loss: 0.4723, Val: 0.9162, Test: 0.9071\n",
      "Epoch: 046, Loss: 0.4605, Val: 0.9181, Test: 0.9081\n",
      "Epoch: 047, Loss: 0.4582, Val: 0.9194, Test: 0.9074\n",
      "Epoch: 048, Loss: 0.4659, Val: 0.9200, Test: 0.9071\n",
      "Epoch: 049, Loss: 0.4623, Val: 0.9209, Test: 0.9082\n",
      "Epoch: 050, Loss: 0.4701, Val: 0.9209, Test: 0.9100\n",
      "Epoch: 051, Loss: 0.4682, Val: 0.9199, Test: 0.9111\n",
      "Epoch: 052, Loss: 0.4626, Val: 0.9191, Test: 0.9103\n",
      "Epoch: 053, Loss: 0.4618, Val: 0.9173, Test: 0.9089\n",
      "Epoch: 054, Loss: 0.4599, Val: 0.9149, Test: 0.9056\n",
      "Epoch: 055, Loss: 0.4624, Val: 0.9136, Test: 0.9053\n",
      "Epoch: 056, Loss: 0.4538, Val: 0.9124, Test: 0.9055\n",
      "Epoch: 057, Loss: 0.4594, Val: 0.9112, Test: 0.9055\n",
      "Epoch: 058, Loss: 0.4585, Val: 0.9111, Test: 0.9048\n",
      "Epoch: 059, Loss: 0.4592, Val: 0.9124, Test: 0.9056\n",
      "Epoch: 060, Loss: 0.4523, Val: 0.9127, Test: 0.9068\n",
      "Epoch: 061, Loss: 0.4565, Val: 0.9141, Test: 0.9074\n",
      "Epoch: 062, Loss: 0.4553, Val: 0.9149, Test: 0.9076\n",
      "Epoch: 063, Loss: 0.4491, Val: 0.9162, Test: 0.9072\n",
      "Epoch: 064, Loss: 0.4547, Val: 0.9175, Test: 0.9086\n",
      "Epoch: 065, Loss: 0.4551, Val: 0.9182, Test: 0.9094\n",
      "Epoch: 066, Loss: 0.4559, Val: 0.9189, Test: 0.9092\n",
      "Epoch: 067, Loss: 0.4478, Val: 0.9187, Test: 0.9086\n",
      "Epoch: 068, Loss: 0.4548, Val: 0.9188, Test: 0.9085\n",
      "Epoch: 069, Loss: 0.4533, Val: 0.9197, Test: 0.9096\n",
      "Epoch: 070, Loss: 0.4540, Val: 0.9202, Test: 0.9113\n",
      "Epoch: 071, Loss: 0.4484, Val: 0.9202, Test: 0.9125\n",
      "Epoch: 072, Loss: 0.4495, Val: 0.9202, Test: 0.9126\n",
      "Epoch: 073, Loss: 0.4477, Val: 0.9195, Test: 0.9110\n",
      "Epoch: 074, Loss: 0.4520, Val: 0.9193, Test: 0.9102\n",
      "Epoch: 075, Loss: 0.4450, Val: 0.9200, Test: 0.9119\n",
      "Epoch: 076, Loss: 0.4395, Val: 0.9202, Test: 0.9134\n",
      "Epoch: 077, Loss: 0.4453, Val: 0.9199, Test: 0.9136\n",
      "Epoch: 078, Loss: 0.4530, Val: 0.9201, Test: 0.9132\n",
      "Epoch: 079, Loss: 0.4474, Val: 0.9200, Test: 0.9124\n",
      "Epoch: 080, Loss: 0.4445, Val: 0.9211, Test: 0.9127\n",
      "Epoch: 081, Loss: 0.4510, Val: 0.9216, Test: 0.9135\n",
      "Epoch: 082, Loss: 0.4487, Val: 0.9225, Test: 0.9142\n",
      "Epoch: 083, Loss: 0.4425, Val: 0.9232, Test: 0.9142\n",
      "Epoch: 084, Loss: 0.4448, Val: 0.9246, Test: 0.9144\n",
      "Epoch: 085, Loss: 0.4395, Val: 0.9252, Test: 0.9150\n",
      "Epoch: 086, Loss: 0.4429, Val: 0.9253, Test: 0.9160\n",
      "Epoch: 087, Loss: 0.4464, Val: 0.9258, Test: 0.9169\n",
      "Epoch: 088, Loss: 0.4387, Val: 0.9265, Test: 0.9169\n",
      "Epoch: 089, Loss: 0.4412, Val: 0.9265, Test: 0.9170\n",
      "Epoch: 090, Loss: 0.4485, Val: 0.9258, Test: 0.9177\n",
      "Epoch: 091, Loss: 0.4401, Val: 0.9248, Test: 0.9171\n",
      "Epoch: 092, Loss: 0.4360, Val: 0.9253, Test: 0.9170\n",
      "Epoch: 093, Loss: 0.4326, Val: 0.9261, Test: 0.9175\n",
      "Epoch: 094, Loss: 0.4410, Val: 0.9261, Test: 0.9174\n",
      "Epoch: 095, Loss: 0.4320, Val: 0.9262, Test: 0.9171\n",
      "Epoch: 096, Loss: 0.4436, Val: 0.9255, Test: 0.9162\n",
      "Epoch: 097, Loss: 0.4369, Val: 0.9254, Test: 0.9159\n",
      "Epoch: 098, Loss: 0.4343, Val: 0.9261, Test: 0.9159\n",
      "Epoch: 099, Loss: 0.4331, Val: 0.9262, Test: 0.9154\n",
      "Epoch: 100, Loss: 0.4374, Val: 0.9256, Test: 0.9148\n",
      "Final Test: 0.9169\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "dataset = Planetoid(\"./\", name='Cora', transform=transform)\n",
    "# After applying the `RandomLinkSplit` transform, the data is transformed from\n",
    "# a data object to a list of tuples (train_data, val_data, test_data), with\n",
    "# each element representing the corresponding split.\n",
    "train_data, val_data, test_data = dataset[0]\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(dataset.num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction on Senators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0., -1.,  0.],\n",
      "        [ 0.,  0.,  0., -1.]])\n",
      "Iteraciones:  100\n",
      "Loss:  tensor(43.7876)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.utils import stochastic_blockmodel_graph, to_dense_adj\n",
    "from graspologic.embed import AdjacencySpectralEmbed \n",
    "from models.RDPG_GD import GRDPG_GD_Armijo\n",
    "from models.GLASE_unshared_normalized import gLASE \n",
    "# from models.GLASE_unshared_normalized_v2 import gLASE_v2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "d = 4\n",
    "n_P1 = 100\n",
    "# n_P2 = 100\n",
    "n_P2 = 50\n",
    "n_L1 = 100\n",
    "# n_L2 = 100\n",
    "n_L2 = 200\n",
    "n_L3 = 30\n",
    "\n",
    "P1_L1 = 0.9\n",
    "P1_L2 = 0.1\n",
    "P1_L3 = 0.3  \n",
    "P2_L1 = 0.1\n",
    "P2_L2 = 0.9\n",
    "P2_L3 = 0.3\n",
    "\n",
    "\n",
    "p = [\n",
    "    [0, 0, P1_L1, P1_L2, P1_L3],\n",
    "    [0, 0, P2_L1, P2_L2, P2_L3],\n",
    "    [P1_L1, P2_L1, 0, 0, 0], \n",
    "    [P1_L2, P2_L2, 0, 0, 0], \n",
    "    [P1_L3, P2_L3, 0, 0, 0]\n",
    "    ]\n",
    "\n",
    "n = [n_P1, n_P2, n_L1, n_L2, n_L3]\n",
    "\n",
    "num_nodes = np.sum(n)\n",
    "edge_index = stochastic_blockmodel_graph(n, p)\n",
    "\n",
    "\n",
    "## MASK\n",
    "n_P1_np = 80\n",
    "n_P2_np = 80\n",
    "# n_P2_np = 40\n",
    "senadores_no_presentes = list(range(n_P1_np)) + list(range(n_P1,n_P1+n_P2_np))\n",
    "\n",
    "mask = torch.ones([num_nodes,num_nodes]).squeeze(0)\n",
    "for i in senadores_no_presentes:\n",
    "    votos = (torch.rand(1, num_nodes) < 0.1).int()\n",
    "    mask[i,:] = votos\n",
    "    mask[:,i] = votos\n",
    "\n",
    "\n",
    "## ASE \n",
    "adj_matrix = to_dense_adj(edge_index.to('cpu')).squeeze(0)\n",
    "ase = AdjacencySpectralEmbed(n_components=d, diag_aug=True, algorithm='full')\n",
    "masked_adj = adj_matrix*mask\n",
    "x_ase = ase.fit_transform(masked_adj.numpy())\n",
    "x_ase = torch.from_numpy(x_ase)\n",
    "\n",
    "A = to_dense_adj(edge_index.to('cpu'), max_num_nodes=num_nodes).squeeze(0)\n",
    "\n",
    "u, V = torch.linalg.eig(A)\n",
    "\n",
    "list_q=[]\n",
    "for i in range(d):\n",
    "    if u[i].numpy()>0:\n",
    "        list_q.append(1)\n",
    "    else:\n",
    "        list_q.append(-1)\n",
    "        \n",
    "list_q.sort(reverse=True)\n",
    "# q = torch.Tensor(list_q)\n",
    "Q=torch.diag(q)\n",
    "\n",
    "print(Q)\n",
    "\n",
    "\n",
    "torch.norm((x_ase@Q@x_ase.T - to_dense_adj(edge_index).squeeze(0))*mask)\n",
    "\n",
    "\n",
    "x_grdpg, cost, k  = GRDPG_GD_Armijo(x_ase, edge_index, Q, mask.nonzero().t().contiguous())\n",
    "x_grdpg = x_grdpg.detach()\n",
    "print(\"Iteraciones: \", k)\n",
    "print(\"Loss: \", torch.norm((x_grdpg@Q@x_grdpg.T - to_dense_adj(edge_index).squeeze(0))*to_dense_adj(mask.nonzero().t().contiguous()).squeeze(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(97.9246, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(57.4205, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(54.2652, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(53.0740, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gd_steps = 20\n",
    "lr = 1e-2\n",
    "device = 'cuda'\n",
    "model = gLASE(d,d, gd_steps)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "epochs = 400\n",
    "\n",
    "## Initialization\n",
    "for step in range(gd_steps):\n",
    "    model.gd[step].lin1.weight.data = (torch.eye(d,d)*lr).to(device)#torch.nn.init.xavier_uniform_(model.gd[step].lin1.weight)*lr\n",
    "    model.gd[step].lin2.weight.data = (torch.eye(d,d)*lr).to(device)#torch.nn.init.xavier_uniform_(model.gd[step].lin2.weight)*lr\n",
    "    \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Define ATT mask\n",
    "edge_index_2 = torch.ones([num_nodes,num_nodes],).nonzero().t().contiguous().to(device)\n",
    "mask = mask.to(device)\n",
    "x_ase = x_ase.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "Q = Q.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x_ase, edge_index, edge_index_2, Q, mask.nonzero().t().contiguous())\n",
    "    loss = torch.norm((out@Q@out.T - to_dense_adj(edge_index).squeeze(0))*mask)\n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "\n",
    "    if epoch % 100 ==0:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(50.3964, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = torch.norm((out@Q@out.T - to_dense_adj(edge_index).squeeze(0))*mask)\n",
    "print(loss)\n",
    "x_glase = out.detach().to('cpu')\n",
    "x_ase = x_ase.to('cpu')\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize =(15,5))\n",
    "axes[0].scatter(x_ase[n_P1_np:n_P1,0],x_ase[n_P1_np:n_P1,2], c='royalblue',marker='o',label='Party 1')\n",
    "axes[0].scatter(x_ase[:n_P1_np,0],x_ase[:n_P1_np,2], c='gold',marker='X',label='Not present (Party 1)')\n",
    "axes[0].scatter(x_ase[n_P1+n_P2_np:n_P1+n_P2,0],x_ase[n_P1+n_P2_np:n_P1+n_P2,2], c='firebrick',marker='o',label='Party 2')\n",
    "axes[0].scatter(x_ase[n_P1:n_P1+n_P2_np,0],x_ase[n_P1:n_P1+n_P2_np,2], c='limegreen',marker='X',label='Not present (Party 2)')\n",
    "axes[0].scatter(x_ase[n_P1+n_P2:n_P1+n_P2+n_L1,0],x_ase[n_P1+n_P2:n_P1+n_P2+n_L1,2], c='cornflowerblue',marker='^',label='Party 1 laws')\n",
    "axes[0].scatter(x_ase[n_P1+n_P2+n_L1:n_P1+n_P2+n_L1+n_L2,0],x_ase[n_P1+n_P2+n_L1:n_P1+n_P2+n_L1+n_L2,2],c='indianred',marker='^',label='Party 2 laws')\n",
    "axes[0].scatter(x_ase[n_P1+n_P2+n_L1+n_L2:,0],x_ase[n_P1+n_P2+n_L1+n_L2:,2],c='darkorange',marker='^',label='Bipartisan laws')\n",
    "axes[0].set_title('ASE')\n",
    "\n",
    "axes[1].scatter(x_grdpg[n_P1_np:n_P1,0],x_grdpg[n_P1_np:n_P1,2], c='royalblue',marker='o',label='Party 1')\n",
    "axes[1].scatter(x_grdpg[:n_P1_np,0],x_grdpg[:n_P1_np,2], c='gold',marker='X',label='Not present (Party 1)')\n",
    "axes[1].scatter(x_grdpg[n_P1+n_P2_np:n_P1+n_P2,0],x_grdpg[n_P1+n_P2_np:n_P1+n_P2,2], c='firebrick',marker='o',label='Party 2')\n",
    "axes[1].scatter(x_grdpg[n_P1:n_P1+n_P2_np,0],x_grdpg[n_P1:n_P1+n_P2_np,2], c='limegreen',marker='X',label='Not present (Party 2)')\n",
    "axes[1].scatter(x_grdpg[n_P1+n_P2:n_P1+n_P2+n_L1,0],x_grdpg[n_P1+n_P2:n_P1+n_P2+n_L1,2], c='cornflowerblue',marker='^',label='Party 1 laws')\n",
    "axes[1].scatter(x_grdpg[n_P1+n_P2+n_L1:n_P1+n_P2+n_L1+n_L2,0],x_grdpg[n_P1+n_P2+n_L1:n_P1+n_P2+n_L1+n_L2,2],c='indianred',marker='^',label='Party 2 laws')\n",
    "axes[1].scatter(x_grdpg[n_P1+n_P2+n_L1+n_L2:,0],x_grdpg[n_P1+n_P2+n_L1+n_L2:,2],c='darkorange',marker='^',label='Bipartisan laws')\n",
    "axes[1].set_title('GD')\n",
    "\n",
    "axes[2].scatter(x_glase[n_P1_np:n_P1,0],x_glase[n_P1_np:n_P1,2], c='royalblue',marker='o',label='Party 1')\n",
    "axes[2].scatter(x_glase[:n_P1_np,0],x_glase[:n_P1_np,2], c='gold',marker='X',label='Not present (Party 1)')\n",
    "axes[2].scatter(x_glase[n_P1+n_P2_np:n_P1+n_P2,0],x_glase[n_P1+n_P2_np:n_P1+n_P2,2], c='firebrick',marker='o',label='Party 2')\n",
    "axes[2].scatter(x_glase[n_P1:n_P1+n_P2_np,0],x_glase[n_P1:n_P1+n_P2_np,2], c='limegreen',marker='X',label='Not present (Party 2)')\n",
    "axes[2].scatter(x_glase[n_P1+n_P2:n_P1+n_P2+n_L1,0],x_glase[n_P1+n_P2:n_P1+n_P2+n_L1,2], c='cornflowerblue',marker='^',label='Party 1 laws')\n",
    "axes[2].scatter(x_glase[n_P1+n_P2+n_L1:n_P1+n_P2+n_L1+n_L2,0],x_glase[n_P1+n_P2+n_L1:n_P1+n_P2+n_L1+n_L2,2],c='indianred',marker='^',label='Party 2 laws')\n",
    "axes[2].scatter(x_glase[n_P1+n_P2+n_L1+n_L2:,0],x_glase[n_P1+n_P2+n_L1+n_L2:,2],c='darkorange',marker='^',label='Bipartisan laws')\n",
    "axes[2].set_title('LASE')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(-1, -0.07),fancybox=True, shadow=True, ncol=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([380, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Node features -- One hot encoding of label P1, P2, L1, L2, L3\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "labels = np.concatenate((np.ones(n_P1)*0,np.ones(n_P2), np.ones(n_L1)*2, np.ones(n_L2)*3, np.ones(n_L3)*4))\n",
    "labels = labels.tolist()\n",
    "labels = torch.tensor(labels).long()\n",
    "labels = F.one_hot(labels)\n",
    "\n",
    "\n",
    "\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Train, Val, Test\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "masked_edge_index = masked_adj.nonzero().t().contiguous()\n",
    "\n",
    "data = Data(x=labels.float(), x_ase=x_ase, x_glase=x_glase, edge_index=masked_edge_index)\n",
    "\n",
    "transform = T.Compose([\n",
    "    # T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6929, Val: 0.9109, Test: 0.8962\n",
      "Epoch: 002, Loss: 0.6478, Val: 0.9019, Test: 0.8843\n",
      "Epoch: 003, Loss: 0.6156, Val: 0.8797, Test: 0.8647\n",
      "Epoch: 004, Loss: 0.5787, Val: 0.8749, Test: 0.8593\n",
      "Epoch: 005, Loss: 0.5600, Val: 0.8679, Test: 0.8514\n",
      "Epoch: 006, Loss: 0.5605, Val: 0.8743, Test: 0.8544\n",
      "Epoch: 007, Loss: 0.5578, Val: 0.8743, Test: 0.8550\n",
      "Epoch: 008, Loss: 0.5525, Val: 0.8749, Test: 0.8539\n",
      "Epoch: 009, Loss: 0.5431, Val: 0.8783, Test: 0.8548\n",
      "Epoch: 010, Loss: 0.5413, Val: 0.8799, Test: 0.8563\n",
      "Epoch: 011, Loss: 0.5432, Val: 0.8802, Test: 0.8560\n",
      "Epoch: 012, Loss: 0.5417, Val: 0.8797, Test: 0.8549\n",
      "Epoch: 013, Loss: 0.5385, Val: 0.8801, Test: 0.8561\n",
      "Epoch: 014, Loss: 0.5393, Val: 0.8794, Test: 0.8570\n",
      "Epoch: 015, Loss: 0.5390, Val: 0.8795, Test: 0.8571\n",
      "Epoch: 016, Loss: 0.5385, Val: 0.8787, Test: 0.8565\n",
      "Epoch: 017, Loss: 0.5346, Val: 0.8772, Test: 0.8560\n",
      "Epoch: 018, Loss: 0.5390, Val: 0.8771, Test: 0.8560\n",
      "Epoch: 019, Loss: 0.5334, Val: 0.8769, Test: 0.8562\n",
      "Epoch: 020, Loss: 0.5331, Val: 0.8770, Test: 0.8558\n",
      "Epoch: 021, Loss: 0.5380, Val: 0.8774, Test: 0.8554\n",
      "Epoch: 022, Loss: 0.5353, Val: 0.8778, Test: 0.8553\n",
      "Epoch: 023, Loss: 0.5301, Val: 0.8779, Test: 0.8555\n",
      "Epoch: 024, Loss: 0.5404, Val: 0.8775, Test: 0.8561\n",
      "Epoch: 025, Loss: 0.5368, Val: 0.8776, Test: 0.8565\n",
      "Epoch: 026, Loss: 0.5398, Val: 0.8781, Test: 0.8561\n",
      "Epoch: 027, Loss: 0.5317, Val: 0.8784, Test: 0.8563\n",
      "Epoch: 028, Loss: 0.5440, Val: 0.8778, Test: 0.8562\n",
      "Epoch: 029, Loss: 0.5438, Val: 0.8773, Test: 0.8578\n",
      "Epoch: 030, Loss: 0.5330, Val: 0.8770, Test: 0.8583\n",
      "Epoch: 031, Loss: 0.5298, Val: 0.8772, Test: 0.8581\n",
      "Epoch: 032, Loss: 0.5288, Val: 0.8772, Test: 0.8573\n",
      "Epoch: 033, Loss: 0.5283, Val: 0.8778, Test: 0.8561\n",
      "Epoch: 034, Loss: 0.5362, Val: 0.8791, Test: 0.8563\n",
      "Epoch: 035, Loss: 0.5446, Val: 0.8793, Test: 0.8563\n",
      "Epoch: 036, Loss: 0.5361, Val: 0.8795, Test: 0.8561\n",
      "Epoch: 037, Loss: 0.5354, Val: 0.8789, Test: 0.8562\n",
      "Epoch: 038, Loss: 0.5362, Val: 0.8789, Test: 0.8560\n",
      "Epoch: 039, Loss: 0.5233, Val: 0.8792, Test: 0.8560\n",
      "Epoch: 040, Loss: 0.5295, Val: 0.8792, Test: 0.8561\n",
      "Epoch: 041, Loss: 0.5296, Val: 0.8790, Test: 0.8563\n",
      "Epoch: 042, Loss: 0.5384, Val: 0.8781, Test: 0.8561\n",
      "Epoch: 043, Loss: 0.5328, Val: 0.8775, Test: 0.8569\n",
      "Epoch: 044, Loss: 0.5279, Val: 0.8778, Test: 0.8576\n",
      "Epoch: 045, Loss: 0.5342, Val: 0.8779, Test: 0.8579\n",
      "Epoch: 046, Loss: 0.5330, Val: 0.8775, Test: 0.8575\n",
      "Epoch: 047, Loss: 0.5288, Val: 0.8782, Test: 0.8564\n",
      "Epoch: 048, Loss: 0.5269, Val: 0.8791, Test: 0.8565\n",
      "Epoch: 049, Loss: 0.5424, Val: 0.8788, Test: 0.8567\n",
      "Epoch: 050, Loss: 0.5304, Val: 0.8789, Test: 0.8575\n",
      "Epoch: 051, Loss: 0.5255, Val: 0.8786, Test: 0.8582\n",
      "Epoch: 052, Loss: 0.5323, Val: 0.8790, Test: 0.8577\n",
      "Epoch: 053, Loss: 0.5257, Val: 0.8795, Test: 0.8570\n",
      "Epoch: 054, Loss: 0.5253, Val: 0.8795, Test: 0.8568\n",
      "Epoch: 055, Loss: 0.5313, Val: 0.8788, Test: 0.8577\n",
      "Epoch: 056, Loss: 0.5347, Val: 0.8786, Test: 0.8580\n",
      "Epoch: 057, Loss: 0.5330, Val: 0.8785, Test: 0.8572\n",
      "Epoch: 058, Loss: 0.5198, Val: 0.8787, Test: 0.8574\n",
      "Epoch: 059, Loss: 0.5309, Val: 0.8786, Test: 0.8579\n",
      "Epoch: 060, Loss: 0.5324, Val: 0.8786, Test: 0.8581\n",
      "Epoch: 061, Loss: 0.5370, Val: 0.8786, Test: 0.8583\n",
      "Epoch: 062, Loss: 0.5334, Val: 0.8787, Test: 0.8580\n",
      "Epoch: 063, Loss: 0.5297, Val: 0.8788, Test: 0.8580\n",
      "Epoch: 064, Loss: 0.5295, Val: 0.8795, Test: 0.8580\n",
      "Epoch: 065, Loss: 0.5334, Val: 0.8796, Test: 0.8591\n",
      "Epoch: 066, Loss: 0.5301, Val: 0.8798, Test: 0.8596\n",
      "Epoch: 067, Loss: 0.5228, Val: 0.8799, Test: 0.8594\n",
      "Epoch: 068, Loss: 0.5310, Val: 0.8797, Test: 0.8588\n",
      "Epoch: 069, Loss: 0.5240, Val: 0.8799, Test: 0.8585\n",
      "Epoch: 070, Loss: 0.5256, Val: 0.8801, Test: 0.8597\n",
      "Epoch: 071, Loss: 0.5287, Val: 0.8802, Test: 0.8599\n",
      "Epoch: 072, Loss: 0.5340, Val: 0.8802, Test: 0.8596\n",
      "Epoch: 073, Loss: 0.5312, Val: 0.8802, Test: 0.8588\n",
      "Epoch: 074, Loss: 0.5283, Val: 0.8804, Test: 0.8590\n",
      "Epoch: 075, Loss: 0.5266, Val: 0.8805, Test: 0.8608\n",
      "Epoch: 076, Loss: 0.5257, Val: 0.8809, Test: 0.8616\n",
      "Epoch: 077, Loss: 0.5262, Val: 0.8805, Test: 0.8614\n",
      "Epoch: 078, Loss: 0.5240, Val: 0.8802, Test: 0.8601\n",
      "Epoch: 079, Loss: 0.5280, Val: 0.8799, Test: 0.8596\n",
      "Epoch: 080, Loss: 0.5261, Val: 0.8802, Test: 0.8598\n",
      "Epoch: 081, Loss: 0.5272, Val: 0.8806, Test: 0.8608\n",
      "Epoch: 082, Loss: 0.5239, Val: 0.8813, Test: 0.8615\n",
      "Epoch: 083, Loss: 0.5284, Val: 0.8819, Test: 0.8616\n",
      "Epoch: 084, Loss: 0.5298, Val: 0.8813, Test: 0.8608\n",
      "Epoch: 085, Loss: 0.5289, Val: 0.8816, Test: 0.8620\n",
      "Epoch: 086, Loss: 0.5285, Val: 0.8808, Test: 0.8625\n",
      "Epoch: 087, Loss: 0.5351, Val: 0.8810, Test: 0.8617\n",
      "Epoch: 088, Loss: 0.5304, Val: 0.8809, Test: 0.8614\n",
      "Epoch: 089, Loss: 0.5308, Val: 0.8812, Test: 0.8624\n",
      "Epoch: 090, Loss: 0.5197, Val: 0.8814, Test: 0.8636\n",
      "Epoch: 091, Loss: 0.5223, Val: 0.8808, Test: 0.8636\n",
      "Epoch: 092, Loss: 0.5203, Val: 0.8813, Test: 0.8628\n",
      "Epoch: 093, Loss: 0.5155, Val: 0.8808, Test: 0.8612\n",
      "Epoch: 094, Loss: 0.5307, Val: 0.8810, Test: 0.8610\n",
      "Epoch: 095, Loss: 0.5235, Val: 0.8822, Test: 0.8622\n",
      "Epoch: 096, Loss: 0.5166, Val: 0.8829, Test: 0.8636\n",
      "Epoch: 097, Loss: 0.5228, Val: 0.8841, Test: 0.8630\n",
      "Epoch: 098, Loss: 0.5282, Val: 0.8832, Test: 0.8619\n",
      "Epoch: 099, Loss: 0.5355, Val: 0.8827, Test: 0.8621\n",
      "Epoch: 100, Loss: 0.5212, Val: 0.8806, Test: 0.8618\n",
      "Final Test: 0.8962\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(5, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict on entire masked graph\n",
    "\n",
    "z = model.encode(data.x, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 230])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix[senadores_no_presentes][:,n_P1+n_P2:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8604)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / 48 / 230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6901, Val: 0.8946, Test: 0.8835\n",
      "Epoch: 002, Loss: 0.6330, Val: 0.8840, Test: 0.8689\n",
      "Epoch: 003, Loss: 0.5941, Val: 0.8782, Test: 0.8613\n",
      "Epoch: 004, Loss: 0.5537, Val: 0.8779, Test: 0.8574\n",
      "Epoch: 005, Loss: 0.5509, Val: 0.8788, Test: 0.8562\n",
      "Epoch: 006, Loss: 0.5619, Val: 0.8800, Test: 0.8563\n",
      "Epoch: 007, Loss: 0.5363, Val: 0.8796, Test: 0.8552\n",
      "Epoch: 008, Loss: 0.5334, Val: 0.8782, Test: 0.8540\n",
      "Epoch: 009, Loss: 0.5376, Val: 0.8797, Test: 0.8558\n",
      "Epoch: 010, Loss: 0.5494, Val: 0.8797, Test: 0.8570\n",
      "Epoch: 011, Loss: 0.5422, Val: 0.8789, Test: 0.8572\n",
      "Epoch: 012, Loss: 0.5431, Val: 0.8784, Test: 0.8570\n",
      "Epoch: 013, Loss: 0.5417, Val: 0.8780, Test: 0.8573\n",
      "Epoch: 014, Loss: 0.5347, Val: 0.8786, Test: 0.8566\n",
      "Epoch: 015, Loss: 0.5312, Val: 0.8794, Test: 0.8560\n",
      "Epoch: 016, Loss: 0.5382, Val: 0.8796, Test: 0.8557\n",
      "Epoch: 017, Loss: 0.5344, Val: 0.8797, Test: 0.8557\n",
      "Epoch: 018, Loss: 0.5309, Val: 0.8791, Test: 0.8561\n",
      "Epoch: 019, Loss: 0.5350, Val: 0.8791, Test: 0.8569\n",
      "Epoch: 020, Loss: 0.5313, Val: 0.8788, Test: 0.8584\n",
      "Epoch: 021, Loss: 0.5382, Val: 0.8785, Test: 0.8589\n",
      "Epoch: 022, Loss: 0.5361, Val: 0.8787, Test: 0.8581\n",
      "Epoch: 023, Loss: 0.5350, Val: 0.8790, Test: 0.8587\n",
      "Epoch: 024, Loss: 0.5335, Val: 0.8791, Test: 0.8582\n",
      "Epoch: 025, Loss: 0.5298, Val: 0.8792, Test: 0.8574\n",
      "Epoch: 026, Loss: 0.5220, Val: 0.8800, Test: 0.8566\n",
      "Epoch: 027, Loss: 0.5341, Val: 0.8804, Test: 0.8565\n",
      "Epoch: 028, Loss: 0.5307, Val: 0.8806, Test: 0.8567\n",
      "Epoch: 029, Loss: 0.5305, Val: 0.8801, Test: 0.8566\n",
      "Epoch: 030, Loss: 0.5306, Val: 0.8786, Test: 0.8582\n",
      "Epoch: 031, Loss: 0.5318, Val: 0.8788, Test: 0.8584\n",
      "Epoch: 032, Loss: 0.5314, Val: 0.8792, Test: 0.8571\n",
      "Epoch: 033, Loss: 0.5351, Val: 0.8793, Test: 0.8569\n",
      "Epoch: 034, Loss: 0.5331, Val: 0.8791, Test: 0.8572\n",
      "Epoch: 035, Loss: 0.5346, Val: 0.8790, Test: 0.8586\n",
      "Epoch: 036, Loss: 0.5275, Val: 0.8788, Test: 0.8592\n",
      "Epoch: 037, Loss: 0.5405, Val: 0.8790, Test: 0.8594\n",
      "Epoch: 038, Loss: 0.5290, Val: 0.8791, Test: 0.8596\n",
      "Epoch: 039, Loss: 0.5273, Val: 0.8791, Test: 0.8594\n",
      "Epoch: 040, Loss: 0.5312, Val: 0.8792, Test: 0.8586\n",
      "Epoch: 041, Loss: 0.5316, Val: 0.8796, Test: 0.8582\n",
      "Epoch: 042, Loss: 0.5239, Val: 0.8797, Test: 0.8591\n",
      "Epoch: 043, Loss: 0.5304, Val: 0.8796, Test: 0.8592\n",
      "Epoch: 044, Loss: 0.5356, Val: 0.8800, Test: 0.8579\n",
      "Epoch: 045, Loss: 0.5250, Val: 0.8801, Test: 0.8573\n",
      "Epoch: 046, Loss: 0.5331, Val: 0.8799, Test: 0.8573\n",
      "Epoch: 047, Loss: 0.5266, Val: 0.8797, Test: 0.8579\n",
      "Epoch: 048, Loss: 0.5247, Val: 0.8796, Test: 0.8591\n",
      "Epoch: 049, Loss: 0.5242, Val: 0.8797, Test: 0.8592\n",
      "Epoch: 050, Loss: 0.5220, Val: 0.8796, Test: 0.8584\n",
      "Epoch: 051, Loss: 0.5364, Val: 0.8795, Test: 0.8586\n",
      "Epoch: 052, Loss: 0.5300, Val: 0.8797, Test: 0.8597\n",
      "Epoch: 053, Loss: 0.5324, Val: 0.8791, Test: 0.8602\n",
      "Epoch: 054, Loss: 0.5366, Val: 0.8793, Test: 0.8604\n",
      "Epoch: 055, Loss: 0.5247, Val: 0.8798, Test: 0.8609\n",
      "Epoch: 056, Loss: 0.5284, Val: 0.8804, Test: 0.8612\n",
      "Epoch: 057, Loss: 0.5327, Val: 0.8807, Test: 0.8616\n",
      "Epoch: 058, Loss: 0.5316, Val: 0.8808, Test: 0.8614\n",
      "Epoch: 059, Loss: 0.5232, Val: 0.8807, Test: 0.8609\n",
      "Epoch: 060, Loss: 0.5294, Val: 0.8806, Test: 0.8606\n",
      "Epoch: 061, Loss: 0.5282, Val: 0.8806, Test: 0.8607\n",
      "Epoch: 062, Loss: 0.5271, Val: 0.8810, Test: 0.8612\n",
      "Epoch: 063, Loss: 0.5349, Val: 0.8814, Test: 0.8618\n",
      "Epoch: 064, Loss: 0.5271, Val: 0.8819, Test: 0.8625\n",
      "Epoch: 065, Loss: 0.5228, Val: 0.8816, Test: 0.8629\n",
      "Epoch: 066, Loss: 0.5265, Val: 0.8817, Test: 0.8624\n",
      "Epoch: 067, Loss: 0.5199, Val: 0.8810, Test: 0.8617\n",
      "Epoch: 068, Loss: 0.5170, Val: 0.8813, Test: 0.8616\n",
      "Epoch: 069, Loss: 0.5268, Val: 0.8797, Test: 0.8613\n",
      "Epoch: 070, Loss: 0.5305, Val: 0.8808, Test: 0.8618\n",
      "Epoch: 071, Loss: 0.5234, Val: 0.8807, Test: 0.8617\n",
      "Epoch: 072, Loss: 0.5255, Val: 0.8817, Test: 0.8634\n",
      "Epoch: 073, Loss: 0.5238, Val: 0.8815, Test: 0.8637\n",
      "Epoch: 074, Loss: 0.5274, Val: 0.8802, Test: 0.8629\n",
      "Epoch: 075, Loss: 0.5245, Val: 0.8800, Test: 0.8630\n",
      "Epoch: 076, Loss: 0.5143, Val: 0.8807, Test: 0.8635\n",
      "Epoch: 077, Loss: 0.5207, Val: 0.8818, Test: 0.8644\n",
      "Epoch: 078, Loss: 0.5220, Val: 0.8832, Test: 0.8646\n",
      "Epoch: 079, Loss: 0.5147, Val: 0.8836, Test: 0.8646\n",
      "Epoch: 080, Loss: 0.5251, Val: 0.8831, Test: 0.8633\n",
      "Epoch: 081, Loss: 0.5208, Val: 0.8831, Test: 0.8626\n",
      "Epoch: 082, Loss: 0.5243, Val: 0.8822, Test: 0.8635\n",
      "Epoch: 083, Loss: 0.5204, Val: 0.8799, Test: 0.8632\n",
      "Epoch: 084, Loss: 0.5232, Val: 0.8792, Test: 0.8624\n",
      "Epoch: 085, Loss: 0.5259, Val: 0.8796, Test: 0.8625\n",
      "Epoch: 086, Loss: 0.5244, Val: 0.8802, Test: 0.8629\n",
      "Epoch: 087, Loss: 0.5223, Val: 0.8809, Test: 0.8645\n",
      "Epoch: 088, Loss: 0.5291, Val: 0.8821, Test: 0.8652\n",
      "Epoch: 089, Loss: 0.5179, Val: 0.8829, Test: 0.8653\n",
      "Epoch: 090, Loss: 0.5158, Val: 0.8831, Test: 0.8645\n",
      "Epoch: 091, Loss: 0.5170, Val: 0.8834, Test: 0.8645\n",
      "Epoch: 092, Loss: 0.5143, Val: 0.8837, Test: 0.8650\n",
      "Epoch: 093, Loss: 0.5230, Val: 0.8816, Test: 0.8624\n",
      "Epoch: 094, Loss: 0.5279, Val: 0.8794, Test: 0.8613\n",
      "Epoch: 095, Loss: 0.5154, Val: 0.8801, Test: 0.8630\n",
      "Epoch: 096, Loss: 0.5202, Val: 0.8812, Test: 0.8649\n",
      "Epoch: 097, Loss: 0.5269, Val: 0.8806, Test: 0.8634\n",
      "Epoch: 098, Loss: 0.5188, Val: 0.8803, Test: 0.8618\n",
      "Epoch: 099, Loss: 0.5252, Val: 0.8811, Test: 0.8628\n",
      "Epoch: 100, Loss: 0.5159, Val: 0.8827, Test: 0.8650\n",
      "Final Test: 0.8835\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_ase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_ase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8276)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / 48 / 230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLASE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.9126, Val: 0.7181, Test: 0.6946\n",
      "Epoch: 002, Loss: 0.6863, Val: 0.7589, Test: 0.7235\n",
      "Epoch: 003, Loss: 0.6503, Val: 0.7725, Test: 0.7417\n",
      "Epoch: 004, Loss: 0.6319, Val: 0.8122, Test: 0.7988\n",
      "Epoch: 005, Loss: 0.6119, Val: 0.8434, Test: 0.8359\n",
      "Epoch: 006, Loss: 0.5813, Val: 0.8629, Test: 0.8494\n",
      "Epoch: 007, Loss: 0.5719, Val: 0.8715, Test: 0.8499\n",
      "Epoch: 008, Loss: 0.5638, Val: 0.8738, Test: 0.8505\n",
      "Epoch: 009, Loss: 0.5510, Val: 0.8760, Test: 0.8530\n",
      "Epoch: 010, Loss: 0.5423, Val: 0.8796, Test: 0.8566\n",
      "Epoch: 011, Loss: 0.5639, Val: 0.8812, Test: 0.8583\n",
      "Epoch: 012, Loss: 0.5478, Val: 0.8813, Test: 0.8579\n",
      "Epoch: 013, Loss: 0.5420, Val: 0.8809, Test: 0.8573\n",
      "Epoch: 014, Loss: 0.5383, Val: 0.8801, Test: 0.8570\n",
      "Epoch: 015, Loss: 0.5342, Val: 0.8792, Test: 0.8572\n",
      "Epoch: 016, Loss: 0.5366, Val: 0.8786, Test: 0.8566\n",
      "Epoch: 017, Loss: 0.5367, Val: 0.8778, Test: 0.8558\n",
      "Epoch: 018, Loss: 0.5422, Val: 0.8770, Test: 0.8547\n",
      "Epoch: 019, Loss: 0.5379, Val: 0.8755, Test: 0.8547\n",
      "Epoch: 020, Loss: 0.5311, Val: 0.8767, Test: 0.8555\n",
      "Epoch: 021, Loss: 0.5361, Val: 0.8776, Test: 0.8560\n",
      "Epoch: 022, Loss: 0.5343, Val: 0.8776, Test: 0.8566\n",
      "Epoch: 023, Loss: 0.5402, Val: 0.8771, Test: 0.8570\n",
      "Epoch: 024, Loss: 0.5436, Val: 0.8767, Test: 0.8577\n",
      "Epoch: 025, Loss: 0.5359, Val: 0.8769, Test: 0.8572\n",
      "Epoch: 026, Loss: 0.5385, Val: 0.8770, Test: 0.8568\n",
      "Epoch: 027, Loss: 0.5224, Val: 0.8774, Test: 0.8567\n",
      "Epoch: 028, Loss: 0.5303, Val: 0.8781, Test: 0.8564\n",
      "Epoch: 029, Loss: 0.5319, Val: 0.8783, Test: 0.8566\n",
      "Epoch: 030, Loss: 0.5310, Val: 0.8783, Test: 0.8566\n",
      "Epoch: 031, Loss: 0.5301, Val: 0.8786, Test: 0.8565\n",
      "Epoch: 032, Loss: 0.5348, Val: 0.8790, Test: 0.8569\n",
      "Epoch: 033, Loss: 0.5407, Val: 0.8791, Test: 0.8571\n",
      "Epoch: 034, Loss: 0.5422, Val: 0.8788, Test: 0.8570\n",
      "Epoch: 035, Loss: 0.5349, Val: 0.8781, Test: 0.8571\n",
      "Epoch: 036, Loss: 0.5328, Val: 0.8777, Test: 0.8586\n",
      "Epoch: 037, Loss: 0.5291, Val: 0.8771, Test: 0.8589\n",
      "Epoch: 038, Loss: 0.5271, Val: 0.8777, Test: 0.8588\n",
      "Epoch: 039, Loss: 0.5292, Val: 0.8783, Test: 0.8572\n",
      "Epoch: 040, Loss: 0.5337, Val: 0.8790, Test: 0.8571\n",
      "Epoch: 041, Loss: 0.5316, Val: 0.8787, Test: 0.8567\n",
      "Epoch: 042, Loss: 0.5237, Val: 0.8776, Test: 0.8575\n",
      "Epoch: 043, Loss: 0.5331, Val: 0.8775, Test: 0.8578\n",
      "Epoch: 044, Loss: 0.5267, Val: 0.8777, Test: 0.8573\n",
      "Epoch: 045, Loss: 0.5383, Val: 0.8783, Test: 0.8565\n",
      "Epoch: 046, Loss: 0.5353, Val: 0.8789, Test: 0.8568\n",
      "Epoch: 047, Loss: 0.5307, Val: 0.8789, Test: 0.8567\n",
      "Epoch: 048, Loss: 0.5340, Val: 0.8789, Test: 0.8567\n",
      "Epoch: 049, Loss: 0.5362, Val: 0.8784, Test: 0.8568\n",
      "Epoch: 050, Loss: 0.5325, Val: 0.8787, Test: 0.8568\n",
      "Epoch: 051, Loss: 0.5310, Val: 0.8786, Test: 0.8569\n",
      "Epoch: 052, Loss: 0.5291, Val: 0.8783, Test: 0.8569\n",
      "Epoch: 053, Loss: 0.5382, Val: 0.8782, Test: 0.8570\n",
      "Epoch: 054, Loss: 0.5282, Val: 0.8784, Test: 0.8571\n",
      "Epoch: 055, Loss: 0.5343, Val: 0.8785, Test: 0.8573\n",
      "Epoch: 056, Loss: 0.5314, Val: 0.8784, Test: 0.8575\n",
      "Epoch: 057, Loss: 0.5340, Val: 0.8790, Test: 0.8571\n",
      "Epoch: 058, Loss: 0.5308, Val: 0.8791, Test: 0.8571\n",
      "Epoch: 059, Loss: 0.5273, Val: 0.8787, Test: 0.8570\n",
      "Epoch: 060, Loss: 0.5319, Val: 0.8781, Test: 0.8577\n",
      "Epoch: 061, Loss: 0.5373, Val: 0.8781, Test: 0.8578\n",
      "Epoch: 062, Loss: 0.5360, Val: 0.8786, Test: 0.8572\n",
      "Epoch: 063, Loss: 0.5332, Val: 0.8791, Test: 0.8571\n",
      "Epoch: 064, Loss: 0.5379, Val: 0.8783, Test: 0.8580\n",
      "Epoch: 065, Loss: 0.5338, Val: 0.8781, Test: 0.8587\n",
      "Epoch: 066, Loss: 0.5273, Val: 0.8779, Test: 0.8583\n",
      "Epoch: 067, Loss: 0.5360, Val: 0.8785, Test: 0.8577\n",
      "Epoch: 068, Loss: 0.5270, Val: 0.8785, Test: 0.8577\n",
      "Epoch: 069, Loss: 0.5221, Val: 0.8789, Test: 0.8585\n",
      "Epoch: 070, Loss: 0.5288, Val: 0.8787, Test: 0.8584\n",
      "Epoch: 071, Loss: 0.5269, Val: 0.8791, Test: 0.8580\n",
      "Epoch: 072, Loss: 0.5341, Val: 0.8796, Test: 0.8573\n",
      "Epoch: 073, Loss: 0.5324, Val: 0.8792, Test: 0.8572\n",
      "Epoch: 074, Loss: 0.5296, Val: 0.8787, Test: 0.8578\n",
      "Epoch: 075, Loss: 0.5302, Val: 0.8785, Test: 0.8592\n",
      "Epoch: 076, Loss: 0.5286, Val: 0.8787, Test: 0.8595\n",
      "Epoch: 077, Loss: 0.5226, Val: 0.8785, Test: 0.8585\n",
      "Epoch: 078, Loss: 0.5316, Val: 0.8796, Test: 0.8578\n",
      "Epoch: 079, Loss: 0.5327, Val: 0.8803, Test: 0.8576\n",
      "Epoch: 080, Loss: 0.5304, Val: 0.8803, Test: 0.8580\n",
      "Epoch: 081, Loss: 0.5317, Val: 0.8795, Test: 0.8584\n",
      "Epoch: 082, Loss: 0.5325, Val: 0.8794, Test: 0.8593\n",
      "Epoch: 083, Loss: 0.5287, Val: 0.8788, Test: 0.8592\n",
      "Epoch: 084, Loss: 0.5304, Val: 0.8784, Test: 0.8587\n",
      "Epoch: 085, Loss: 0.5295, Val: 0.8782, Test: 0.8593\n",
      "Epoch: 086, Loss: 0.5305, Val: 0.8787, Test: 0.8597\n",
      "Epoch: 087, Loss: 0.5366, Val: 0.8788, Test: 0.8600\n",
      "Epoch: 088, Loss: 0.5347, Val: 0.8793, Test: 0.8600\n",
      "Epoch: 089, Loss: 0.5336, Val: 0.8802, Test: 0.8604\n",
      "Epoch: 090, Loss: 0.5269, Val: 0.8808, Test: 0.8598\n",
      "Epoch: 091, Loss: 0.5237, Val: 0.8807, Test: 0.8597\n",
      "Epoch: 092, Loss: 0.5339, Val: 0.8800, Test: 0.8604\n",
      "Epoch: 093, Loss: 0.5296, Val: 0.8791, Test: 0.8608\n",
      "Epoch: 094, Loss: 0.5235, Val: 0.8788, Test: 0.8606\n",
      "Epoch: 095, Loss: 0.5293, Val: 0.8791, Test: 0.8601\n",
      "Epoch: 096, Loss: 0.5308, Val: 0.8793, Test: 0.8595\n",
      "Epoch: 097, Loss: 0.5215, Val: 0.8808, Test: 0.8598\n",
      "Epoch: 098, Loss: 0.5259, Val: 0.8820, Test: 0.8608\n",
      "Epoch: 099, Loss: 0.5331, Val: 0.8821, Test: 0.8615\n",
      "Epoch: 100, Loss: 0.5294, Val: 0.8812, Test: 0.8612\n",
      "Final Test: 0.8615\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_glase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_glase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8653)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / 48 / 230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904],\n",
       "        [0.6009, 0.2566, 0.7936, 0.9408, 0.1332],\n",
       "        [0.9346, 0.5936, 0.8694, 0.5677, 0.7411],\n",
       "        ...,\n",
       "        [0.8276, 0.9544, 0.1218, 0.7100, 0.2182],\n",
       "        [0.1134, 0.4780, 0.2132, 0.0694, 0.0327],\n",
       "        [0.1653, 0.9109, 0.7789, 0.3021, 0.1653]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "random_features=torch.rand([num_nodes, 5])\n",
    "random_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Train, Val, Test\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "masked_edge_index = masked_adj.nonzero().t().contiguous()\n",
    "\n",
    "data = Data(x=random_features.float(), x_ase=x_ase, x_glase=x_glase, x_grdpg=x_grdpg, edge_index=masked_edge_index)\n",
    "\n",
    "transform = T.Compose([\n",
    "    # T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6697, Val: 0.7305, Test: 0.7177\n",
      "Epoch: 002, Loss: 0.6806, Val: 0.7419, Test: 0.7341\n",
      "Epoch: 003, Loss: 0.6870, Val: 0.7420, Test: 0.7331\n",
      "Epoch: 004, Loss: 0.6722, Val: 0.7459, Test: 0.7361\n",
      "Epoch: 005, Loss: 0.6757, Val: 0.7499, Test: 0.7404\n",
      "Epoch: 006, Loss: 0.6736, Val: 0.7525, Test: 0.7437\n",
      "Epoch: 007, Loss: 0.6671, Val: 0.7524, Test: 0.7446\n",
      "Epoch: 008, Loss: 0.6627, Val: 0.7539, Test: 0.7466\n",
      "Epoch: 009, Loss: 0.6690, Val: 0.7552, Test: 0.7481\n",
      "Epoch: 010, Loss: 0.6581, Val: 0.7562, Test: 0.7498\n",
      "Epoch: 011, Loss: 0.6577, Val: 0.7573, Test: 0.7521\n",
      "Epoch: 012, Loss: 0.6572, Val: 0.7582, Test: 0.7537\n",
      "Epoch: 013, Loss: 0.6518, Val: 0.7592, Test: 0.7555\n",
      "Epoch: 014, Loss: 0.6484, Val: 0.7604, Test: 0.7568\n",
      "Epoch: 015, Loss: 0.6493, Val: 0.7612, Test: 0.7579\n",
      "Epoch: 016, Loss: 0.6469, Val: 0.7613, Test: 0.7585\n",
      "Epoch: 017, Loss: 0.6418, Val: 0.7603, Test: 0.7591\n",
      "Epoch: 018, Loss: 0.6443, Val: 0.7612, Test: 0.7603\n",
      "Epoch: 019, Loss: 0.6411, Val: 0.7623, Test: 0.7611\n",
      "Epoch: 020, Loss: 0.6387, Val: 0.7625, Test: 0.7614\n",
      "Epoch: 021, Loss: 0.6385, Val: 0.7610, Test: 0.7610\n",
      "Epoch: 022, Loss: 0.6315, Val: 0.7571, Test: 0.7597\n",
      "Epoch: 023, Loss: 0.6331, Val: 0.7546, Test: 0.7585\n",
      "Epoch: 024, Loss: 0.6324, Val: 0.7557, Test: 0.7587\n",
      "Epoch: 025, Loss: 0.6267, Val: 0.7570, Test: 0.7594\n",
      "Epoch: 026, Loss: 0.6276, Val: 0.7522, Test: 0.7573\n",
      "Epoch: 027, Loss: 0.6282, Val: 0.7444, Test: 0.7517\n",
      "Epoch: 028, Loss: 0.6290, Val: 0.7406, Test: 0.7488\n",
      "Epoch: 029, Loss: 0.6275, Val: 0.7440, Test: 0.7506\n",
      "Epoch: 030, Loss: 0.6209, Val: 0.7448, Test: 0.7502\n",
      "Epoch: 031, Loss: 0.6246, Val: 0.7368, Test: 0.7433\n",
      "Epoch: 032, Loss: 0.6241, Val: 0.7237, Test: 0.7340\n",
      "Epoch: 033, Loss: 0.6266, Val: 0.7297, Test: 0.7367\n",
      "Epoch: 034, Loss: 0.6214, Val: 0.7351, Test: 0.7401\n",
      "Epoch: 035, Loss: 0.6185, Val: 0.7313, Test: 0.7367\n",
      "Epoch: 036, Loss: 0.6224, Val: 0.7083, Test: 0.7227\n",
      "Epoch: 037, Loss: 0.6210, Val: 0.7060, Test: 0.7208\n",
      "Epoch: 038, Loss: 0.6183, Val: 0.7261, Test: 0.7305\n",
      "Epoch: 039, Loss: 0.6149, Val: 0.7269, Test: 0.7303\n",
      "Epoch: 040, Loss: 0.6216, Val: 0.7008, Test: 0.7137\n",
      "Epoch: 041, Loss: 0.6191, Val: 0.6818, Test: 0.6942\n",
      "Epoch: 042, Loss: 0.6174, Val: 0.7094, Test: 0.7169\n",
      "Epoch: 043, Loss: 0.6182, Val: 0.7177, Test: 0.7224\n",
      "Epoch: 044, Loss: 0.6144, Val: 0.7032, Test: 0.7125\n",
      "Epoch: 045, Loss: 0.6122, Val: 0.6817, Test: 0.6938\n",
      "Epoch: 046, Loss: 0.6118, Val: 0.6897, Test: 0.7023\n",
      "Epoch: 047, Loss: 0.6139, Val: 0.7049, Test: 0.7111\n",
      "Epoch: 048, Loss: 0.6100, Val: 0.6978, Test: 0.7074\n",
      "Epoch: 049, Loss: 0.6139, Val: 0.6760, Test: 0.6867\n",
      "Epoch: 050, Loss: 0.6160, Val: 0.6666, Test: 0.6773\n",
      "Epoch: 051, Loss: 0.6156, Val: 0.6875, Test: 0.6977\n",
      "Epoch: 052, Loss: 0.6125, Val: 0.6922, Test: 0.7014\n",
      "Epoch: 053, Loss: 0.6108, Val: 0.6697, Test: 0.6803\n",
      "Epoch: 054, Loss: 0.6129, Val: 0.6590, Test: 0.6695\n",
      "Epoch: 055, Loss: 0.6104, Val: 0.6865, Test: 0.6959\n",
      "Epoch: 056, Loss: 0.6106, Val: 0.6864, Test: 0.6956\n",
      "Epoch: 057, Loss: 0.6145, Val: 0.6515, Test: 0.6610\n",
      "Epoch: 058, Loss: 0.6117, Val: 0.6577, Test: 0.6676\n",
      "Epoch: 059, Loss: 0.6152, Val: 0.6769, Test: 0.6884\n",
      "Epoch: 060, Loss: 0.6064, Val: 0.6832, Test: 0.6937\n",
      "Epoch: 061, Loss: 0.6089, Val: 0.6626, Test: 0.6711\n",
      "Epoch: 062, Loss: 0.6098, Val: 0.6507, Test: 0.6609\n",
      "Epoch: 063, Loss: 0.6102, Val: 0.6710, Test: 0.6803\n",
      "Epoch: 064, Loss: 0.6106, Val: 0.6755, Test: 0.6871\n",
      "Epoch: 065, Loss: 0.6106, Val: 0.6561, Test: 0.6646\n",
      "Epoch: 066, Loss: 0.6082, Val: 0.6511, Test: 0.6617\n",
      "Epoch: 067, Loss: 0.6078, Val: 0.6717, Test: 0.6808\n",
      "Epoch: 068, Loss: 0.6092, Val: 0.6721, Test: 0.6812\n",
      "Epoch: 069, Loss: 0.6151, Val: 0.6358, Test: 0.6499\n",
      "Epoch: 070, Loss: 0.6091, Val: 0.6603, Test: 0.6686\n",
      "Epoch: 071, Loss: 0.6107, Val: 0.6775, Test: 0.6875\n",
      "Epoch: 072, Loss: 0.6204, Val: 0.6425, Test: 0.6543\n",
      "Epoch: 073, Loss: 0.6088, Val: 0.6447, Test: 0.6558\n",
      "Epoch: 074, Loss: 0.6132, Val: 0.6693, Test: 0.6765\n",
      "Epoch: 075, Loss: 0.6101, Val: 0.6694, Test: 0.6770\n",
      "Epoch: 076, Loss: 0.6085, Val: 0.6529, Test: 0.6625\n",
      "Epoch: 077, Loss: 0.6080, Val: 0.6525, Test: 0.6624\n",
      "Epoch: 078, Loss: 0.6080, Val: 0.6643, Test: 0.6727\n",
      "Epoch: 079, Loss: 0.6086, Val: 0.6682, Test: 0.6749\n",
      "Epoch: 080, Loss: 0.6106, Val: 0.6526, Test: 0.6620\n",
      "Epoch: 081, Loss: 0.6079, Val: 0.6575, Test: 0.6666\n",
      "Epoch: 082, Loss: 0.6090, Val: 0.6631, Test: 0.6724\n",
      "Epoch: 083, Loss: 0.6048, Val: 0.6733, Test: 0.6787\n",
      "Epoch: 084, Loss: 0.6067, Val: 0.6579, Test: 0.6666\n",
      "Epoch: 085, Loss: 0.6094, Val: 0.6523, Test: 0.6608\n",
      "Epoch: 086, Loss: 0.6081, Val: 0.6701, Test: 0.6770\n",
      "Epoch: 087, Loss: 0.6117, Val: 0.6581, Test: 0.6658\n",
      "Epoch: 088, Loss: 0.6051, Val: 0.6618, Test: 0.6707\n",
      "Epoch: 089, Loss: 0.6114, Val: 0.6607, Test: 0.6685\n",
      "Epoch: 090, Loss: 0.6056, Val: 0.6636, Test: 0.6724\n",
      "Epoch: 091, Loss: 0.6017, Val: 0.6739, Test: 0.6806\n",
      "Epoch: 092, Loss: 0.6088, Val: 0.6540, Test: 0.6602\n",
      "Epoch: 093, Loss: 0.6079, Val: 0.6617, Test: 0.6689\n",
      "Epoch: 094, Loss: 0.6062, Val: 0.6792, Test: 0.6839\n",
      "Epoch: 095, Loss: 0.6141, Val: 0.6465, Test: 0.6532\n",
      "Epoch: 096, Loss: 0.6075, Val: 0.6635, Test: 0.6722\n",
      "Epoch: 097, Loss: 0.6122, Val: 0.6668, Test: 0.6752\n",
      "Epoch: 098, Loss: 0.6094, Val: 0.6591, Test: 0.6655\n",
      "Epoch: 099, Loss: 0.6117, Val: 0.6584, Test: 0.6642\n",
      "Epoch: 100, Loss: 0.6039, Val: 0.6792, Test: 0.6841\n",
      "Final Test: 0.7614\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(5, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4646)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "\n",
    "z = model.encode(data.x, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / (n_L1 + n_L2 + n_L3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7050, Val: 0.7268, Test: 0.7066\n",
      "Epoch: 002, Loss: 0.6750, Val: 0.7590, Test: 0.7476\n",
      "Epoch: 003, Loss: 0.6933, Val: 0.7673, Test: 0.7528\n",
      "Epoch: 004, Loss: 0.6656, Val: 0.7738, Test: 0.7565\n",
      "Epoch: 005, Loss: 0.6650, Val: 0.7772, Test: 0.7549\n",
      "Epoch: 006, Loss: 0.6654, Val: 0.7773, Test: 0.7525\n",
      "Epoch: 007, Loss: 0.6607, Val: 0.7775, Test: 0.7529\n",
      "Epoch: 008, Loss: 0.6514, Val: 0.7783, Test: 0.7542\n",
      "Epoch: 009, Loss: 0.6463, Val: 0.7803, Test: 0.7555\n",
      "Epoch: 010, Loss: 0.6490, Val: 0.7823, Test: 0.7552\n",
      "Epoch: 011, Loss: 0.6403, Val: 0.7834, Test: 0.7535\n",
      "Epoch: 012, Loss: 0.6356, Val: 0.7810, Test: 0.7499\n",
      "Epoch: 013, Loss: 0.6315, Val: 0.7774, Test: 0.7455\n",
      "Epoch: 014, Loss: 0.6279, Val: 0.7762, Test: 0.7431\n",
      "Epoch: 015, Loss: 0.6222, Val: 0.7754, Test: 0.7423\n",
      "Epoch: 016, Loss: 0.6183, Val: 0.7723, Test: 0.7392\n",
      "Epoch: 017, Loss: 0.6131, Val: 0.7659, Test: 0.7296\n",
      "Epoch: 018, Loss: 0.6096, Val: 0.7614, Test: 0.7129\n",
      "Epoch: 019, Loss: 0.6115, Val: 0.7379, Test: 0.7089\n",
      "Epoch: 020, Loss: 0.6035, Val: 0.7426, Test: 0.7132\n",
      "Epoch: 021, Loss: 0.5995, Val: 0.7539, Test: 0.7128\n",
      "Epoch: 022, Loss: 0.5907, Val: 0.7581, Test: 0.7175\n",
      "Epoch: 023, Loss: 0.5924, Val: 0.7625, Test: 0.7457\n",
      "Epoch: 024, Loss: 0.5840, Val: 0.7471, Test: 0.7643\n",
      "Epoch: 025, Loss: 0.5858, Val: 0.7416, Test: 0.7663\n",
      "Epoch: 026, Loss: 0.5814, Val: 0.7473, Test: 0.7669\n",
      "Epoch: 027, Loss: 0.5793, Val: 0.7506, Test: 0.7649\n",
      "Epoch: 028, Loss: 0.5734, Val: 0.7477, Test: 0.7669\n",
      "Epoch: 029, Loss: 0.5651, Val: 0.7411, Test: 0.7665\n",
      "Epoch: 030, Loss: 0.5688, Val: 0.7351, Test: 0.7628\n",
      "Epoch: 031, Loss: 0.5776, Val: 0.7388, Test: 0.7652\n",
      "Epoch: 032, Loss: 0.5703, Val: 0.7453, Test: 0.7668\n",
      "Epoch: 033, Loss: 0.5712, Val: 0.7411, Test: 0.7662\n",
      "Epoch: 034, Loss: 0.5753, Val: 0.7276, Test: 0.7582\n",
      "Epoch: 035, Loss: 0.5733, Val: 0.7323, Test: 0.7618\n",
      "Epoch: 036, Loss: 0.5794, Val: 0.7414, Test: 0.7667\n",
      "Epoch: 037, Loss: 0.5672, Val: 0.7442, Test: 0.7669\n",
      "Epoch: 038, Loss: 0.5753, Val: 0.7390, Test: 0.7672\n",
      "Epoch: 039, Loss: 0.5732, Val: 0.7314, Test: 0.7634\n",
      "Epoch: 040, Loss: 0.5713, Val: 0.7360, Test: 0.7668\n",
      "Epoch: 041, Loss: 0.5687, Val: 0.7444, Test: 0.7681\n",
      "Epoch: 042, Loss: 0.5681, Val: 0.7468, Test: 0.7670\n",
      "Epoch: 043, Loss: 0.5723, Val: 0.7434, Test: 0.7689\n",
      "Epoch: 044, Loss: 0.5717, Val: 0.7360, Test: 0.7676\n",
      "Epoch: 045, Loss: 0.5769, Val: 0.7390, Test: 0.7694\n",
      "Epoch: 046, Loss: 0.5733, Val: 0.7468, Test: 0.7683\n",
      "Epoch: 047, Loss: 0.5765, Val: 0.7486, Test: 0.7675\n",
      "Epoch: 048, Loss: 0.5778, Val: 0.7447, Test: 0.7701\n",
      "Epoch: 049, Loss: 0.5668, Val: 0.7410, Test: 0.7702\n",
      "Epoch: 050, Loss: 0.5710, Val: 0.7441, Test: 0.7703\n",
      "Epoch: 051, Loss: 0.5716, Val: 0.7462, Test: 0.7694\n",
      "Epoch: 052, Loss: 0.5865, Val: 0.7418, Test: 0.7702\n",
      "Epoch: 053, Loss: 0.5731, Val: 0.7368, Test: 0.7694\n",
      "Epoch: 054, Loss: 0.5705, Val: 0.7406, Test: 0.7702\n",
      "Epoch: 055, Loss: 0.5670, Val: 0.7454, Test: 0.7699\n",
      "Epoch: 056, Loss: 0.5718, Val: 0.7456, Test: 0.7692\n",
      "Epoch: 057, Loss: 0.5707, Val: 0.7405, Test: 0.7701\n",
      "Epoch: 058, Loss: 0.5674, Val: 0.7360, Test: 0.7691\n",
      "Epoch: 059, Loss: 0.5698, Val: 0.7385, Test: 0.7700\n",
      "Epoch: 060, Loss: 0.5725, Val: 0.7434, Test: 0.7702\n",
      "Epoch: 061, Loss: 0.5677, Val: 0.7437, Test: 0.7704\n",
      "Epoch: 062, Loss: 0.5695, Val: 0.7401, Test: 0.7704\n",
      "Epoch: 063, Loss: 0.5705, Val: 0.7359, Test: 0.7693\n",
      "Epoch: 064, Loss: 0.5656, Val: 0.7409, Test: 0.7706\n",
      "Epoch: 065, Loss: 0.5656, Val: 0.7459, Test: 0.7704\n",
      "Epoch: 066, Loss: 0.5745, Val: 0.7419, Test: 0.7708\n",
      "Epoch: 067, Loss: 0.5637, Val: 0.7377, Test: 0.7705\n",
      "Epoch: 068, Loss: 0.5681, Val: 0.7388, Test: 0.7707\n",
      "Epoch: 069, Loss: 0.5698, Val: 0.7420, Test: 0.7711\n",
      "Epoch: 070, Loss: 0.5747, Val: 0.7427, Test: 0.7712\n",
      "Epoch: 071, Loss: 0.5654, Val: 0.7430, Test: 0.7714\n",
      "Epoch: 072, Loss: 0.5689, Val: 0.7428, Test: 0.7718\n",
      "Epoch: 073, Loss: 0.5726, Val: 0.7429, Test: 0.7722\n",
      "Epoch: 074, Loss: 0.5577, Val: 0.7456, Test: 0.7723\n",
      "Epoch: 075, Loss: 0.5689, Val: 0.7452, Test: 0.7728\n",
      "Epoch: 076, Loss: 0.5688, Val: 0.7400, Test: 0.7727\n",
      "Epoch: 077, Loss: 0.5724, Val: 0.7387, Test: 0.7727\n",
      "Epoch: 078, Loss: 0.5707, Val: 0.7454, Test: 0.7726\n",
      "Epoch: 079, Loss: 0.5620, Val: 0.7476, Test: 0.7710\n",
      "Epoch: 080, Loss: 0.5723, Val: 0.7428, Test: 0.7728\n",
      "Epoch: 081, Loss: 0.5650, Val: 0.7387, Test: 0.7724\n",
      "Epoch: 082, Loss: 0.5703, Val: 0.7438, Test: 0.7727\n",
      "Epoch: 083, Loss: 0.5628, Val: 0.7467, Test: 0.7714\n",
      "Epoch: 084, Loss: 0.5633, Val: 0.7463, Test: 0.7721\n",
      "Epoch: 085, Loss: 0.5671, Val: 0.7409, Test: 0.7730\n",
      "Epoch: 086, Loss: 0.5666, Val: 0.7426, Test: 0.7733\n",
      "Epoch: 087, Loss: 0.5667, Val: 0.7433, Test: 0.7733\n",
      "Epoch: 088, Loss: 0.5610, Val: 0.7456, Test: 0.7725\n",
      "Epoch: 089, Loss: 0.5697, Val: 0.7427, Test: 0.7734\n",
      "Epoch: 090, Loss: 0.5617, Val: 0.7428, Test: 0.7735\n",
      "Epoch: 091, Loss: 0.5650, Val: 0.7427, Test: 0.7736\n",
      "Epoch: 092, Loss: 0.5648, Val: 0.7434, Test: 0.7740\n",
      "Epoch: 093, Loss: 0.5597, Val: 0.7465, Test: 0.7735\n",
      "Epoch: 094, Loss: 0.5695, Val: 0.7444, Test: 0.7741\n",
      "Epoch: 095, Loss: 0.5573, Val: 0.7419, Test: 0.7743\n",
      "Epoch: 096, Loss: 0.5652, Val: 0.7424, Test: 0.7741\n",
      "Epoch: 097, Loss: 0.5648, Val: 0.7444, Test: 0.7736\n",
      "Epoch: 098, Loss: 0.5754, Val: 0.7412, Test: 0.7737\n",
      "Epoch: 099, Loss: 0.5567, Val: 0.7435, Test: 0.7736\n",
      "Epoch: 100, Loss: 0.5640, Val: 0.7438, Test: 0.7741\n",
      "Final Test: 0.7535\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_ase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_ase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7337)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / (n_L1 + n_L2 + n_L3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLASE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.9386, Val: 0.5728, Test: 0.5463\n",
      "Epoch: 002, Loss: 0.7005, Val: 0.7178, Test: 0.6992\n",
      "Epoch: 003, Loss: 0.6910, Val: 0.7487, Test: 0.7327\n",
      "Epoch: 004, Loss: 0.6908, Val: 0.7542, Test: 0.7363\n",
      "Epoch: 005, Loss: 0.6768, Val: 0.7554, Test: 0.7316\n",
      "Epoch: 006, Loss: 0.6690, Val: 0.7578, Test: 0.7271\n",
      "Epoch: 007, Loss: 0.6661, Val: 0.7622, Test: 0.7238\n",
      "Epoch: 008, Loss: 0.6600, Val: 0.7696, Test: 0.7268\n",
      "Epoch: 009, Loss: 0.6540, Val: 0.7741, Test: 0.7313\n",
      "Epoch: 010, Loss: 0.6486, Val: 0.7745, Test: 0.7323\n",
      "Epoch: 011, Loss: 0.6400, Val: 0.7693, Test: 0.7279\n",
      "Epoch: 012, Loss: 0.6376, Val: 0.7581, Test: 0.7196\n",
      "Epoch: 013, Loss: 0.6292, Val: 0.7486, Test: 0.7113\n",
      "Epoch: 014, Loss: 0.6280, Val: 0.7436, Test: 0.7054\n",
      "Epoch: 015, Loss: 0.6225, Val: 0.7435, Test: 0.7034\n",
      "Epoch: 016, Loss: 0.6167, Val: 0.7432, Test: 0.7029\n",
      "Epoch: 017, Loss: 0.6087, Val: 0.7443, Test: 0.7037\n",
      "Epoch: 018, Loss: 0.6037, Val: 0.7566, Test: 0.7147\n",
      "Epoch: 019, Loss: 0.6038, Val: 0.7606, Test: 0.7383\n",
      "Epoch: 020, Loss: 0.5915, Val: 0.7562, Test: 0.7483\n",
      "Epoch: 021, Loss: 0.5943, Val: 0.7532, Test: 0.7521\n",
      "Epoch: 022, Loss: 0.5907, Val: 0.7531, Test: 0.7537\n",
      "Epoch: 023, Loss: 0.5872, Val: 0.7519, Test: 0.7549\n",
      "Epoch: 024, Loss: 0.5914, Val: 0.7485, Test: 0.7553\n",
      "Epoch: 025, Loss: 0.5874, Val: 0.7457, Test: 0.7554\n",
      "Epoch: 026, Loss: 0.5824, Val: 0.7465, Test: 0.7573\n",
      "Epoch: 027, Loss: 0.5793, Val: 0.7491, Test: 0.7598\n",
      "Epoch: 028, Loss: 0.5826, Val: 0.7484, Test: 0.7611\n",
      "Epoch: 029, Loss: 0.5757, Val: 0.7450, Test: 0.7613\n",
      "Epoch: 030, Loss: 0.5810, Val: 0.7439, Test: 0.7622\n",
      "Epoch: 031, Loss: 0.5758, Val: 0.7471, Test: 0.7650\n",
      "Epoch: 032, Loss: 0.5763, Val: 0.7498, Test: 0.7666\n",
      "Epoch: 033, Loss: 0.5732, Val: 0.7512, Test: 0.7671\n",
      "Epoch: 034, Loss: 0.5758, Val: 0.7482, Test: 0.7680\n",
      "Epoch: 035, Loss: 0.5781, Val: 0.7462, Test: 0.7685\n",
      "Epoch: 036, Loss: 0.5741, Val: 0.7481, Test: 0.7694\n",
      "Epoch: 037, Loss: 0.5707, Val: 0.7523, Test: 0.7697\n",
      "Epoch: 038, Loss: 0.5736, Val: 0.7530, Test: 0.7698\n",
      "Epoch: 039, Loss: 0.5772, Val: 0.7511, Test: 0.7706\n",
      "Epoch: 040, Loss: 0.5713, Val: 0.7477, Test: 0.7716\n",
      "Epoch: 041, Loss: 0.5708, Val: 0.7468, Test: 0.7720\n",
      "Epoch: 042, Loss: 0.5703, Val: 0.7484, Test: 0.7720\n",
      "Epoch: 043, Loss: 0.5735, Val: 0.7497, Test: 0.7721\n",
      "Epoch: 044, Loss: 0.5701, Val: 0.7482, Test: 0.7724\n",
      "Epoch: 045, Loss: 0.5781, Val: 0.7440, Test: 0.7725\n",
      "Epoch: 046, Loss: 0.5728, Val: 0.7429, Test: 0.7724\n",
      "Epoch: 047, Loss: 0.5729, Val: 0.7470, Test: 0.7734\n",
      "Epoch: 048, Loss: 0.5752, Val: 0.7496, Test: 0.7730\n",
      "Epoch: 049, Loss: 0.5763, Val: 0.7502, Test: 0.7731\n",
      "Epoch: 050, Loss: 0.5698, Val: 0.7477, Test: 0.7740\n",
      "Epoch: 051, Loss: 0.5686, Val: 0.7463, Test: 0.7742\n",
      "Epoch: 052, Loss: 0.5698, Val: 0.7469, Test: 0.7743\n",
      "Epoch: 053, Loss: 0.5719, Val: 0.7487, Test: 0.7734\n",
      "Epoch: 054, Loss: 0.5666, Val: 0.7495, Test: 0.7731\n",
      "Epoch: 055, Loss: 0.5734, Val: 0.7463, Test: 0.7741\n",
      "Epoch: 056, Loss: 0.5708, Val: 0.7454, Test: 0.7747\n",
      "Epoch: 057, Loss: 0.5683, Val: 0.7472, Test: 0.7745\n",
      "Epoch: 058, Loss: 0.5721, Val: 0.7483, Test: 0.7744\n",
      "Epoch: 059, Loss: 0.5802, Val: 0.7476, Test: 0.7751\n",
      "Epoch: 060, Loss: 0.5703, Val: 0.7472, Test: 0.7750\n",
      "Epoch: 061, Loss: 0.5682, Val: 0.7480, Test: 0.7750\n",
      "Epoch: 062, Loss: 0.5695, Val: 0.7489, Test: 0.7749\n",
      "Epoch: 063, Loss: 0.5677, Val: 0.7487, Test: 0.7746\n",
      "Epoch: 064, Loss: 0.5726, Val: 0.7459, Test: 0.7748\n",
      "Epoch: 065, Loss: 0.5654, Val: 0.7458, Test: 0.7746\n",
      "Epoch: 066, Loss: 0.5726, Val: 0.7460, Test: 0.7747\n",
      "Epoch: 067, Loss: 0.5652, Val: 0.7486, Test: 0.7740\n",
      "Epoch: 068, Loss: 0.5710, Val: 0.7490, Test: 0.7740\n",
      "Epoch: 069, Loss: 0.5698, Val: 0.7470, Test: 0.7748\n",
      "Epoch: 070, Loss: 0.5709, Val: 0.7468, Test: 0.7749\n",
      "Epoch: 071, Loss: 0.5706, Val: 0.7470, Test: 0.7748\n",
      "Epoch: 072, Loss: 0.5717, Val: 0.7498, Test: 0.7740\n",
      "Epoch: 073, Loss: 0.5625, Val: 0.7507, Test: 0.7737\n",
      "Epoch: 074, Loss: 0.5657, Val: 0.7500, Test: 0.7742\n",
      "Epoch: 075, Loss: 0.5683, Val: 0.7465, Test: 0.7750\n",
      "Epoch: 076, Loss: 0.5699, Val: 0.7445, Test: 0.7744\n",
      "Epoch: 077, Loss: 0.5756, Val: 0.7448, Test: 0.7744\n",
      "Epoch: 078, Loss: 0.5720, Val: 0.7474, Test: 0.7743\n",
      "Epoch: 079, Loss: 0.5721, Val: 0.7489, Test: 0.7744\n",
      "Epoch: 080, Loss: 0.5667, Val: 0.7475, Test: 0.7753\n",
      "Epoch: 081, Loss: 0.5707, Val: 0.7468, Test: 0.7756\n",
      "Epoch: 082, Loss: 0.5716, Val: 0.7479, Test: 0.7755\n",
      "Epoch: 083, Loss: 0.5689, Val: 0.7494, Test: 0.7754\n",
      "Epoch: 084, Loss: 0.5696, Val: 0.7498, Test: 0.7746\n",
      "Epoch: 085, Loss: 0.5682, Val: 0.7465, Test: 0.7749\n",
      "Epoch: 086, Loss: 0.5634, Val: 0.7456, Test: 0.7749\n",
      "Epoch: 087, Loss: 0.5666, Val: 0.7451, Test: 0.7747\n",
      "Epoch: 088, Loss: 0.5623, Val: 0.7455, Test: 0.7750\n",
      "Epoch: 089, Loss: 0.5665, Val: 0.7461, Test: 0.7755\n",
      "Epoch: 090, Loss: 0.5643, Val: 0.7482, Test: 0.7760\n",
      "Epoch: 091, Loss: 0.5661, Val: 0.7482, Test: 0.7760\n",
      "Epoch: 092, Loss: 0.5650, Val: 0.7474, Test: 0.7759\n",
      "Epoch: 093, Loss: 0.5670, Val: 0.7462, Test: 0.7757\n",
      "Epoch: 094, Loss: 0.5682, Val: 0.7463, Test: 0.7753\n",
      "Epoch: 095, Loss: 0.5695, Val: 0.7457, Test: 0.7753\n",
      "Epoch: 096, Loss: 0.5684, Val: 0.7451, Test: 0.7754\n",
      "Epoch: 097, Loss: 0.5751, Val: 0.7460, Test: 0.7757\n",
      "Epoch: 098, Loss: 0.5677, Val: 0.7482, Test: 0.7759\n",
      "Epoch: 099, Loss: 0.5677, Val: 0.7487, Test: 0.7762\n",
      "Epoch: 100, Loss: 0.5697, Val: 0.7475, Test: 0.7766\n",
      "Final Test: 0.7323\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_glase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_glase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7603)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / (n_L1 + n_L2 + n_L3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7426, Val: 0.6737, Test: 0.6621\n",
      "Epoch: 002, Loss: 0.6878, Val: 0.8423, Test: 0.8082\n",
      "Epoch: 003, Loss: 0.6708, Val: 0.8571, Test: 0.8260\n",
      "Epoch: 004, Loss: 0.6635, Val: 0.8691, Test: 0.8522\n",
      "Epoch: 005, Loss: 0.6459, Val: 0.8641, Test: 0.8548\n",
      "Epoch: 006, Loss: 0.6362, Val: 0.8437, Test: 0.8399\n",
      "Epoch: 007, Loss: 0.6271, Val: 0.8298, Test: 0.8310\n",
      "Epoch: 008, Loss: 0.6136, Val: 0.8240, Test: 0.8293\n",
      "Epoch: 009, Loss: 0.6011, Val: 0.8218, Test: 0.8302\n",
      "Epoch: 010, Loss: 0.5850, Val: 0.8196, Test: 0.8302\n",
      "Epoch: 011, Loss: 0.5822, Val: 0.8173, Test: 0.8303\n",
      "Epoch: 012, Loss: 0.5798, Val: 0.8129, Test: 0.8284\n",
      "Epoch: 013, Loss: 0.5805, Val: 0.8109, Test: 0.8275\n",
      "Epoch: 014, Loss: 0.5782, Val: 0.8125, Test: 0.8282\n",
      "Epoch: 015, Loss: 0.5736, Val: 0.8104, Test: 0.8254\n",
      "Epoch: 016, Loss: 0.5723, Val: 0.8114, Test: 0.8263\n",
      "Epoch: 017, Loss: 0.5813, Val: 0.8122, Test: 0.8278\n",
      "Epoch: 018, Loss: 0.5755, Val: 0.8113, Test: 0.8273\n",
      "Epoch: 019, Loss: 0.5724, Val: 0.8130, Test: 0.8286\n",
      "Epoch: 020, Loss: 0.5679, Val: 0.8142, Test: 0.8291\n",
      "Epoch: 021, Loss: 0.5601, Val: 0.8146, Test: 0.8290\n",
      "Epoch: 022, Loss: 0.5709, Val: 0.8154, Test: 0.8303\n",
      "Epoch: 023, Loss: 0.5749, Val: 0.8147, Test: 0.8294\n",
      "Epoch: 024, Loss: 0.5776, Val: 0.8153, Test: 0.8296\n",
      "Epoch: 025, Loss: 0.5682, Val: 0.8166, Test: 0.8300\n",
      "Epoch: 026, Loss: 0.5648, Val: 0.8167, Test: 0.8292\n",
      "Epoch: 027, Loss: 0.5654, Val: 0.8164, Test: 0.8294\n",
      "Epoch: 028, Loss: 0.5684, Val: 0.8184, Test: 0.8305\n",
      "Epoch: 029, Loss: 0.5744, Val: 0.8200, Test: 0.8322\n",
      "Epoch: 030, Loss: 0.5649, Val: 0.8208, Test: 0.8327\n",
      "Epoch: 031, Loss: 0.5632, Val: 0.8219, Test: 0.8334\n",
      "Epoch: 032, Loss: 0.5633, Val: 0.8224, Test: 0.8338\n",
      "Epoch: 033, Loss: 0.5665, Val: 0.8214, Test: 0.8334\n",
      "Epoch: 034, Loss: 0.5618, Val: 0.8208, Test: 0.8330\n",
      "Epoch: 035, Loss: 0.5612, Val: 0.8193, Test: 0.8323\n",
      "Epoch: 036, Loss: 0.5597, Val: 0.8189, Test: 0.8321\n",
      "Epoch: 037, Loss: 0.5591, Val: 0.8195, Test: 0.8322\n",
      "Epoch: 038, Loss: 0.5615, Val: 0.8203, Test: 0.8323\n",
      "Epoch: 039, Loss: 0.5600, Val: 0.8217, Test: 0.8331\n",
      "Epoch: 040, Loss: 0.5591, Val: 0.8237, Test: 0.8344\n",
      "Epoch: 041, Loss: 0.5545, Val: 0.8258, Test: 0.8353\n",
      "Epoch: 042, Loss: 0.5584, Val: 0.8264, Test: 0.8355\n",
      "Epoch: 043, Loss: 0.5545, Val: 0.8255, Test: 0.8353\n",
      "Epoch: 044, Loss: 0.5532, Val: 0.8250, Test: 0.8348\n",
      "Epoch: 045, Loss: 0.5526, Val: 0.8249, Test: 0.8346\n",
      "Epoch: 046, Loss: 0.5576, Val: 0.8256, Test: 0.8345\n",
      "Epoch: 047, Loss: 0.5488, Val: 0.8269, Test: 0.8353\n",
      "Epoch: 048, Loss: 0.5524, Val: 0.8283, Test: 0.8362\n",
      "Epoch: 049, Loss: 0.5538, Val: 0.8280, Test: 0.8362\n",
      "Epoch: 050, Loss: 0.5508, Val: 0.8276, Test: 0.8359\n",
      "Epoch: 051, Loss: 0.5468, Val: 0.8269, Test: 0.8353\n",
      "Epoch: 052, Loss: 0.5433, Val: 0.8275, Test: 0.8356\n",
      "Epoch: 053, Loss: 0.5502, Val: 0.8289, Test: 0.8365\n",
      "Epoch: 054, Loss: 0.5434, Val: 0.8311, Test: 0.8379\n",
      "Epoch: 055, Loss: 0.5422, Val: 0.8333, Test: 0.8392\n",
      "Epoch: 056, Loss: 0.5444, Val: 0.8336, Test: 0.8391\n",
      "Epoch: 057, Loss: 0.5444, Val: 0.8318, Test: 0.8383\n",
      "Epoch: 058, Loss: 0.5458, Val: 0.8303, Test: 0.8376\n",
      "Epoch: 059, Loss: 0.5445, Val: 0.8311, Test: 0.8385\n",
      "Epoch: 060, Loss: 0.5439, Val: 0.8335, Test: 0.8396\n",
      "Epoch: 061, Loss: 0.5392, Val: 0.8355, Test: 0.8402\n",
      "Epoch: 062, Loss: 0.5385, Val: 0.8344, Test: 0.8393\n",
      "Epoch: 063, Loss: 0.5382, Val: 0.8311, Test: 0.8370\n",
      "Epoch: 064, Loss: 0.5335, Val: 0.8311, Test: 0.8374\n",
      "Epoch: 065, Loss: 0.5347, Val: 0.8302, Test: 0.8367\n",
      "Epoch: 066, Loss: 0.5400, Val: 0.8287, Test: 0.8351\n",
      "Epoch: 067, Loss: 0.5336, Val: 0.8296, Test: 0.8359\n",
      "Epoch: 068, Loss: 0.5383, Val: 0.8313, Test: 0.8375\n",
      "Epoch: 069, Loss: 0.5359, Val: 0.8298, Test: 0.8363\n",
      "Epoch: 070, Loss: 0.5309, Val: 0.8302, Test: 0.8362\n",
      "Epoch: 071, Loss: 0.5371, Val: 0.8305, Test: 0.8364\n",
      "Epoch: 072, Loss: 0.5326, Val: 0.8307, Test: 0.8365\n",
      "Epoch: 073, Loss: 0.5317, Val: 0.8310, Test: 0.8366\n",
      "Epoch: 074, Loss: 0.5347, Val: 0.8299, Test: 0.8360\n",
      "Epoch: 075, Loss: 0.5351, Val: 0.8275, Test: 0.8345\n",
      "Epoch: 076, Loss: 0.5364, Val: 0.8264, Test: 0.8336\n",
      "Epoch: 077, Loss: 0.5302, Val: 0.8312, Test: 0.8359\n",
      "Epoch: 078, Loss: 0.5308, Val: 0.8296, Test: 0.8349\n",
      "Epoch: 079, Loss: 0.5312, Val: 0.8247, Test: 0.8323\n",
      "Epoch: 080, Loss: 0.5305, Val: 0.8228, Test: 0.8311\n",
      "Epoch: 081, Loss: 0.5378, Val: 0.8246, Test: 0.8318\n",
      "Epoch: 082, Loss: 0.5349, Val: 0.8248, Test: 0.8310\n",
      "Epoch: 083, Loss: 0.5345, Val: 0.8225, Test: 0.8290\n",
      "Epoch: 084, Loss: 0.5370, Val: 0.8242, Test: 0.8293\n",
      "Epoch: 085, Loss: 0.5353, Val: 0.8256, Test: 0.8297\n",
      "Epoch: 086, Loss: 0.5295, Val: 0.8286, Test: 0.8318\n",
      "Epoch: 087, Loss: 0.5378, Val: 0.8225, Test: 0.8287\n",
      "Epoch: 088, Loss: 0.5369, Val: 0.8207, Test: 0.8277\n",
      "Epoch: 089, Loss: 0.5381, Val: 0.8223, Test: 0.8288\n",
      "Epoch: 090, Loss: 0.5224, Val: 0.8288, Test: 0.8324\n",
      "Epoch: 091, Loss: 0.5338, Val: 0.8247, Test: 0.8301\n",
      "Epoch: 092, Loss: 0.5347, Val: 0.8155, Test: 0.8249\n",
      "Epoch: 093, Loss: 0.5275, Val: 0.8242, Test: 0.8291\n",
      "Epoch: 094, Loss: 0.5301, Val: 0.8328, Test: 0.8338\n",
      "Epoch: 095, Loss: 0.5268, Val: 0.8277, Test: 0.8312\n",
      "Epoch: 096, Loss: 0.5321, Val: 0.8138, Test: 0.8232\n",
      "Epoch: 097, Loss: 0.5359, Val: 0.8173, Test: 0.8255\n",
      "Epoch: 098, Loss: 0.5305, Val: 0.8291, Test: 0.8319\n",
      "Epoch: 099, Loss: 0.5307, Val: 0.8286, Test: 0.8317\n",
      "Epoch: 100, Loss: 0.5281, Val: 0.8195, Test: 0.8273\n",
      "Final Test: 0.8522\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_grdpg), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_grdpg), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_grdpg), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8257)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_grdpg), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / (n_L1 + n_L2 + n_L3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "ones_features=torch.ones([410, 5])\n",
    "ones_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Train, Val, Test\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "masked_edge_index = masked_adj.nonzero().t().contiguous()\n",
    "\n",
    "data = Data(x=random_features.float(), x_ase=x_ase, x_glase=x_glase, edge_index=masked_edge_index)\n",
    "\n",
    "transform = T.Compose([\n",
    "    # T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6809, Val: 0.7945, Test: 0.7740\n",
      "Epoch: 002, Loss: 0.6746, Val: 0.7971, Test: 0.7769\n",
      "Epoch: 003, Loss: 0.6950, Val: 0.7963, Test: 0.7767\n",
      "Epoch: 004, Loss: 0.6680, Val: 0.7958, Test: 0.7767\n",
      "Epoch: 005, Loss: 0.6728, Val: 0.7960, Test: 0.7770\n",
      "Epoch: 006, Loss: 0.6740, Val: 0.7967, Test: 0.7769\n",
      "Epoch: 007, Loss: 0.6702, Val: 0.7971, Test: 0.7770\n",
      "Epoch: 008, Loss: 0.6666, Val: 0.7976, Test: 0.7770\n",
      "Epoch: 009, Loss: 0.6665, Val: 0.7977, Test: 0.7771\n",
      "Epoch: 010, Loss: 0.6687, Val: 0.7976, Test: 0.7770\n",
      "Epoch: 011, Loss: 0.6626, Val: 0.7970, Test: 0.7770\n",
      "Epoch: 012, Loss: 0.6600, Val: 0.7965, Test: 0.7766\n",
      "Epoch: 013, Loss: 0.6582, Val: 0.7964, Test: 0.7763\n",
      "Epoch: 014, Loss: 0.6561, Val: 0.7963, Test: 0.7763\n",
      "Epoch: 015, Loss: 0.6524, Val: 0.7970, Test: 0.7766\n",
      "Epoch: 016, Loss: 0.6488, Val: 0.7971, Test: 0.7768\n",
      "Epoch: 017, Loss: 0.6482, Val: 0.7968, Test: 0.7768\n",
      "Epoch: 018, Loss: 0.6453, Val: 0.7962, Test: 0.7761\n",
      "Epoch: 019, Loss: 0.6414, Val: 0.7954, Test: 0.7750\n",
      "Epoch: 020, Loss: 0.6410, Val: 0.7946, Test: 0.7740\n",
      "Epoch: 021, Loss: 0.6389, Val: 0.7941, Test: 0.7734\n",
      "Epoch: 022, Loss: 0.6380, Val: 0.7941, Test: 0.7734\n",
      "Epoch: 023, Loss: 0.6368, Val: 0.7941, Test: 0.7736\n",
      "Epoch: 024, Loss: 0.6316, Val: 0.7937, Test: 0.7735\n",
      "Epoch: 025, Loss: 0.6322, Val: 0.7924, Test: 0.7723\n",
      "Epoch: 026, Loss: 0.6282, Val: 0.7901, Test: 0.7705\n",
      "Epoch: 027, Loss: 0.6278, Val: 0.7879, Test: 0.7686\n",
      "Epoch: 028, Loss: 0.6280, Val: 0.7867, Test: 0.7676\n",
      "Epoch: 029, Loss: 0.6200, Val: 0.7880, Test: 0.7687\n",
      "Epoch: 030, Loss: 0.6225, Val: 0.7870, Test: 0.7681\n",
      "Epoch: 031, Loss: 0.6191, Val: 0.7841, Test: 0.7654\n",
      "Epoch: 032, Loss: 0.6198, Val: 0.7795, Test: 0.7606\n",
      "Epoch: 033, Loss: 0.6162, Val: 0.7792, Test: 0.7593\n",
      "Epoch: 034, Loss: 0.6162, Val: 0.7797, Test: 0.7602\n",
      "Epoch: 035, Loss: 0.6167, Val: 0.7796, Test: 0.7597\n",
      "Epoch: 036, Loss: 0.6155, Val: 0.7771, Test: 0.7559\n",
      "Epoch: 037, Loss: 0.6115, Val: 0.7726, Test: 0.7534\n",
      "Epoch: 038, Loss: 0.6128, Val: 0.7725, Test: 0.7533\n",
      "Epoch: 039, Loss: 0.6134, Val: 0.7727, Test: 0.7532\n",
      "Epoch: 040, Loss: 0.6102, Val: 0.7716, Test: 0.7520\n",
      "Epoch: 041, Loss: 0.6098, Val: 0.7685, Test: 0.7504\n",
      "Epoch: 042, Loss: 0.6141, Val: 0.7634, Test: 0.7480\n",
      "Epoch: 043, Loss: 0.6078, Val: 0.7659, Test: 0.7496\n",
      "Epoch: 044, Loss: 0.6050, Val: 0.7683, Test: 0.7503\n",
      "Epoch: 045, Loss: 0.6045, Val: 0.7629, Test: 0.7483\n",
      "Epoch: 046, Loss: 0.6052, Val: 0.7581, Test: 0.7441\n",
      "Epoch: 047, Loss: 0.6005, Val: 0.7597, Test: 0.7460\n",
      "Epoch: 048, Loss: 0.5993, Val: 0.7634, Test: 0.7492\n",
      "Epoch: 049, Loss: 0.6076, Val: 0.7573, Test: 0.7432\n",
      "Epoch: 050, Loss: 0.6031, Val: 0.7543, Test: 0.7386\n",
      "Epoch: 051, Loss: 0.6033, Val: 0.7571, Test: 0.7430\n",
      "Epoch: 052, Loss: 0.6035, Val: 0.7579, Test: 0.7444\n",
      "Epoch: 053, Loss: 0.5991, Val: 0.7564, Test: 0.7415\n",
      "Epoch: 054, Loss: 0.5981, Val: 0.7547, Test: 0.7385\n",
      "Epoch: 055, Loss: 0.5979, Val: 0.7552, Test: 0.7394\n",
      "Epoch: 056, Loss: 0.6024, Val: 0.7555, Test: 0.7399\n",
      "Epoch: 057, Loss: 0.5980, Val: 0.7550, Test: 0.7391\n",
      "Epoch: 058, Loss: 0.6022, Val: 0.7527, Test: 0.7359\n",
      "Epoch: 059, Loss: 0.5947, Val: 0.7543, Test: 0.7376\n",
      "Epoch: 060, Loss: 0.5983, Val: 0.7546, Test: 0.7383\n",
      "Epoch: 061, Loss: 0.5973, Val: 0.7534, Test: 0.7362\n",
      "Epoch: 062, Loss: 0.5967, Val: 0.7524, Test: 0.7357\n",
      "Epoch: 063, Loss: 0.5983, Val: 0.7528, Test: 0.7359\n",
      "Epoch: 064, Loss: 0.5984, Val: 0.7514, Test: 0.7349\n",
      "Epoch: 065, Loss: 0.6021, Val: 0.7502, Test: 0.7339\n",
      "Epoch: 066, Loss: 0.5959, Val: 0.7530, Test: 0.7361\n",
      "Epoch: 067, Loss: 0.5956, Val: 0.7529, Test: 0.7360\n",
      "Epoch: 068, Loss: 0.5951, Val: 0.7499, Test: 0.7335\n",
      "Epoch: 069, Loss: 0.6007, Val: 0.7484, Test: 0.7325\n",
      "Epoch: 070, Loss: 0.6032, Val: 0.7496, Test: 0.7334\n",
      "Epoch: 071, Loss: 0.5987, Val: 0.7513, Test: 0.7349\n",
      "Epoch: 072, Loss: 0.5915, Val: 0.7522, Test: 0.7355\n",
      "Epoch: 073, Loss: 0.6035, Val: 0.7461, Test: 0.7312\n",
      "Epoch: 074, Loss: 0.6034, Val: 0.7478, Test: 0.7320\n",
      "Epoch: 075, Loss: 0.6028, Val: 0.7516, Test: 0.7348\n",
      "Epoch: 076, Loss: 0.5984, Val: 0.7499, Test: 0.7336\n",
      "Epoch: 077, Loss: 0.5924, Val: 0.7488, Test: 0.7330\n",
      "Epoch: 078, Loss: 0.5909, Val: 0.7511, Test: 0.7345\n",
      "Epoch: 079, Loss: 0.5995, Val: 0.7481, Test: 0.7325\n",
      "Epoch: 080, Loss: 0.5948, Val: 0.7491, Test: 0.7331\n",
      "Epoch: 081, Loss: 0.5978, Val: 0.7497, Test: 0.7334\n",
      "Epoch: 082, Loss: 0.5958, Val: 0.7495, Test: 0.7333\n",
      "Epoch: 083, Loss: 0.6003, Val: 0.7482, Test: 0.7326\n",
      "Epoch: 084, Loss: 0.5914, Val: 0.7509, Test: 0.7343\n",
      "Epoch: 085, Loss: 0.6010, Val: 0.7472, Test: 0.7317\n",
      "Epoch: 086, Loss: 0.5918, Val: 0.7493, Test: 0.7331\n",
      "Epoch: 087, Loss: 0.5970, Val: 0.7500, Test: 0.7336\n",
      "Epoch: 088, Loss: 0.6014, Val: 0.7469, Test: 0.7315\n",
      "Epoch: 089, Loss: 0.5973, Val: 0.7492, Test: 0.7330\n",
      "Epoch: 090, Loss: 0.6049, Val: 0.7474, Test: 0.7318\n",
      "Epoch: 091, Loss: 0.6010, Val: 0.7474, Test: 0.7319\n",
      "Epoch: 092, Loss: 0.6019, Val: 0.7491, Test: 0.7329\n",
      "Epoch: 093, Loss: 0.5981, Val: 0.7498, Test: 0.7334\n",
      "Epoch: 094, Loss: 0.5965, Val: 0.7486, Test: 0.7328\n",
      "Epoch: 095, Loss: 0.5989, Val: 0.7470, Test: 0.7317\n",
      "Epoch: 096, Loss: 0.6064, Val: 0.7469, Test: 0.7316\n",
      "Epoch: 097, Loss: 0.5939, Val: 0.7527, Test: 0.7356\n",
      "Epoch: 098, Loss: 0.6036, Val: 0.7470, Test: 0.7317\n",
      "Epoch: 099, Loss: 0.5990, Val: 0.7466, Test: 0.7312\n",
      "Epoch: 100, Loss: 0.6001, Val: 0.7519, Test: 0.7352\n",
      "Final Test: 0.7771\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(5, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5545)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "\n",
    "z = model.encode(data.x, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / 230\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6694, Val: 0.8427, Test: 0.8247\n",
      "Epoch: 002, Loss: 0.6670, Val: 0.8461, Test: 0.8198\n",
      "Epoch: 003, Loss: 0.6483, Val: 0.8730, Test: 0.8557\n",
      "Epoch: 004, Loss: 0.6304, Val: 0.8706, Test: 0.8652\n",
      "Epoch: 005, Loss: 0.6054, Val: 0.8490, Test: 0.8508\n",
      "Epoch: 006, Loss: 0.5772, Val: 0.8428, Test: 0.8379\n",
      "Epoch: 007, Loss: 0.5562, Val: 0.8371, Test: 0.8408\n",
      "Epoch: 008, Loss: 0.5409, Val: 0.8396, Test: 0.8331\n",
      "Epoch: 009, Loss: 0.5562, Val: 0.8187, Test: 0.8304\n",
      "Epoch: 010, Loss: 0.5673, Val: 0.8372, Test: 0.8344\n",
      "Epoch: 011, Loss: 0.5494, Val: 0.8351, Test: 0.8300\n",
      "Epoch: 012, Loss: 0.5546, Val: 0.8293, Test: 0.8346\n",
      "Epoch: 013, Loss: 0.5419, Val: 0.8241, Test: 0.8326\n",
      "Epoch: 014, Loss: 0.5455, Val: 0.8312, Test: 0.8298\n",
      "Epoch: 015, Loss: 0.5385, Val: 0.8334, Test: 0.8291\n",
      "Epoch: 016, Loss: 0.5485, Val: 0.8347, Test: 0.8319\n",
      "Epoch: 017, Loss: 0.5405, Val: 0.8370, Test: 0.8389\n",
      "Epoch: 018, Loss: 0.5395, Val: 0.8387, Test: 0.8411\n",
      "Epoch: 019, Loss: 0.5379, Val: 0.8420, Test: 0.8413\n",
      "Epoch: 020, Loss: 0.5321, Val: 0.8434, Test: 0.8402\n",
      "Epoch: 021, Loss: 0.5367, Val: 0.8419, Test: 0.8406\n",
      "Epoch: 022, Loss: 0.5285, Val: 0.8393, Test: 0.8406\n",
      "Epoch: 023, Loss: 0.5305, Val: 0.8394, Test: 0.8396\n",
      "Epoch: 024, Loss: 0.5324, Val: 0.8394, Test: 0.8375\n",
      "Epoch: 025, Loss: 0.5272, Val: 0.8380, Test: 0.8361\n",
      "Epoch: 026, Loss: 0.5289, Val: 0.8369, Test: 0.8372\n",
      "Epoch: 027, Loss: 0.5339, Val: 0.8362, Test: 0.8372\n",
      "Epoch: 028, Loss: 0.5343, Val: 0.8365, Test: 0.8359\n",
      "Epoch: 029, Loss: 0.5307, Val: 0.8370, Test: 0.8365\n",
      "Epoch: 030, Loss: 0.5255, Val: 0.8385, Test: 0.8380\n",
      "Epoch: 031, Loss: 0.5264, Val: 0.8394, Test: 0.8382\n",
      "Epoch: 032, Loss: 0.5333, Val: 0.8406, Test: 0.8386\n",
      "Epoch: 033, Loss: 0.5338, Val: 0.8401, Test: 0.8414\n",
      "Epoch: 034, Loss: 0.5304, Val: 0.8378, Test: 0.8414\n",
      "Epoch: 035, Loss: 0.5370, Val: 0.8396, Test: 0.8403\n",
      "Epoch: 036, Loss: 0.5270, Val: 0.8402, Test: 0.8384\n",
      "Epoch: 037, Loss: 0.5256, Val: 0.8398, Test: 0.8391\n",
      "Epoch: 038, Loss: 0.5278, Val: 0.8387, Test: 0.8402\n",
      "Epoch: 039, Loss: 0.5293, Val: 0.8394, Test: 0.8409\n",
      "Epoch: 040, Loss: 0.5255, Val: 0.8407, Test: 0.8412\n",
      "Epoch: 041, Loss: 0.5266, Val: 0.8415, Test: 0.8411\n",
      "Epoch: 042, Loss: 0.5260, Val: 0.8415, Test: 0.8435\n",
      "Epoch: 043, Loss: 0.5266, Val: 0.8409, Test: 0.8433\n",
      "Epoch: 044, Loss: 0.5165, Val: 0.8407, Test: 0.8423\n",
      "Epoch: 045, Loss: 0.5242, Val: 0.8406, Test: 0.8405\n",
      "Epoch: 046, Loss: 0.5349, Val: 0.8396, Test: 0.8413\n",
      "Epoch: 047, Loss: 0.5236, Val: 0.8394, Test: 0.8411\n",
      "Epoch: 048, Loss: 0.5253, Val: 0.8397, Test: 0.8420\n",
      "Epoch: 049, Loss: 0.5207, Val: 0.8413, Test: 0.8439\n",
      "Epoch: 050, Loss: 0.5283, Val: 0.8423, Test: 0.8449\n",
      "Epoch: 051, Loss: 0.5120, Val: 0.8430, Test: 0.8454\n",
      "Epoch: 052, Loss: 0.5232, Val: 0.8420, Test: 0.8448\n",
      "Epoch: 053, Loss: 0.5241, Val: 0.8398, Test: 0.8429\n",
      "Epoch: 054, Loss: 0.5202, Val: 0.8399, Test: 0.8421\n",
      "Epoch: 055, Loss: 0.5231, Val: 0.8401, Test: 0.8418\n",
      "Epoch: 056, Loss: 0.5229, Val: 0.8410, Test: 0.8437\n",
      "Epoch: 057, Loss: 0.5227, Val: 0.8418, Test: 0.8455\n",
      "Epoch: 058, Loss: 0.5235, Val: 0.8434, Test: 0.8457\n",
      "Epoch: 059, Loss: 0.5211, Val: 0.8426, Test: 0.8438\n",
      "Epoch: 060, Loss: 0.5212, Val: 0.8387, Test: 0.8423\n",
      "Epoch: 061, Loss: 0.5222, Val: 0.8366, Test: 0.8415\n",
      "Epoch: 062, Loss: 0.5236, Val: 0.8398, Test: 0.8419\n",
      "Epoch: 063, Loss: 0.5172, Val: 0.8426, Test: 0.8440\n",
      "Epoch: 064, Loss: 0.5229, Val: 0.8425, Test: 0.8459\n",
      "Epoch: 065, Loss: 0.5181, Val: 0.8393, Test: 0.8442\n",
      "Epoch: 066, Loss: 0.5192, Val: 0.8365, Test: 0.8404\n",
      "Epoch: 067, Loss: 0.5242, Val: 0.8374, Test: 0.8375\n",
      "Epoch: 068, Loss: 0.5144, Val: 0.8413, Test: 0.8424\n",
      "Epoch: 069, Loss: 0.5176, Val: 0.8424, Test: 0.8468\n",
      "Epoch: 070, Loss: 0.5127, Val: 0.8403, Test: 0.8447\n",
      "Epoch: 071, Loss: 0.5162, Val: 0.8377, Test: 0.8389\n",
      "Epoch: 072, Loss: 0.5206, Val: 0.8352, Test: 0.8350\n",
      "Epoch: 073, Loss: 0.5100, Val: 0.8369, Test: 0.8406\n",
      "Epoch: 074, Loss: 0.5168, Val: 0.8416, Test: 0.8451\n",
      "Epoch: 075, Loss: 0.5227, Val: 0.8432, Test: 0.8438\n",
      "Epoch: 076, Loss: 0.5147, Val: 0.8370, Test: 0.8375\n",
      "Epoch: 077, Loss: 0.5135, Val: 0.8340, Test: 0.8357\n",
      "Epoch: 078, Loss: 0.5137, Val: 0.8375, Test: 0.8403\n",
      "Epoch: 079, Loss: 0.5244, Val: 0.8419, Test: 0.8423\n",
      "Epoch: 080, Loss: 0.5147, Val: 0.8405, Test: 0.8402\n",
      "Epoch: 081, Loss: 0.5160, Val: 0.8349, Test: 0.8369\n",
      "Epoch: 082, Loss: 0.5171, Val: 0.8325, Test: 0.8349\n",
      "Epoch: 083, Loss: 0.5213, Val: 0.8350, Test: 0.8360\n",
      "Epoch: 084, Loss: 0.5139, Val: 0.8397, Test: 0.8391\n",
      "Epoch: 085, Loss: 0.5176, Val: 0.8366, Test: 0.8368\n",
      "Epoch: 086, Loss: 0.5097, Val: 0.8338, Test: 0.8341\n",
      "Epoch: 087, Loss: 0.5115, Val: 0.8325, Test: 0.8341\n",
      "Epoch: 088, Loss: 0.5097, Val: 0.8360, Test: 0.8372\n",
      "Epoch: 089, Loss: 0.5088, Val: 0.8430, Test: 0.8408\n",
      "Epoch: 090, Loss: 0.5093, Val: 0.8414, Test: 0.8392\n",
      "Epoch: 091, Loss: 0.5165, Val: 0.8356, Test: 0.8356\n",
      "Epoch: 092, Loss: 0.5064, Val: 0.8360, Test: 0.8366\n",
      "Epoch: 093, Loss: 0.5091, Val: 0.8383, Test: 0.8386\n",
      "Epoch: 094, Loss: 0.5099, Val: 0.8413, Test: 0.8386\n",
      "Epoch: 095, Loss: 0.5095, Val: 0.8361, Test: 0.8346\n",
      "Epoch: 096, Loss: 0.5176, Val: 0.8313, Test: 0.8330\n",
      "Epoch: 097, Loss: 0.5161, Val: 0.8328, Test: 0.8336\n",
      "Epoch: 098, Loss: 0.5106, Val: 0.8366, Test: 0.8357\n",
      "Epoch: 099, Loss: 0.5068, Val: 0.8365, Test: 0.8345\n",
      "Epoch: 100, Loss: 0.5118, Val: 0.8319, Test: 0.8317\n",
      "Final Test: 0.8557\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_ase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_ase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8357)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / 230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLASE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.8382, Val: 0.6897, Test: 0.7031\n",
      "Epoch: 002, Loss: 0.6994, Val: 0.7442, Test: 0.7525\n",
      "Epoch: 003, Loss: 0.6994, Val: 0.7943, Test: 0.7836\n",
      "Epoch: 004, Loss: 0.6782, Val: 0.8038, Test: 0.7913\n",
      "Epoch: 005, Loss: 0.6596, Val: 0.8057, Test: 0.7909\n",
      "Epoch: 006, Loss: 0.6519, Val: 0.7889, Test: 0.7664\n",
      "Epoch: 007, Loss: 0.6490, Val: 0.7955, Test: 0.7749\n",
      "Epoch: 008, Loss: 0.6417, Val: 0.8108, Test: 0.7998\n",
      "Epoch: 009, Loss: 0.6352, Val: 0.8050, Test: 0.7956\n",
      "Epoch: 010, Loss: 0.6222, Val: 0.7920, Test: 0.7842\n",
      "Epoch: 011, Loss: 0.6096, Val: 0.7921, Test: 0.7866\n",
      "Epoch: 012, Loss: 0.6048, Val: 0.8015, Test: 0.7905\n",
      "Epoch: 013, Loss: 0.5982, Val: 0.8124, Test: 0.8027\n",
      "Epoch: 014, Loss: 0.5842, Val: 0.8129, Test: 0.8052\n",
      "Epoch: 015, Loss: 0.5827, Val: 0.8105, Test: 0.8022\n",
      "Epoch: 016, Loss: 0.5685, Val: 0.8095, Test: 0.8089\n",
      "Epoch: 017, Loss: 0.5680, Val: 0.8197, Test: 0.8168\n",
      "Epoch: 018, Loss: 0.5601, Val: 0.8284, Test: 0.8236\n",
      "Epoch: 019, Loss: 0.5519, Val: 0.8314, Test: 0.8238\n",
      "Epoch: 020, Loss: 0.5501, Val: 0.8363, Test: 0.8329\n",
      "Epoch: 021, Loss: 0.5432, Val: 0.8355, Test: 0.8381\n",
      "Epoch: 022, Loss: 0.5427, Val: 0.8339, Test: 0.8378\n",
      "Epoch: 023, Loss: 0.5417, Val: 0.8371, Test: 0.8367\n",
      "Epoch: 024, Loss: 0.5428, Val: 0.8355, Test: 0.8318\n",
      "Epoch: 025, Loss: 0.5345, Val: 0.8341, Test: 0.8315\n",
      "Epoch: 026, Loss: 0.5386, Val: 0.8324, Test: 0.8325\n",
      "Epoch: 027, Loss: 0.5446, Val: 0.8298, Test: 0.8336\n",
      "Epoch: 028, Loss: 0.5400, Val: 0.8325, Test: 0.8305\n",
      "Epoch: 029, Loss: 0.5448, Val: 0.8318, Test: 0.8285\n",
      "Epoch: 030, Loss: 0.5431, Val: 0.8326, Test: 0.8296\n",
      "Epoch: 031, Loss: 0.5399, Val: 0.8297, Test: 0.8351\n",
      "Epoch: 032, Loss: 0.5385, Val: 0.8306, Test: 0.8361\n",
      "Epoch: 033, Loss: 0.5415, Val: 0.8351, Test: 0.8334\n",
      "Epoch: 034, Loss: 0.5325, Val: 0.8363, Test: 0.8329\n",
      "Epoch: 035, Loss: 0.5383, Val: 0.8367, Test: 0.8360\n",
      "Epoch: 036, Loss: 0.5328, Val: 0.8359, Test: 0.8379\n",
      "Epoch: 037, Loss: 0.5344, Val: 0.8351, Test: 0.8382\n",
      "Epoch: 038, Loss: 0.5347, Val: 0.8365, Test: 0.8379\n",
      "Epoch: 039, Loss: 0.5360, Val: 0.8374, Test: 0.8368\n",
      "Epoch: 040, Loss: 0.5364, Val: 0.8377, Test: 0.8368\n",
      "Epoch: 041, Loss: 0.5357, Val: 0.8379, Test: 0.8375\n",
      "Epoch: 042, Loss: 0.5351, Val: 0.8378, Test: 0.8367\n",
      "Epoch: 043, Loss: 0.5306, Val: 0.8374, Test: 0.8367\n",
      "Epoch: 044, Loss: 0.5339, Val: 0.8374, Test: 0.8355\n",
      "Epoch: 045, Loss: 0.5320, Val: 0.8373, Test: 0.8356\n",
      "Epoch: 046, Loss: 0.5332, Val: 0.8344, Test: 0.8385\n",
      "Epoch: 047, Loss: 0.5310, Val: 0.8351, Test: 0.8381\n",
      "Epoch: 048, Loss: 0.5361, Val: 0.8373, Test: 0.8347\n",
      "Epoch: 049, Loss: 0.5226, Val: 0.8377, Test: 0.8351\n",
      "Epoch: 050, Loss: 0.5310, Val: 0.8355, Test: 0.8385\n",
      "Epoch: 051, Loss: 0.5310, Val: 0.8345, Test: 0.8385\n",
      "Epoch: 052, Loss: 0.5344, Val: 0.8367, Test: 0.8369\n",
      "Epoch: 053, Loss: 0.5405, Val: 0.8370, Test: 0.8361\n",
      "Epoch: 054, Loss: 0.5395, Val: 0.8372, Test: 0.8380\n",
      "Epoch: 055, Loss: 0.5318, Val: 0.8368, Test: 0.8400\n",
      "Epoch: 056, Loss: 0.5276, Val: 0.8392, Test: 0.8398\n",
      "Epoch: 057, Loss: 0.5271, Val: 0.8397, Test: 0.8393\n",
      "Epoch: 058, Loss: 0.5280, Val: 0.8398, Test: 0.8394\n",
      "Epoch: 059, Loss: 0.5278, Val: 0.8388, Test: 0.8412\n",
      "Epoch: 060, Loss: 0.5306, Val: 0.8370, Test: 0.8409\n",
      "Epoch: 061, Loss: 0.5394, Val: 0.8381, Test: 0.8396\n",
      "Epoch: 062, Loss: 0.5346, Val: 0.8382, Test: 0.8374\n",
      "Epoch: 063, Loss: 0.5301, Val: 0.8385, Test: 0.8390\n",
      "Epoch: 064, Loss: 0.5259, Val: 0.8385, Test: 0.8406\n",
      "Epoch: 065, Loss: 0.5335, Val: 0.8381, Test: 0.8417\n",
      "Epoch: 066, Loss: 0.5341, Val: 0.8371, Test: 0.8415\n",
      "Epoch: 067, Loss: 0.5259, Val: 0.8381, Test: 0.8406\n",
      "Epoch: 068, Loss: 0.5257, Val: 0.8388, Test: 0.8376\n",
      "Epoch: 069, Loss: 0.5279, Val: 0.8390, Test: 0.8389\n",
      "Epoch: 070, Loss: 0.5284, Val: 0.8385, Test: 0.8413\n",
      "Epoch: 071, Loss: 0.5318, Val: 0.8383, Test: 0.8422\n",
      "Epoch: 072, Loss: 0.5247, Val: 0.8399, Test: 0.8421\n",
      "Epoch: 073, Loss: 0.5313, Val: 0.8406, Test: 0.8415\n",
      "Epoch: 074, Loss: 0.5235, Val: 0.8403, Test: 0.8419\n",
      "Epoch: 075, Loss: 0.5249, Val: 0.8392, Test: 0.8427\n",
      "Epoch: 076, Loss: 0.5233, Val: 0.8387, Test: 0.8429\n",
      "Epoch: 077, Loss: 0.5222, Val: 0.8398, Test: 0.8422\n",
      "Epoch: 078, Loss: 0.5194, Val: 0.8402, Test: 0.8422\n",
      "Epoch: 079, Loss: 0.5312, Val: 0.8387, Test: 0.8421\n",
      "Epoch: 080, Loss: 0.5297, Val: 0.8390, Test: 0.8425\n",
      "Epoch: 081, Loss: 0.5162, Val: 0.8415, Test: 0.8437\n",
      "Epoch: 082, Loss: 0.5233, Val: 0.8410, Test: 0.8448\n",
      "Epoch: 083, Loss: 0.5206, Val: 0.8397, Test: 0.8447\n",
      "Epoch: 084, Loss: 0.5190, Val: 0.8393, Test: 0.8440\n",
      "Epoch: 085, Loss: 0.5182, Val: 0.8391, Test: 0.8431\n",
      "Epoch: 086, Loss: 0.5206, Val: 0.8409, Test: 0.8417\n",
      "Epoch: 087, Loss: 0.5226, Val: 0.8412, Test: 0.8432\n",
      "Epoch: 088, Loss: 0.5231, Val: 0.8409, Test: 0.8451\n",
      "Epoch: 089, Loss: 0.5222, Val: 0.8409, Test: 0.8456\n",
      "Epoch: 090, Loss: 0.5214, Val: 0.8424, Test: 0.8450\n",
      "Epoch: 091, Loss: 0.5207, Val: 0.8412, Test: 0.8441\n",
      "Epoch: 092, Loss: 0.5197, Val: 0.8385, Test: 0.8429\n",
      "Epoch: 093, Loss: 0.5219, Val: 0.8369, Test: 0.8417\n",
      "Epoch: 094, Loss: 0.5205, Val: 0.8375, Test: 0.8417\n",
      "Epoch: 095, Loss: 0.5228, Val: 0.8409, Test: 0.8431\n",
      "Epoch: 096, Loss: 0.5131, Val: 0.8436, Test: 0.8459\n",
      "Epoch: 097, Loss: 0.5175, Val: 0.8434, Test: 0.8471\n",
      "Epoch: 098, Loss: 0.5179, Val: 0.8407, Test: 0.8447\n",
      "Epoch: 099, Loss: 0.5172, Val: 0.8381, Test: 0.8412\n",
      "Epoch: 100, Loss: 0.5165, Val: 0.8374, Test: 0.8400\n",
      "Final Test: 0.8459\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(9, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_glase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_glase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8571)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "(adj_matrix[senadores_no_presentes][:,n_P1+n_P2:]==predicted_adj[senadores_no_presentes][:,n_P1+n_P2:]).sum() / (n_P1_np+n_P2_np) / 230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pycountry_convert as pc\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from cycler import cycler\n",
    "\n",
    "resolutions_issues = {'me': 'Palestinian conflict', \n",
    "                      'nu': 'Nuclear weapons and nuclear material', \n",
    "                      'di': 'Arms control and disarmament',\n",
    "                      'co': 'Colonialism',\n",
    "                      'hr': 'Human rights',\n",
    "                      'ec': 'Economic Development',\n",
    "                      'N/A': 'Not specified'}\n",
    "\n",
    "resolutions_issues_color = {'me': 'salmon', \n",
    "                            'nu': 'yellow', \n",
    "                            'di': 'teal',\n",
    "                            'co': 'orchid',\n",
    "                            'hr': 'navy',\n",
    "                            'ec': 'orange',\n",
    "                            'N/A': 'black'}\n",
    "\n",
    "continents_colors = {'North America': 'yellow',\n",
    "                     'South America': 'forestgreen',\n",
    "                     'Europe': 'royalblue',\n",
    "                     'Africa': 'plum',\n",
    "                     'Asia': 'darkorange',\n",
    "                     'Oceania': 'firebrick'}\n",
    "\n",
    "cycler_colors = ['royalblue','firebrick','forestgreen','olive']\n",
    "\n",
    "\n",
    "def load_un_dataset(un_data_path, initial_year=1946, final_year=2018, remove_nonmembers=True, remove_nonpresent=False, unknown_votes=False):\n",
    "    \n",
    "    if os.path.isdir(os.path.dirname(un_data_path)):\n",
    "        if not os.path.exists(un_data_path):\n",
    "            download_un_dataset(un_data_path)\n",
    "    else:\n",
    "        raise Exception(\"Provided path for UN dataset is not reachable\")\n",
    "    \n",
    "    # Load data    \n",
    "    votes_df = pd.read_csv(un_data_path, low_memory=False, encoding='latin-1', index_col=0)\n",
    "    # Keep only desired years\n",
    "    votes_df = votes_df[votes_df.year>=initial_year]\n",
    "    votes_df = votes_df[votes_df.year<=final_year]\n",
    "    \n",
    "    if remove_nonmembers:\n",
    "        # Remove votes by nonmembers\n",
    "        votes_df = votes_df[votes_df.vote!=9]\n",
    "    \n",
    "    if remove_nonpresent:\n",
    "        # Remove votes by nonmembers\n",
    "        votes_df = votes_df[votes_df.vote!=8]\n",
    "        \n",
    "    # Edges in graph represent an affirmative vote\n",
    "    votes_df['weight'] = (votes_df.vote==1)\n",
    "    \n",
    "    if unknown_votes:\n",
    "        # Voters preference is assumed unknown if it is an abstention or voter is not present\n",
    "        votes_df['unknown'] =  (votes_df.vote==2) | (votes_df.vote==8)\n",
    "    \n",
    "    votes_df['res_features'] = votes_df[['me','nu','di','co','hr','ec']].apply(lambda row: np.array(row), axis=1)\n",
    "\n",
    "    return votes_df\n",
    "\n",
    "        \n",
    "def download_un_dataset(filename='UNVotes-1.csv', data_url='https://dataverse.harvard.edu/api/access/datafile/6358426'):\n",
    "    # Code from https://gist.github.com/yanqd0/c13ed29e29432e3cf3e7c38467f42f51\n",
    "    response = requests.get(data_url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length',0))\n",
    "    with open(filename, \"wb\") as f, tqdm(desc='Downloading UN dataset', total=total_size, unit='B', unit_divisor=1024, unit_scale=True) as pbar:\n",
    "        for un_data in response.iter_content(chunk_size=1024):\n",
    "            size = f.write(un_data)\n",
    "            pbar.update(size)\n",
    "            \n",
    "def get_continents_dict(votes_df):\n",
    "    continents_dict = {}\n",
    "    countries = votes_df.Country.unique()\n",
    "    for country in countries: \n",
    "        try:\n",
    "            continent_code = pc.country_alpha2_to_continent_code(pc.country_alpha3_to_country_alpha2(country))\n",
    "            continents_dict[country] = pc.convert_continent_code_to_continent_name(continent_code)\n",
    "        except:\n",
    "            continue\n",
    "            # print(pais)\n",
    "            \n",
    "    continents_dict['DDR'] = 'Europe'\n",
    "    continents_dict['CSK'] = 'Europe'\n",
    "    continents_dict['YUG'] = 'Europe'\n",
    "    continents_dict['EAZ'] = 'Africa'\n",
    "    continents_dict['YAR'] = 'Asia'\n",
    "    continents_dict['TLS'] = 'Asia'\n",
    "    \n",
    "    return continents_dict\n",
    "\n",
    "def get_countries_name_conversion_dict(votes_df):\n",
    "    countries = votes_df.Countryname.unique()\n",
    "    conversion_dict = {}\n",
    "    for country in countries: \n",
    "        conversion_dict[country] = votes_df[votes_df.Countryname==country].Country.unique()[0]\n",
    "        \n",
    "    return conversion_dict\n",
    "\n",
    "def create_un_graphs(votes_df):\n",
    "    \n",
    "    continents_dict = get_continents_dict(votes_df)\n",
    "    conversion_dict = get_countries_name_conversion_dict(votes_df)\n",
    "    \n",
    "    all_graphs = {}\n",
    "    \n",
    "    initial_year = votes_df.year.min()\n",
    "    final_year = votes_df.year.max()\n",
    "    \n",
    "    edge_attr = ['weight', 'unknown'] if 'unknown' in votes_df.columns else 'weight'    \n",
    "    \n",
    "    for year in range(initial_year,final_year+1):\n",
    "        \n",
    "        votes_df_year= votes_df[votes_df.year==year]\n",
    "        \n",
    "        g = nx.from_pandas_edgelist(votes_df_year,source='Countryname',target='resid',edge_attr=edge_attr,create_using=nx.DiGraph())\n",
    "        if g.number_of_edges()>0:\n",
    "            \n",
    "            countries_list = votes_df_year.Countryname.unique()\n",
    "            \n",
    "            # Add country's code and continent as graph attributes\n",
    "            countries_codes = {}\n",
    "            countries_continents = {}\n",
    "            nodes_colors = {}\n",
    "            node_types = {}\n",
    "            for country in countries_list:\n",
    "                countries_codes[country] = conversion_dict[country]\n",
    "                countries_continents[country] = continents_dict[conversion_dict[country]]\n",
    "                nodes_colors[country] = continents_colors[countries_continents[country]]\n",
    "                node_types[country] = \"country\"\n",
    "                \n",
    "            nx.set_node_attributes(g, countries_codes, name='country code')\n",
    "            nx.set_node_attributes(g, countries_continents, name='continent')\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Add resolution's issue as graph attribute\n",
    "            resolutions_list = votes_df_year.resid.unique()\n",
    "            resolutions_issues_dict = {}\n",
    "            important_resolutions_dict = {}\n",
    "            resolutions_features = {}\n",
    "\n",
    "            \n",
    "            for resolution_id in resolutions_list:\n",
    "                df_votes_sum = votes_df_year[votes_df_year.resid==resolution_id][['me','nu','di','co','hr','ec']].sum()\n",
    "                if df_votes_sum.max()>0:\n",
    "                    resolutions_issues_dict[resolution_id] = df_votes_sum.idxmax()\n",
    "                else:\n",
    "                    resolutions_issues_dict[resolution_id] = 'N/A'\n",
    "                    \n",
    "                nodes_colors[resolution_id] = resolutions_issues_color[resolutions_issues_dict[resolution_id]]\n",
    "                resolutions_features[resolution_id] = votes_df_year[votes_df_year.resid==resolution_id]['res_features'].mean()\n",
    "                node_types[resolution_id] = \"resolution\"\n",
    "                \n",
    "                important_vote = votes_df_year[votes_df_year.resid==resolution_id]['importantvote'].max()\n",
    "                if important_vote > 0:\n",
    "                    important_resolutions_dict[resolution_id] = True\n",
    "                else:\n",
    "                    important_resolutions_dict[resolution_id] = False\n",
    "            \n",
    "            nx.set_node_attributes(g, resolutions_issues_dict,name='issue code')\n",
    "            nx.set_node_attributes(g, nodes_colors, name='color')\n",
    "            nx.set_node_attributes(g, important_resolutions_dict, name='important vote')\n",
    "            nx.set_node_attributes(g, node_types, name='type') \n",
    "            nx.set_node_attributes(g, resolutions_features, name='res_features') \n",
    "                \n",
    "            all_graphs[year] = g\n",
    "            \n",
    "    return all_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rcid</th>\n",
       "      <th>ccode</th>\n",
       "      <th>member</th>\n",
       "      <th>vote</th>\n",
       "      <th>Country</th>\n",
       "      <th>Countryname</th>\n",
       "      <th>year</th>\n",
       "      <th>session</th>\n",
       "      <th>abstain</th>\n",
       "      <th>yes</th>\n",
       "      <th>...</th>\n",
       "      <th>nu</th>\n",
       "      <th>di</th>\n",
       "      <th>hr</th>\n",
       "      <th>co</th>\n",
       "      <th>ec</th>\n",
       "      <th>ident</th>\n",
       "      <th>resid</th>\n",
       "      <th>weight</th>\n",
       "      <th>unknown</th>\n",
       "      <th>res_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>USA</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>CUB</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>HTI</td>\n",
       "      <td>Haiti</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DOM</td>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>1946</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199018</th>\n",
       "      <td>6102</td>\n",
       "      <td>552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2018</td>\n",
       "      <td>73</td>\n",
       "      <td>12.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73090</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199019</th>\n",
       "      <td>6138</td>\n",
       "      <td>552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2018</td>\n",
       "      <td>73</td>\n",
       "      <td>33.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73091</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199020</th>\n",
       "      <td>6139</td>\n",
       "      <td>552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2018</td>\n",
       "      <td>73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73092</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199021</th>\n",
       "      <td>6097</td>\n",
       "      <td>552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2018</td>\n",
       "      <td>73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73093</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199022</th>\n",
       "      <td>6098</td>\n",
       "      <td>552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2018</td>\n",
       "      <td>73</td>\n",
       "      <td>29.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73094</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>943616 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rcid  ccode  member  vote Country               Countryname  year  \\\n",
       "1           3      2     1.0     1     USA  United States of America  1946   \n",
       "2           3     20     1.0     3     CAN                    Canada  1946   \n",
       "4           3     40     1.0     1     CUB                      Cuba  1946   \n",
       "5           3     41     1.0     1     HTI                     Haiti  1946   \n",
       "6           3     42     1.0     1     DOM        Dominican Republic  1946   \n",
       "...       ...    ...     ...   ...     ...                       ...   ...   \n",
       "1199018  6102    552     1.0     1     ZWE                  Zimbabwe  2018   \n",
       "1199019  6138    552     1.0     1     ZWE                  Zimbabwe  2018   \n",
       "1199020  6139    552     1.0     1     ZWE                  Zimbabwe  2018   \n",
       "1199021  6097    552     1.0     1     ZWE                  Zimbabwe  2018   \n",
       "1199022  6098    552     1.0     2     ZWE                  Zimbabwe  2018   \n",
       "\n",
       "         session  abstain    yes  ...  nu  di hr co  ec  ident  resid weight  \\\n",
       "1              1      4.0   29.0  ...   0   0  0  0   0    0.0   1001   True   \n",
       "2              1      4.0   29.0  ...   0   0  0  0   0    0.0   1001  False   \n",
       "4              1      4.0   29.0  ...   0   0  0  0   0    0.0   1001   True   \n",
       "5              1      4.0   29.0  ...   0   0  0  0   0    0.0   1001   True   \n",
       "6              1      4.0   29.0  ...   0   0  0  0   0    0.0   1001   True   \n",
       "...          ...      ...    ...  ...  ..  .. .. ..  ..    ...    ...    ...   \n",
       "1199018       73     12.0  156.0  ...   0   0  0  0   0    0.0  73090   True   \n",
       "1199019       73     33.0   94.0  ...   0   0  0  0   0    0.0  73091   True   \n",
       "1199020       73      0.0  188.0  ...   0   0  0  0   1    0.0  73092   True   \n",
       "1199021       73      1.0  180.0  ...   0   0  0  0   0    0.0  73093   True   \n",
       "1199022       73     29.0  151.0  ...   0   0  0  0   1    0.0  73094  False   \n",
       "\n",
       "         unknown        res_features  \n",
       "1          False  [0, 0, 0, 0, 0, 0]  \n",
       "2          False  [0, 0, 0, 0, 0, 0]  \n",
       "4          False  [0, 0, 0, 0, 0, 0]  \n",
       "5          False  [0, 0, 0, 0, 0, 0]  \n",
       "6          False  [0, 0, 0, 0, 0, 0]  \n",
       "...          ...                 ...  \n",
       "1199018    False  [0, 0, 0, 0, 0, 0]  \n",
       "1199019    False  [0, 0, 0, 0, 0, 0]  \n",
       "1199020    False  [0, 0, 0, 0, 0, 1]  \n",
       "1199021    False  [0, 0, 0, 0, 0, 0]  \n",
       "1199022     True  [0, 0, 0, 0, 0, 1]  \n",
       "\n",
       "[943616 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes_df = load_un_dataset('data/UNVotes-1.csv', unknown_votes=True)\n",
    "votes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rcid', 'ccode', 'member', 'vote', 'Country', 'Countryname', 'year',\n",
       "       'session', 'abstain', 'yes', 'no', 'importantvote', 'date', 'unres',\n",
       "       'amend', 'para', 'short', 'descr', 'me', 'nu', 'di', 'hr', 'co', 'ec',\n",
       "       'ident', 'resid', 'weight', 'unknown', 'res_features'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graphs = create_un_graphs(votes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = all_graphs[2018].to_undirected()\n",
    "# rename nodes\n",
    "mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "G_ = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "country_indexes = []\n",
    "res_indexes = []\n",
    "features = []\n",
    "\n",
    "for node, data in G_.nodes(data=True):\n",
    "    if data[\"type\"] == \"country\":\n",
    "        country_indexes.append(node)\n",
    "        features.append(np.ones(6))\n",
    "    else:\n",
    "        res_indexes.append(node)\n",
    "        features.append(data['res_features'])\n",
    "\n",
    "\n",
    "unknown_edges = []\n",
    "\n",
    "for u, v, data in G_.edges(data=True):\n",
    "    if data['unknown']:\n",
    "        unknown_edges.append((u,v))\n",
    "        unknown_edges.append((v,u))\n",
    "    \n",
    "\n",
    "\n",
    "adj_matrix = nx.adjacency_matrix(G).todense().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "num_nodes = adj_matrix.shape[0]\n",
    "\n",
    "edge_index = torch.tensor(adj_matrix).nonzero().t().contiguous()\n",
    "\n",
    "mask = torch.ones([num_nodes,num_nodes]).squeeze(0)\n",
    "\n",
    "random.seed(42)\n",
    "# not_present_countries = random.sample(country_indexes, 10)\n",
    "\n",
    "for i in country_indexes:\n",
    "    votos = (torch.rand(1, num_nodes) < 0.3).int()\n",
    "    mask[i,:] = votos\n",
    "    mask[:,i] = votos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.tensor(features)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1.]])\n",
      "Iteraciones:  100\n",
      "Loss:  tensor(16.3506)\n"
     ]
    }
   ],
   "source": [
    "## ASE \n",
    "\n",
    "d = 6\n",
    "adj_matrix = to_dense_adj(edge_index.to('cpu')).squeeze(0)\n",
    "ase = AdjacencySpectralEmbed(n_components=d, diag_aug=True, algorithm='full')\n",
    "masked_adj = adj_matrix*mask\n",
    "x_ase = ase.fit_transform(masked_adj.numpy())\n",
    "x_ase = torch.from_numpy(x_ase)\n",
    "\n",
    "A = to_dense_adj(edge_index.to('cpu'), max_num_nodes=num_nodes).squeeze(0)\n",
    "\n",
    "u, V = torch.linalg.eig(A)\n",
    "\n",
    "list_q=[]\n",
    "for i in range(d):\n",
    "    if u[i].numpy()>0:\n",
    "        list_q.append(1)\n",
    "    else:\n",
    "        list_q.append(-1)\n",
    "        \n",
    "# list_q.sort(reverse=True)\n",
    "q = torch.Tensor(list_q)\n",
    "Q=torch.diag(q)\n",
    "\n",
    "print(Q)\n",
    "\n",
    "\n",
    "torch.norm((x_ase@Q@x_ase.T - to_dense_adj(edge_index).squeeze(0))*mask)\n",
    "\n",
    "\n",
    "x_grdpg, cost, k  = GRDPG_GD_Armijo(x_ase, edge_index, Q, mask.nonzero().t().contiguous())\n",
    "x_grdpg = x_grdpg.detach()\n",
    "print(\"Iteraciones: \", k)\n",
    "print(\"Loss: \", torch.norm((x_grdpg@Q@x_grdpg.T - to_dense_adj(edge_index).squeeze(0))*to_dense_adj(mask.nonzero().t().contiguous()).squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(56.0560, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(23.0081, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(21.7985, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(26.5601, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(21.6407, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(21.0882, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(20.8992, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(20.5893, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(20.2220, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(20.0568, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gd_steps = 10\n",
    "lr = 1e-2\n",
    "device = 'cuda'\n",
    "model = gLASE(d,d, gd_steps)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "## Initialization\n",
    "for step in range(gd_steps):\n",
    "    model.gd[step].lin1.weight.data = (torch.eye(d,d)*lr).to(device)#torch.nn.init.xavier_uniform_(model.gd[step].lin1.weight)*lr\n",
    "    model.gd[step].lin2.weight.data = (torch.eye(d,d)*lr).to(device)#torch.nn.init.xavier_uniform_(model.gd[step].lin2.weight)*lr\n",
    "    \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Define ATT mask\n",
    "edge_index_2 = torch.ones([num_nodes,num_nodes],).nonzero().t().contiguous().to(device)\n",
    "mask = mask.to(device)\n",
    "x_ase = x_ase.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "Q = Q.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x_ase, edge_index, edge_index_2, Q, mask.nonzero().t().contiguous())\n",
    "    loss = torch.norm((out@Q@out.T - to_dense_adj(edge_index).squeeze(0))*mask)\n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "\n",
    "    if epoch % 100 ==0:\n",
    "        print(loss)\n",
    "\n",
    "\n",
    "x_glase = out.detach().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Train, Val, Test\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "masked_edge_index = masked_adj.nonzero().t().contiguous()\n",
    "\n",
    "data = Data(x=features.float(), x_ase=x_ase, x_glase=x_glase, edge_index=masked_edge_index)\n",
    "# torch.manual_seed(42)\n",
    "# random_features=torch.rand([288, 6])\n",
    "# random_features\n",
    "\n",
    "data = Data(x=features.float(), x_ase=x_ase, x_glase=x_glase, edge_index=masked_edge_index)\n",
    "\n",
    "\n",
    "transform = T.Compose([\n",
    "    # T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.1535, Val: 0.3751, Test: 0.3485\n",
      "Epoch: 002, Loss: 0.7430, Val: 0.3933, Test: 0.3717\n",
      "Epoch: 003, Loss: 0.7176, Val: 0.4130, Test: 0.3915\n",
      "Epoch: 004, Loss: 0.7347, Val: 0.4162, Test: 0.3878\n",
      "Epoch: 005, Loss: 0.7339, Val: 0.4079, Test: 0.3785\n",
      "Epoch: 006, Loss: 0.7240, Val: 0.4022, Test: 0.3710\n",
      "Epoch: 007, Loss: 0.7141, Val: 0.4022, Test: 0.3639\n",
      "Epoch: 008, Loss: 0.7069, Val: 0.4091, Test: 0.3573\n",
      "Epoch: 009, Loss: 0.7030, Val: 0.4162, Test: 0.3424\n",
      "Epoch: 010, Loss: 0.7000, Val: 0.4208, Test: 0.3247\n",
      "Epoch: 011, Loss: 0.6993, Val: 0.4334, Test: 0.3181\n",
      "Epoch: 012, Loss: 0.6969, Val: 0.4635, Test: 0.3412\n",
      "Epoch: 013, Loss: 0.6963, Val: 0.5278, Test: 0.3952\n",
      "Epoch: 014, Loss: 0.6954, Val: 0.6175, Test: 0.4778\n",
      "Epoch: 015, Loss: 0.6946, Val: 0.6795, Test: 0.5387\n",
      "Epoch: 016, Loss: 0.6940, Val: 0.7073, Test: 0.5778\n",
      "Epoch: 017, Loss: 0.6932, Val: 0.7202, Test: 0.6027\n",
      "Epoch: 018, Loss: 0.6920, Val: 0.7227, Test: 0.6140\n",
      "Epoch: 019, Loss: 0.6919, Val: 0.7218, Test: 0.6180\n",
      "Epoch: 020, Loss: 0.6922, Val: 0.7215, Test: 0.6183\n",
      "Epoch: 021, Loss: 0.6908, Val: 0.7220, Test: 0.6210\n",
      "Epoch: 022, Loss: 0.6912, Val: 0.7234, Test: 0.6218\n",
      "Epoch: 023, Loss: 0.6904, Val: 0.7245, Test: 0.6229\n",
      "Epoch: 024, Loss: 0.6895, Val: 0.7259, Test: 0.6236\n",
      "Epoch: 025, Loss: 0.6881, Val: 0.7287, Test: 0.6251\n",
      "Epoch: 026, Loss: 0.6887, Val: 0.7300, Test: 0.6250\n",
      "Epoch: 027, Loss: 0.6885, Val: 0.7309, Test: 0.6247\n",
      "Epoch: 028, Loss: 0.6869, Val: 0.7314, Test: 0.6248\n",
      "Epoch: 029, Loss: 0.6887, Val: 0.7332, Test: 0.6251\n",
      "Epoch: 030, Loss: 0.6879, Val: 0.7335, Test: 0.6248\n",
      "Epoch: 031, Loss: 0.6893, Val: 0.7365, Test: 0.6239\n",
      "Epoch: 032, Loss: 0.6878, Val: 0.7378, Test: 0.6236\n",
      "Epoch: 033, Loss: 0.6889, Val: 0.7362, Test: 0.6225\n",
      "Epoch: 034, Loss: 0.6873, Val: 0.7339, Test: 0.6210\n",
      "Epoch: 035, Loss: 0.6878, Val: 0.7335, Test: 0.6198\n",
      "Epoch: 036, Loss: 0.6870, Val: 0.7348, Test: 0.6196\n",
      "Epoch: 037, Loss: 0.6879, Val: 0.7328, Test: 0.6197\n",
      "Epoch: 038, Loss: 0.6870, Val: 0.7303, Test: 0.6213\n",
      "Epoch: 039, Loss: 0.6873, Val: 0.7305, Test: 0.6220\n",
      "Epoch: 040, Loss: 0.6879, Val: 0.7321, Test: 0.6216\n",
      "Epoch: 041, Loss: 0.6859, Val: 0.7316, Test: 0.6206\n",
      "Epoch: 042, Loss: 0.6874, Val: 0.7323, Test: 0.6198\n",
      "Epoch: 043, Loss: 0.6861, Val: 0.7335, Test: 0.6204\n",
      "Epoch: 044, Loss: 0.6883, Val: 0.7323, Test: 0.6198\n",
      "Epoch: 045, Loss: 0.6867, Val: 0.7323, Test: 0.6187\n",
      "Epoch: 046, Loss: 0.6869, Val: 0.7298, Test: 0.6164\n",
      "Epoch: 047, Loss: 0.6852, Val: 0.7312, Test: 0.6176\n",
      "Epoch: 048, Loss: 0.6868, Val: 0.7323, Test: 0.6188\n",
      "Epoch: 049, Loss: 0.6863, Val: 0.7335, Test: 0.6194\n",
      "Epoch: 050, Loss: 0.6869, Val: 0.7321, Test: 0.6193\n",
      "Epoch: 051, Loss: 0.6858, Val: 0.7307, Test: 0.6176\n",
      "Epoch: 052, Loss: 0.6861, Val: 0.7321, Test: 0.6178\n",
      "Epoch: 053, Loss: 0.6845, Val: 0.7309, Test: 0.6171\n",
      "Epoch: 054, Loss: 0.6863, Val: 0.7323, Test: 0.6172\n",
      "Epoch: 055, Loss: 0.6853, Val: 0.7314, Test: 0.6153\n",
      "Epoch: 056, Loss: 0.6860, Val: 0.7284, Test: 0.6153\n",
      "Epoch: 057, Loss: 0.6855, Val: 0.7261, Test: 0.6124\n",
      "Epoch: 058, Loss: 0.6846, Val: 0.7300, Test: 0.6143\n",
      "Epoch: 059, Loss: 0.6836, Val: 0.7296, Test: 0.6153\n",
      "Epoch: 060, Loss: 0.6854, Val: 0.7298, Test: 0.6159\n",
      "Epoch: 061, Loss: 0.6850, Val: 0.7291, Test: 0.6164\n",
      "Epoch: 062, Loss: 0.6846, Val: 0.7289, Test: 0.6154\n",
      "Epoch: 063, Loss: 0.6858, Val: 0.7289, Test: 0.6139\n",
      "Epoch: 064, Loss: 0.6851, Val: 0.7273, Test: 0.6102\n",
      "Epoch: 065, Loss: 0.6841, Val: 0.7231, Test: 0.6053\n",
      "Epoch: 066, Loss: 0.6846, Val: 0.7195, Test: 0.6027\n",
      "Epoch: 067, Loss: 0.6832, Val: 0.7195, Test: 0.6045\n",
      "Epoch: 068, Loss: 0.6838, Val: 0.7229, Test: 0.6107\n",
      "Epoch: 069, Loss: 0.6845, Val: 0.7215, Test: 0.6112\n",
      "Epoch: 070, Loss: 0.6835, Val: 0.7199, Test: 0.6119\n",
      "Epoch: 071, Loss: 0.6817, Val: 0.7167, Test: 0.6101\n",
      "Epoch: 072, Loss: 0.6846, Val: 0.7167, Test: 0.6045\n",
      "Epoch: 073, Loss: 0.6829, Val: 0.7107, Test: 0.6028\n",
      "Epoch: 074, Loss: 0.6836, Val: 0.7073, Test: 0.5987\n",
      "Epoch: 075, Loss: 0.6830, Val: 0.7066, Test: 0.5978\n",
      "Epoch: 076, Loss: 0.6824, Val: 0.7080, Test: 0.5995\n",
      "Epoch: 077, Loss: 0.6820, Val: 0.7101, Test: 0.6031\n",
      "Epoch: 078, Loss: 0.6810, Val: 0.7084, Test: 0.6078\n",
      "Epoch: 079, Loss: 0.6814, Val: 0.7096, Test: 0.6072\n",
      "Epoch: 080, Loss: 0.6809, Val: 0.7075, Test: 0.6071\n",
      "Epoch: 081, Loss: 0.6771, Val: 0.7098, Test: 0.6096\n",
      "Epoch: 082, Loss: 0.6815, Val: 0.7048, Test: 0.6061\n",
      "Epoch: 083, Loss: 0.6807, Val: 0.6995, Test: 0.5974\n",
      "Epoch: 084, Loss: 0.6789, Val: 0.6926, Test: 0.5923\n",
      "Epoch: 085, Loss: 0.6802, Val: 0.6919, Test: 0.5913\n",
      "Epoch: 086, Loss: 0.6813, Val: 0.6961, Test: 0.5931\n",
      "Epoch: 087, Loss: 0.6832, Val: 0.6974, Test: 0.5950\n",
      "Epoch: 088, Loss: 0.6809, Val: 0.7011, Test: 0.5964\n",
      "Epoch: 089, Loss: 0.6807, Val: 0.7002, Test: 0.5978\n",
      "Epoch: 090, Loss: 0.6823, Val: 0.6974, Test: 0.5962\n",
      "Epoch: 091, Loss: 0.6785, Val: 0.6972, Test: 0.5945\n",
      "Epoch: 092, Loss: 0.6801, Val: 0.6926, Test: 0.5931\n",
      "Epoch: 093, Loss: 0.6807, Val: 0.6864, Test: 0.5919\n",
      "Epoch: 094, Loss: 0.6776, Val: 0.6857, Test: 0.5919\n",
      "Epoch: 095, Loss: 0.6812, Val: 0.6820, Test: 0.5906\n",
      "Epoch: 096, Loss: 0.6776, Val: 0.6811, Test: 0.5904\n",
      "Epoch: 097, Loss: 0.6793, Val: 0.6795, Test: 0.5906\n",
      "Epoch: 098, Loss: 0.6786, Val: 0.6786, Test: 0.5902\n",
      "Epoch: 099, Loss: 0.6804, Val: 0.6749, Test: 0.5884\n",
      "Epoch: 100, Loss: 0.6825, Val: 0.6685, Test: 0.5860\n",
      "Final Test: 0.6236\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(6, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5518)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "z = model.encode(data.x, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "inverted_mask_matrix = torch.ones([num_nodes,num_nodes]).squeeze(0) - mask.to('cpu')\n",
    "\n",
    "(adj_matrix*inverted_mask_matrix == predicted_adj*inverted_mask_matrix).sum() / adj_matrix.shape[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.2692, Val: 0.3781, Test: 0.3453\n",
      "Epoch: 002, Loss: 0.7576, Val: 0.4183, Test: 0.3903\n",
      "Epoch: 003, Loss: 0.7321, Val: 0.4477, Test: 0.4240\n",
      "Epoch: 004, Loss: 0.7378, Val: 0.4500, Test: 0.4237\n",
      "Epoch: 005, Loss: 0.7303, Val: 0.4463, Test: 0.4169\n",
      "Epoch: 006, Loss: 0.7177, Val: 0.4470, Test: 0.4086\n",
      "Epoch: 007, Loss: 0.7083, Val: 0.4405, Test: 0.4025\n",
      "Epoch: 008, Loss: 0.7026, Val: 0.4394, Test: 0.3883\n",
      "Epoch: 009, Loss: 0.7010, Val: 0.4461, Test: 0.3746\n",
      "Epoch: 010, Loss: 0.6993, Val: 0.4667, Test: 0.3799\n",
      "Epoch: 011, Loss: 0.6980, Val: 0.5211, Test: 0.4205\n",
      "Epoch: 012, Loss: 0.6961, Val: 0.5712, Test: 0.4763\n",
      "Epoch: 013, Loss: 0.6956, Val: 0.6084, Test: 0.5244\n",
      "Epoch: 014, Loss: 0.6938, Val: 0.6439, Test: 0.5735\n",
      "Epoch: 015, Loss: 0.6928, Val: 0.6646, Test: 0.6085\n",
      "Epoch: 016, Loss: 0.6923, Val: 0.6745, Test: 0.6212\n",
      "Epoch: 017, Loss: 0.6912, Val: 0.6742, Test: 0.6260\n",
      "Epoch: 018, Loss: 0.6905, Val: 0.6765, Test: 0.6309\n",
      "Epoch: 019, Loss: 0.6903, Val: 0.6772, Test: 0.6326\n",
      "Epoch: 020, Loss: 0.6901, Val: 0.6758, Test: 0.6337\n",
      "Epoch: 021, Loss: 0.6909, Val: 0.6761, Test: 0.6333\n",
      "Epoch: 022, Loss: 0.6886, Val: 0.6786, Test: 0.6344\n",
      "Epoch: 023, Loss: 0.6882, Val: 0.6765, Test: 0.6330\n",
      "Epoch: 024, Loss: 0.6879, Val: 0.6747, Test: 0.6331\n",
      "Epoch: 025, Loss: 0.6892, Val: 0.6745, Test: 0.6351\n",
      "Epoch: 026, Loss: 0.6882, Val: 0.6745, Test: 0.6362\n",
      "Epoch: 027, Loss: 0.6876, Val: 0.6742, Test: 0.6371\n",
      "Epoch: 028, Loss: 0.6871, Val: 0.6715, Test: 0.6362\n",
      "Epoch: 029, Loss: 0.6860, Val: 0.6669, Test: 0.6354\n",
      "Epoch: 030, Loss: 0.6851, Val: 0.6635, Test: 0.6327\n",
      "Epoch: 031, Loss: 0.6847, Val: 0.6600, Test: 0.6309\n",
      "Epoch: 032, Loss: 0.6865, Val: 0.6600, Test: 0.6290\n",
      "Epoch: 033, Loss: 0.6854, Val: 0.6623, Test: 0.6269\n",
      "Epoch: 034, Loss: 0.6839, Val: 0.6616, Test: 0.6255\n",
      "Epoch: 035, Loss: 0.6851, Val: 0.6605, Test: 0.6227\n",
      "Epoch: 036, Loss: 0.6858, Val: 0.6570, Test: 0.6182\n",
      "Epoch: 037, Loss: 0.6838, Val: 0.6547, Test: 0.6143\n",
      "Epoch: 038, Loss: 0.6850, Val: 0.6513, Test: 0.6129\n",
      "Epoch: 039, Loss: 0.6820, Val: 0.6492, Test: 0.6114\n",
      "Epoch: 040, Loss: 0.6811, Val: 0.6462, Test: 0.6114\n",
      "Epoch: 041, Loss: 0.6830, Val: 0.6449, Test: 0.6100\n",
      "Epoch: 042, Loss: 0.6821, Val: 0.6435, Test: 0.6058\n",
      "Epoch: 043, Loss: 0.6837, Val: 0.6407, Test: 0.6018\n",
      "Epoch: 044, Loss: 0.6821, Val: 0.6350, Test: 0.5985\n",
      "Epoch: 045, Loss: 0.6830, Val: 0.6309, Test: 0.5942\n",
      "Epoch: 046, Loss: 0.6797, Val: 0.6244, Test: 0.5894\n",
      "Epoch: 047, Loss: 0.6802, Val: 0.6159, Test: 0.5903\n",
      "Epoch: 048, Loss: 0.6803, Val: 0.6141, Test: 0.5950\n",
      "Epoch: 049, Loss: 0.6766, Val: 0.6146, Test: 0.5971\n",
      "Epoch: 050, Loss: 0.6769, Val: 0.6155, Test: 0.5979\n",
      "Epoch: 051, Loss: 0.6748, Val: 0.6157, Test: 0.5969\n",
      "Epoch: 052, Loss: 0.6775, Val: 0.6244, Test: 0.5950\n",
      "Epoch: 053, Loss: 0.6719, Val: 0.6272, Test: 0.5925\n",
      "Epoch: 054, Loss: 0.6729, Val: 0.6272, Test: 0.5917\n",
      "Epoch: 055, Loss: 0.6701, Val: 0.6329, Test: 0.5907\n",
      "Epoch: 056, Loss: 0.6689, Val: 0.6336, Test: 0.5899\n",
      "Epoch: 057, Loss: 0.6666, Val: 0.6352, Test: 0.5958\n",
      "Epoch: 058, Loss: 0.6632, Val: 0.6201, Test: 0.6067\n",
      "Epoch: 059, Loss: 0.6614, Val: 0.6152, Test: 0.6113\n",
      "Epoch: 060, Loss: 0.6548, Val: 0.6283, Test: 0.6059\n",
      "Epoch: 061, Loss: 0.6550, Val: 0.6263, Test: 0.6115\n",
      "Epoch: 062, Loss: 0.6499, Val: 0.6127, Test: 0.6136\n",
      "Epoch: 063, Loss: 0.6466, Val: 0.6244, Test: 0.6077\n",
      "Epoch: 064, Loss: 0.6458, Val: 0.6169, Test: 0.6068\n",
      "Epoch: 065, Loss: 0.6421, Val: 0.5907, Test: 0.6124\n",
      "Epoch: 066, Loss: 0.6335, Val: 0.5902, Test: 0.6117\n",
      "Epoch: 067, Loss: 0.6356, Val: 0.6111, Test: 0.5952\n",
      "Epoch: 068, Loss: 0.6303, Val: 0.6109, Test: 0.5926\n",
      "Epoch: 069, Loss: 0.6368, Val: 0.5978, Test: 0.6018\n",
      "Epoch: 070, Loss: 0.6222, Val: 0.5758, Test: 0.6114\n",
      "Epoch: 071, Loss: 0.6185, Val: 0.5790, Test: 0.6087\n",
      "Epoch: 072, Loss: 0.6201, Val: 0.6065, Test: 0.5930\n",
      "Epoch: 073, Loss: 0.6180, Val: 0.6056, Test: 0.5981\n",
      "Epoch: 074, Loss: 0.6147, Val: 0.5930, Test: 0.6104\n",
      "Epoch: 075, Loss: 0.6199, Val: 0.5902, Test: 0.6137\n",
      "Epoch: 076, Loss: 0.6122, Val: 0.5978, Test: 0.6105\n",
      "Epoch: 077, Loss: 0.5953, Val: 0.6035, Test: 0.6059\n",
      "Epoch: 078, Loss: 0.6191, Val: 0.6118, Test: 0.6053\n",
      "Epoch: 079, Loss: 0.6159, Val: 0.6102, Test: 0.6090\n",
      "Epoch: 080, Loss: 0.6068, Val: 0.5783, Test: 0.6258\n",
      "Epoch: 081, Loss: 0.6122, Val: 0.5909, Test: 0.6214\n",
      "Epoch: 082, Loss: 0.5958, Val: 0.6097, Test: 0.6106\n",
      "Epoch: 083, Loss: 0.6147, Val: 0.6104, Test: 0.6135\n",
      "Epoch: 084, Loss: 0.6101, Val: 0.5898, Test: 0.6264\n",
      "Epoch: 085, Loss: 0.6160, Val: 0.5854, Test: 0.6284\n",
      "Epoch: 086, Loss: 0.6054, Val: 0.6010, Test: 0.6240\n",
      "Epoch: 087, Loss: 0.6046, Val: 0.6169, Test: 0.6161\n",
      "Epoch: 088, Loss: 0.6163, Val: 0.6129, Test: 0.6173\n",
      "Epoch: 089, Loss: 0.6062, Val: 0.6051, Test: 0.6219\n",
      "Epoch: 090, Loss: 0.6114, Val: 0.6008, Test: 0.6251\n",
      "Epoch: 091, Loss: 0.6123, Val: 0.5960, Test: 0.6271\n",
      "Epoch: 092, Loss: 0.6136, Val: 0.5944, Test: 0.6271\n",
      "Epoch: 093, Loss: 0.6118, Val: 0.5971, Test: 0.6274\n",
      "Epoch: 094, Loss: 0.6090, Val: 0.5985, Test: 0.6261\n",
      "Epoch: 095, Loss: 0.6067, Val: 0.5923, Test: 0.6285\n",
      "Epoch: 096, Loss: 0.6125, Val: 0.5941, Test: 0.6270\n",
      "Epoch: 097, Loss: 0.6071, Val: 0.5999, Test: 0.6246\n",
      "Epoch: 098, Loss: 0.6088, Val: 0.6026, Test: 0.6224\n",
      "Epoch: 099, Loss: 0.6047, Val: 0.6061, Test: 0.6178\n",
      "Epoch: 100, Loss: 0.6160, Val: 0.6077, Test: 0.6187\n",
      "Final Test: 0.6344\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(12, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_ase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6060)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_ase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "inverted_mask_matrix = torch.ones([num_nodes,num_nodes]).squeeze(0) - mask.to('cpu')\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "\n",
    "(adj_matrix*inverted_mask_matrix == predicted_adj*inverted_mask_matrix).sum() / adj_matrix.shape[0]**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLASE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.9136, Val: 0.2950, Test: 0.1806\n",
      "Epoch: 002, Loss: 0.7345, Val: 0.5354, Test: 0.4638\n",
      "Epoch: 003, Loss: 0.7256, Val: 0.5452, Test: 0.4712\n",
      "Epoch: 004, Loss: 0.7168, Val: 0.5016, Test: 0.4162\n",
      "Epoch: 005, Loss: 0.7059, Val: 0.4904, Test: 0.3919\n",
      "Epoch: 006, Loss: 0.7010, Val: 0.5142, Test: 0.4106\n",
      "Epoch: 007, Loss: 0.6974, Val: 0.5627, Test: 0.4644\n",
      "Epoch: 008, Loss: 0.6954, Val: 0.6405, Test: 0.5425\n",
      "Epoch: 009, Loss: 0.6935, Val: 0.6983, Test: 0.6022\n",
      "Epoch: 010, Loss: 0.6918, Val: 0.7176, Test: 0.6288\n",
      "Epoch: 011, Loss: 0.6915, Val: 0.7220, Test: 0.6380\n",
      "Epoch: 012, Loss: 0.6903, Val: 0.7208, Test: 0.6382\n",
      "Epoch: 013, Loss: 0.6905, Val: 0.7204, Test: 0.6376\n",
      "Epoch: 014, Loss: 0.6879, Val: 0.7204, Test: 0.6354\n",
      "Epoch: 015, Loss: 0.6906, Val: 0.7213, Test: 0.6331\n",
      "Epoch: 016, Loss: 0.6874, Val: 0.7202, Test: 0.6303\n",
      "Epoch: 017, Loss: 0.6891, Val: 0.7225, Test: 0.6291\n",
      "Epoch: 018, Loss: 0.6886, Val: 0.7218, Test: 0.6278\n",
      "Epoch: 019, Loss: 0.6875, Val: 0.7234, Test: 0.6287\n",
      "Epoch: 020, Loss: 0.6882, Val: 0.7254, Test: 0.6298\n",
      "Epoch: 021, Loss: 0.6883, Val: 0.7277, Test: 0.6301\n",
      "Epoch: 022, Loss: 0.6872, Val: 0.7305, Test: 0.6307\n",
      "Epoch: 023, Loss: 0.6859, Val: 0.7321, Test: 0.6299\n",
      "Epoch: 024, Loss: 0.6864, Val: 0.7348, Test: 0.6292\n",
      "Epoch: 025, Loss: 0.6869, Val: 0.7353, Test: 0.6291\n",
      "Epoch: 026, Loss: 0.6853, Val: 0.7385, Test: 0.6280\n",
      "Epoch: 027, Loss: 0.6848, Val: 0.7392, Test: 0.6271\n",
      "Epoch: 028, Loss: 0.6836, Val: 0.7408, Test: 0.6266\n",
      "Epoch: 029, Loss: 0.6849, Val: 0.7436, Test: 0.6263\n",
      "Epoch: 030, Loss: 0.6824, Val: 0.7459, Test: 0.6251\n",
      "Epoch: 031, Loss: 0.6826, Val: 0.7456, Test: 0.6255\n",
      "Epoch: 032, Loss: 0.6806, Val: 0.7454, Test: 0.6252\n",
      "Epoch: 033, Loss: 0.6819, Val: 0.7449, Test: 0.6259\n",
      "Epoch: 034, Loss: 0.6785, Val: 0.7408, Test: 0.6243\n",
      "Epoch: 035, Loss: 0.6803, Val: 0.7378, Test: 0.6239\n",
      "Epoch: 036, Loss: 0.6786, Val: 0.7275, Test: 0.6216\n",
      "Epoch: 037, Loss: 0.6786, Val: 0.7045, Test: 0.6220\n",
      "Epoch: 038, Loss: 0.6787, Val: 0.6811, Test: 0.6257\n",
      "Epoch: 039, Loss: 0.6751, Val: 0.6651, Test: 0.6279\n",
      "Epoch: 040, Loss: 0.6698, Val: 0.6623, Test: 0.6294\n",
      "Epoch: 041, Loss: 0.6693, Val: 0.6683, Test: 0.6154\n",
      "Epoch: 042, Loss: 0.6701, Val: 0.6524, Test: 0.6124\n",
      "Epoch: 043, Loss: 0.6696, Val: 0.6162, Test: 0.6210\n",
      "Epoch: 044, Loss: 0.6617, Val: 0.5960, Test: 0.6260\n",
      "Epoch: 045, Loss: 0.6610, Val: 0.5865, Test: 0.6239\n",
      "Epoch: 046, Loss: 0.6613, Val: 0.5859, Test: 0.6116\n",
      "Epoch: 047, Loss: 0.6628, Val: 0.5969, Test: 0.5987\n",
      "Epoch: 048, Loss: 0.6594, Val: 0.5980, Test: 0.5898\n",
      "Epoch: 049, Loss: 0.6676, Val: 0.5872, Test: 0.5950\n",
      "Epoch: 050, Loss: 0.6625, Val: 0.5794, Test: 0.6013\n",
      "Epoch: 051, Loss: 0.6594, Val: 0.5863, Test: 0.5953\n",
      "Epoch: 052, Loss: 0.6598, Val: 0.5932, Test: 0.5961\n",
      "Epoch: 053, Loss: 0.6696, Val: 0.5838, Test: 0.6038\n",
      "Epoch: 054, Loss: 0.6631, Val: 0.5769, Test: 0.6036\n",
      "Epoch: 055, Loss: 0.6531, Val: 0.5904, Test: 0.6010\n",
      "Epoch: 056, Loss: 0.6548, Val: 0.6155, Test: 0.5908\n",
      "Epoch: 057, Loss: 0.6621, Val: 0.6175, Test: 0.5913\n",
      "Epoch: 058, Loss: 0.6535, Val: 0.6198, Test: 0.5969\n",
      "Epoch: 059, Loss: 0.6542, Val: 0.5992, Test: 0.6045\n",
      "Epoch: 060, Loss: 0.6512, Val: 0.5960, Test: 0.6025\n",
      "Epoch: 061, Loss: 0.6580, Val: 0.6006, Test: 0.6011\n",
      "Epoch: 062, Loss: 0.6502, Val: 0.6125, Test: 0.5957\n",
      "Epoch: 063, Loss: 0.6535, Val: 0.6292, Test: 0.5895\n",
      "Epoch: 064, Loss: 0.6564, Val: 0.6274, Test: 0.5892\n",
      "Epoch: 065, Loss: 0.6587, Val: 0.6088, Test: 0.5960\n",
      "Epoch: 066, Loss: 0.6550, Val: 0.5976, Test: 0.6026\n",
      "Epoch: 067, Loss: 0.6531, Val: 0.6022, Test: 0.6004\n",
      "Epoch: 068, Loss: 0.6469, Val: 0.6045, Test: 0.5901\n",
      "Epoch: 069, Loss: 0.6567, Val: 0.6077, Test: 0.5871\n",
      "Epoch: 070, Loss: 0.6526, Val: 0.6180, Test: 0.5968\n",
      "Epoch: 071, Loss: 0.6487, Val: 0.6116, Test: 0.6042\n",
      "Epoch: 072, Loss: 0.6542, Val: 0.5978, Test: 0.5983\n",
      "Epoch: 073, Loss: 0.6442, Val: 0.6012, Test: 0.5880\n",
      "Epoch: 074, Loss: 0.6321, Val: 0.6182, Test: 0.5868\n",
      "Epoch: 075, Loss: 0.6511, Val: 0.6290, Test: 0.5899\n",
      "Epoch: 076, Loss: 0.6555, Val: 0.6198, Test: 0.5942\n",
      "Epoch: 077, Loss: 0.6536, Val: 0.6049, Test: 0.5942\n",
      "Epoch: 078, Loss: 0.6426, Val: 0.5941, Test: 0.5960\n",
      "Epoch: 079, Loss: 0.6519, Val: 0.5932, Test: 0.5914\n",
      "Epoch: 080, Loss: 0.6518, Val: 0.6125, Test: 0.5880\n",
      "Epoch: 081, Loss: 0.6531, Val: 0.6191, Test: 0.5907\n",
      "Epoch: 082, Loss: 0.6409, Val: 0.6221, Test: 0.5949\n",
      "Epoch: 083, Loss: 0.6565, Val: 0.6102, Test: 0.5931\n",
      "Epoch: 084, Loss: 0.6507, Val: 0.5999, Test: 0.5915\n",
      "Epoch: 085, Loss: 0.6404, Val: 0.5996, Test: 0.5926\n",
      "Epoch: 086, Loss: 0.6484, Val: 0.6166, Test: 0.5880\n",
      "Epoch: 087, Loss: 0.6382, Val: 0.6361, Test: 0.5863\n",
      "Epoch: 088, Loss: 0.6376, Val: 0.6407, Test: 0.5817\n",
      "Epoch: 089, Loss: 0.6547, Val: 0.6152, Test: 0.5688\n",
      "Epoch: 090, Loss: 0.6483, Val: 0.5964, Test: 0.5830\n",
      "Epoch: 091, Loss: 0.6543, Val: 0.5957, Test: 0.5988\n",
      "Epoch: 092, Loss: 0.6379, Val: 0.6079, Test: 0.5933\n",
      "Epoch: 093, Loss: 0.6435, Val: 0.6182, Test: 0.5862\n",
      "Epoch: 094, Loss: 0.6347, Val: 0.6244, Test: 0.5773\n",
      "Epoch: 095, Loss: 0.6384, Val: 0.6175, Test: 0.5677\n",
      "Epoch: 096, Loss: 0.6397, Val: 0.6045, Test: 0.5813\n",
      "Epoch: 097, Loss: 0.6421, Val: 0.6061, Test: 0.5934\n",
      "Epoch: 098, Loss: 0.6442, Val: 0.6162, Test: 0.5897\n",
      "Epoch: 099, Loss: 0.6391, Val: 0.6283, Test: 0.5807\n",
      "Epoch: 100, Loss: 0.6342, Val: 0.6214, Test: 0.5894\n",
      "Final Test: 0.6251\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "model = Net(12, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_train = torch.concatenate((train_data.x, train_data.x_glase), axis=1)\n",
    "    z = model.encode(x_train, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "    z = model.encode(x_test, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "x_test = torch.concatenate((test_data.x, test_data.x_glase), axis=1)\n",
    "z = model.encode(x_test, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5943)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predict on entire masked graph\n",
    "x_test = torch.concatenate((data.x, data.x_glase), axis=1)\n",
    "z = model.encode(x_test, data.edge_index)\n",
    "final_edge_index = model.decode_all(z)\n",
    "\n",
    "predicted_adj = to_dense_adj(final_edge_index).squeeze(0).to('cpu')\n",
    "# (adj_matrix[not_present_countries][:,res_indexes]==predicted_adj[not_present_countries][:,res_indexes]).sum() / len(not_present_countries) / len(res_indexes)\n",
    "\n",
    "(adj_matrix*inverted_mask_matrix == predicted_adj*inverted_mask_matrix).sum() / adj_matrix.shape[0]**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
